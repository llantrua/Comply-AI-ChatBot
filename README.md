# Comply by Sepefrei

![Comply Logo](./image/logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks]
    ‚Üì
[Context Building] ‚Üí [Prompt Engineering] ‚Üí [Claude LLM] ‚Üí [Response Formatting]
    ‚Üì
[JSON R√©ponse] ‚Üí [Slack Bot / Web UI / API Client]
```

### Phase 1 : Acquisition des Donn√©es (Scraping)

#### Architecture du Scraping Selenium

Le scraping s'effectue via des scripts Python d√©di√©s par source, utilisant Selenium WebDriver pour g√©rer le JavaScript et les interactions complexes.

**Script principal** : `scrapers/kiwi_scraper.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import json
from datetime import datetime

class KiwiScraper:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def scrape_kiwi_legal(self):
        """Scrape Kiwi Legal documents"""
        base_url = "https://kiwi.cnje.fr/legal"
        self.driver.get(base_url)
        
        # Attente du chargement dynamique
        self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "document-list"))
        )
        
        documents = []
        doc_elements = self.driver.find_elements(By.CLASS_NAME, "document-item")
        
        for element in doc_elements:
            doc_data = self._extract_legal_document(element)
            documents.append(doc_data)
        
        return documents
```

**Gestion de la pagination** :
```python
def scrape_with_pagination(self, url, max_pages=None):
    page = 1
    all_data = []
    
    while True:
        print(f"Scraping page {page}...")
        self.driver.get(f"{url}?page={page}")
        
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "content"))
            )
        except TimeoutException:
            print(f"No more pages after page {page-1}")
            break
        
        page_data = self._extract_page_content()
        if not page_data:
            break
        
        all_data.extend(page_data)
        page += 1
        
        if max_pages and page > max_pages:
            break
    
    return all_data
```

**Gestion des erreurs et retry** :
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def robust_scrape(self, url):
    try:
        self.driver.get(url)
        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        return self._extract_content()
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        raise
```

#### Extraction et Nettoyage HTML

Apr√®s extraction Selenium, parsing avec BeautifulSoup pour nettoyage :

```python
from bs4 import BeautifulSoup
import re

def clean_html_content(raw_html):
    """Nettoyage HTML et extraction texte pertinent"""
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # Suppression √©l√©ments non pertinents
    for element in soup(['script', 'style', 'nav', 'footer', 'header']):
        element.decompose()
    
    # Suppression classes publicitaires
    for ad in soup.find_all(class_=['advertisement', 'popup', 'banner']):
        ad.decompose()
    
    # Extraction texte
    text = soup.get_text(separator='\n', strip=True)
    
    # Nettoyage espaces multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text

def extract_metadata(soup):
    """Extraction m√©tadonn√©es structur√©es"""
    metadata = {}
    
    # Titre
    title_tag = soup.find('h1') or soup.find('title')
    metadata['title'] = title_tag.get_text(strip=True) if title_tag else "Unknown"
    
    # Date publication
    date_tag = soup.find('time') or soup.find(class_='date')
    if date_tag:
        metadata['date'] = date_tag.get('datetime') or date_tag.get_text(strip=True)
    
    # Auteur
    author_tag = soup.find(class_='author') or soup.find(rel='author')
    if author_tag:
        metadata['author'] = author_tag.get_text(strip=True)
    
    # Cat√©gorie
    category_tag = soup.find(class_='category')
    if category_tag:
        metadata['category'] = category_tag.get_text(strip=True)
    
    return metadata
```

#### Structure JSON Standardis√©e

Export dans un format JSON unifi√© facilitant le traitement ult√©rieur :

**Format Legal** :
```json
{
  "source": "kiwi_legal",
  "document_type": "statuts",
  "scraping_metadata": {
    "url": "https://kiwi.cnje.fr/legal/statuts-types-association",
    "date_scraped": "2025-01-15T10:30:00Z",
    "scraper_version": "2.1.0"
  },
  "metadata": {
    "title": "Statuts types Junior-Entreprise association loi 1901",
    "category": "juridique",
    "subcategory": "statuts",
    "publication_date": "2024-06-01",
    "author": "Commission Juridique CNJE"
  },
  "content": {
    "sections": [
      {
        "title": "TITRE I - Dispositions g√©n√©rales",
        "articles": [
          {
            "number": 1,
            "title": "D√©nomination",
            "content": "Il est fond√© entre les adh√©rents aux pr√©sents statuts..."
          }
        ]
      }
    ],
    "full_text": "STATUTS TYPES..."
  }
}
```

**Format RSE** :
```json
{
  "source": "kiwi_rse",
  "document_type": "module_rse",
  "scraping_metadata": {...},
  "metadata": {
    "title": "Module Environnement - Gestion des D√©chets",
    "pilier": "environnemental",
    "odd_concernes": [12, 13],
    "niveau_difficulte": "d√©butant"
  },
  "content": {
    "introduction": "La gestion des d√©chets...",
    "objectifs": ["R√©duire la production", "Recycler"],
    "actions": [
      {
        "titre": "Mise en place du tri s√©lectif",
        "description": "...",
        "indicateurs": ["Taux de recyclage", "Volume d√©chets"]
      }
    ]
  }
}
```

**Format FAQ** :
```json
{
  "source": "kiwi_faq",
  "document_type": "faq",
  "scraping_metadata": {...},
  "metadata": {
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2
  },
  "content": {
    "questions": [
      {
        "id": "compta_tva_001",
        "question": "Comment d√©clarer la TVA en tant que JE ?",
        "reponse": "Les Junior-Entreprises b√©n√©ficient...",
        "tags": ["tva", "d√©claration", "comptabilit√©"],
        "related_questions": ["compta_tva_002", "compta_tva_005"]
      }
    ]
  }
}
```

#### Stockage et Versioning

**Arborescence de stockage** :
```
data/
‚îú‚îÄ‚îÄ raw/                          # Donn√©es brutes apr√®s scraping
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_2025-01-15.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_2025-01-15.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_2025-01-15.json
‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_processed.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_processed.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_processed.json
‚îú‚îÄ‚îÄ index/                        # Index g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îî‚îÄ‚îÄ logs/                         # Logs de scraping
    ‚îî‚îÄ‚îÄ scraping_2025-01-15.log
```

**Logging d√©taill√©** :
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scraping_{datetime.now().date()}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Dans le scraper
logger.info(f"Starting scrape of {url}")
logger.info(f"Extracted {len(documents)} documents")
logger.warning(f"Failed to extract metadata for document {doc_id}")
logger.error(f"Scraping failed: {exception}")
```

### Phase 2 : Preprocessing & Transformation

#### Type Detection Automatique

Algorithme de d√©tection bas√© sur plusieurs signaux :

```python
class DocumentTypeDetector:
    def __init__(self):
        self.type_patterns = {
            'legal': {
                'filename': ['statuts', 'contrat', 'legal', 'juridique'],
                'fields': ['articles', 'sections', 'clauses'],
                'keywords': ['article', 'alin√©a', 'conform√©ment', 'obligation']
            },
            'rse': {
                'filename': ['rse', 'durable', 'environnement'],
                'fields': ['pilier', 'odd', 'actions'],
                'keywords': ['d√©veloppement durable', 'odd', 'responsabilit√©']
            },
            'faq': {
                'filename': ['faq', 'questions'],
                'fields': ['questions', 'reponses'],
                'keywords': ['comment', 'pourquoi', 'qu\'est-ce']
            },
            'je': {
                'filename': ['annuaire', 'je', 'junior'],
                'fields': ['nom', 'ville', 'ecole', 'domaines'],
                'keywords': ['junior-entreprise', '√©cole', 'domaine']
            }
        }
    
    def detect_type(self, document_data, filename):
        scores = {doc_type: 0 for doc_type in self.type_patterns}
        
        # Score filename
        for doc_type, patterns in self.type_patterns.items():
            for pattern in patterns['filename']:
                if pattern in filename.lower():
                    scores[doc_type] += 2
        
        # Score fields pr√©sents
        doc_fields = set(document_data.get('content', {}).keys())
        for doc_type, patterns in self.type_patterns.items():
            matching_fields = doc_fields.intersection(patterns['fields'])
            scores[doc_type] += len(matching_fields) * 3
        
        # Score keywords dans le contenu
        content_text = json.dumps(document_data).lower()
        for doc_type, patterns in self.type_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_text:
                    scores[doc_type] += 1
        
        # S√©lection du type avec le score maximal
        detected_type = max(scores, key=scores.get)
        confidence = scores[detected_type] / sum(scores.values()) if sum(scores.values()) > 0 else 0
        
        return {
            'type': detected_type if confidence > 0.3 else 'general',
            'confidence': confidence,
            'scores': scores
        }
```

#### Extraction Sp√©cialis√©e par Type

**Extracteur Legal** :
```python
class LegalExtractor:
    def extract(self, document):
        extracted_data = []
        
        sections = document['content']['sections']
        for section in sections:
            section_title = section['title']
            
            for article in section.get('articles', []):
                extracted_data.append({
                    'text': f"{article['title']}\n{article['content']}",
                    'type': 'legal',
                    'metadata': {
                        'document_type': document['document_type'],
                        'section': section_title,
                        'article_num': article['number'],
                        'title': article['title']
                    }
                })
        
        return extracted_data
```

**Extracteur FAQ** :
```python
class FAQExtractor:
    def extract(self, document):
        extracted_data = []
        
        category = document['metadata']['category']
        subcategory = document['metadata'].get('subcategory', '')
        level = document['metadata'].get('level', 1)
        
        for qa in document['content']['questions']:
            # Contexte hi√©rarchique
            context_path = f"{category}"
            if subcategory:
                context_path += f" > {subcategory}"
            
            text = f"Question: {qa['question']}\n\nR√©ponse: {qa['reponse']}"
            
            extracted_data.append({
                'text': text,
                'type': 'faq',
                'metadata': {
                    'question': qa['question'],
                    'category': category,
                    'subcategory': subcategory,
                    'level': level,
                    'context_path': context_path,
                    'tags': qa.get('tags', []),
                    'related_questions': qa.get('related_questions', [])
                }
            })
        
        return extracted_data
```

**Extracteur JE** :
```python
class JEExtractor:
    def extract(self, document):
        extracted_data = []
        
        for je in document['content']['junior_entreprises']:
            # Construction texte descriptif
            text = f"""
            Nom: {je['nom']}
            Ville: {je['ville']}
            √âcole: {je['ecole']}
            Domaines d'expertise: {', '.join(je['domaines'])}
            Contact: {je['contact']['email']}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'je',
                'metadata': {
                    'nom': je['nom'],
                    'ville': je['ville'],
                    'ecole': je['ecole'],
                    'domaines': je['domaines'],
                    'contact': je['contact'],
                    'certified': je.get('certified_cnje', False)
                }
            })
        
        return extracted_data
```

**Extracteur RSE** :
```python
class RSEExtractor:
    def extract(self, document):
        extracted_data = []
        
        pilier = document['metadata']['pilier']
        odd = document['metadata']['odd_concernes']
        
        # Extraction par action
        for action in document['content']['actions']:
            text = f"""
            Module RSE: {document['metadata']['title']}
            Pilier: {pilier}
            
            Action: {action['titre']}
            {action['description']}
            
            Indicateurs: {', '.join(action['indicateurs'])}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'rse',
                'metadata': {
                    'module': document['metadata']['title'],
                    'pilier': pilier,
                    'odd': odd,
                    'action_titre': action['titre'],
                    'indicateurs': action['indicateurs']
                }
            })
        
        return extracted_data
```

#### Smart Chunking S√©mantique

Le chunking respecte la logique m√©tier plut√¥t qu'une simple d√©coupe par longueur :

```python
class SemanticChunker:
    def __init__(self, min_length=50, max_length=1000, target_length=300):
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
    
    def chunk_text(self, text, doc_type, metadata):
        if doc_type == 'faq':
            # FAQ: chaque Q/A est un chunk autonome
            return self._chunk_faq(text, metadata)
        elif doc_type == 'legal':
            # Legal: d√©coupage par article/section
            return self._chunk_legal(text, metadata)
        elif doc_type == 'je':
            # JE: entit√© atomique, pas de d√©coupage
            return [self._create_chunk(text, doc_type, metadata)]
        elif doc_type == 'rse':
            # RSE: d√©coupage par action
            return self._chunk_rse(text, metadata)
        else:
            # G√©n√©rique: d√©coupage par paragraphes avec overlap
            return self._chunk_generic(text, doc_type, metadata)
    
    def _chunk_generic(self, text, doc_type, metadata):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_length:
                current_chunk += para + "\n\n"
            else:
                if len(current_chunk) > self.min_length:
                    chunks.append(
                        self._create_chunk(current_chunk.strip(), doc_type, metadata)
                    )
                current_chunk = para + "\n\n"
        
        if len(current_chunk) > self.min_length:
            chunks.append(
                self._create_chunk(current_chunk.strip(), doc_type, metadata)
            )
        
        return chunks
    
    def _create_chunk(self, text, doc_type, metadata):
        return {
            'text': text,
            'type': doc_type,
            'metadata': metadata,
            'length': len(text),
            'word_count': len(text.split())
        }
```

#### Enrichissement M√©tadonn√©es

Chaque chunk est enrichi automatiquement :

```python
class MetadataEnricher:
    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.category_classifier = CategoryClassifier()
    
    def enrich_chunk(self, chunk):
        text = chunk['text']
        
        # Extraction keywords automatique
        keywords = self.keyword_extractor.extract(text, top_n=5)
        chunk['metadata']['keywords'] = keywords
        
        # Classification cat√©gorie fine (si pas d√©j√† pr√©sente)
        if 'category' not in chunk['metadata']:
            category = self.category_classifier.classify(text)
            chunk['metadata']['category'] = category
        
        # Calcul de priorit√© (bas√© sur usage historique si disponible)
        chunk['metadata']['priority'] = self._calculate_priority(chunk)
        
        # Ajout timestamps
        chunk['metadata']['indexed_at'] = datetime.now().isoformat()
        
        # G√©n√©ration d'un hash pour d√©tecter les modifications
        chunk['metadata']['content_hash'] = hashlib.md5(
            text.encode()
        ).hexdigest()
        
        return chunk
    
    def _calculate_priority(self, chunk):
        # Heuristique simple : sources officielles = haute priorit√©
        priority = 0.5
        
        if chunk['type'] == 'legal':
            priority += 0.2
        if 'statuts' in chunk.get('metadata', {}).get('category', '').lower():
            priority += 0.15
        if chunk['metadata'].get('is_featured', False):
            priority += 0.1
        
        return min(priority, 1.0)
```

### Phase 3 : Vectorisation et Indexation

#### Configuration TF-IDF Optimis√©e

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IndexBuilder:
    def __init__(self):
        # Stopwords personnalis√©s JE
        self.custom_stopwords = [
            'junior', 'entreprise', 'je', 'cnje',
            '√©tudiant', '√©tudiante', 'projet', 'mission',
            'conform√©ment', 'article', 'alin√©a', 'paragraphe'
        ]
        
        # Configuration TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8,
            stop_words=self.custom_stopwords,
            sublinear_tf=True,
            norm='l2',
            strip_accents='unicode'
        )
    
    def build_index(self, chunks):
        print(f"Building index from {len(chunks)} chunks...")
        
        # Extraction des textes
        texts = [chunk['text'] for chunk in chunks]
        
        # Vectorisation TF-IDF
        print("Vectorizing with TF-IDF...")
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
        
        # R√©duction dimensionnelle SVD
        print("Applying SVD dimensionality reduction...")
        n_components = min(300, tfidf_matrix.shape[0] - 1)
        svd_model = TruncatedSVD(
            n_components=n_components,
            algorithm='randomized',
            n_iter=7,
            random_state=42
        )
        vectors_reduced = svd_model.fit_transform(tfidf_matrix)
        print(f"Reduced to {n_components} dimensions")
        
        # Construction des index secondaires
        print("Building secondary indexes...")
        metadata_index = self._build_metadata_indexes(chunks)
        
        # Assemblage de l'index complet
        index = {
            'vectorizer': self.vectorizer,
            'svd_model': svd_model,
            'vectors': vectors_reduced,
            'chunks': chunks,
            'metadata_index': metadata_index,
            'version': '2.1.0',
            'build_date': datetime.now().isoformat(),
            'statistics': {
                'n_chunks': len(chunks),
                'n_features': tfidf_matrix.shape[1],
                'n_components': n_components,
                'vocabulary_size': len(self.vectorizer.vocabulary_)
            }
        }
        
        return index
    
    def _build_metadata_indexes(self, chunks):
        by_type = {}
        by_category = {}
        by_source = {}
        
        for idx, chunk in enumerate(chunks):
            # Index by type
            chunk_type = chunk['type']
            if chunk_type not in by_type:
                by_type[chunk_type] = []
            by_type[chunk_type].append(idx)
            
            # Index by category
            category = chunk['metadata'].get('category', 'unknown')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(idx)
            
            # Index by source
            source = chunk['metadata'].get('source_file', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(idx)
        
        return {
            'by_type': by_type,
            'by_category': by_category,
            'by_source': by_source
        }
    
    def save_index(self, index, filepath='data/index/kiwi_advanced_index.pkl'):
        print(f"Saving index to {filepath}...")
        with open(filepath, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"Index saved successfully ({file_size_mb:.2f} MB)")
```

#### Processus Complet d'Indexation

Script principal orchestrant tout le pipeline :

```python
def main_indexation_pipeline():
    print("=== COMPLY INDEXATION PIPELINE ===\n")
    
    # 1. Chargement des donn√©es sources
    print("Step 1: Loading source data...")
    legal_data = load_json('data/processed/kiwi_legal_processed.json')
    rse_data = load_json('data/processed/kiwi_rse_processed.json')
    faq_data = load_json('data/processed/kiwi_faq_processed.json')
    je_data = load_json('data/processed/kiwi_je_processed.json')
    
    all_sources = [
        ('legal', legal_data),
        ('rse', rse_data),
        ('faq', faq_data),
        ('je', je_data)
    ]
    
    # 2. Extraction et chunking
    print("\nStep 2: Extracting and chunking...")
    all_chunks = []
    
    for source_type, data in all_sources:
        extractor = get_extractor(source_type)
        chunks = extractor.extract(data)
        
        # Chunking s√©mantique
        chunker = SemanticChunker()
        chunked_data = []
        for chunk in chunks:
            chunked_data.extend(
                chunker.chunk_text(
                    chunk['text'],
                    chunk['type'],
                    chunk['metadata']
                )
            )
        
        print(f"  - {source_type}: {len(chunked_data)} chunks")
        all_chunks.extend(chunked_data)
    
    print(f"Total chunks: {len(all_chunks)}")
    
    # 3. Enrichissement
    print("\nStep 3: Enriching metadata...")
    enricher = MetadataEnricher()
    enriched_chunks = [enricher.enrich_chunk(c) for c in all_chunks]
    
    # 4. Construction de l'index
    print("\nStep 4: Building vector index...")
    builder = IndexBuilder()
    index = builder.build_index(enriched_chunks)
    
    # 5. Persistance
    print("\nStep 5: Saving index...")
    builder.save_index(index)
    
    # 6. Statistiques finales
    print("\n=== INDEXATION COMPLETE ===")
    print(f"Total chunks indexed: {index['statistics']['n_chunks']}")
    print(f"Vocabulary size: {index['statistics']['vocabulary_size']}")
    print(f"Vector dimensions: {index['statistics']['n_components']}")
    print(f"Index version: {index['version']}")
    
    return index

if __name__ == "__main__":
    main_indexation_pipeline()
```

### Phase 4 : Serving et Recherche

#### Chargement de l'Index au D√©marrage

```python
from fastapi import FastAPI
import pickle

app = FastAPI(title="Comply API", version="2.1.0")

# Chargement de l'index au d√©marrage (√©v√©nement startup)
@app.on_event("startup")
async def load_index():
    global INDEX
    
    print("Loading Comply index...")
    start_time = time.time()
    
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        INDEX = pickle.load(f)
    
    load_time = time.time() - start_time
    print(f"Index loaded in {load_time:.2f}s")
    print(f"  - Version: {INDEX['version']}")
    print(f"  - Chunks: {INDEX['statistics']['n_chunks']}")
    print(f"  - Memory: {sys.getsizeof(INDEX) / (1024**2):.2f} MB")
```

#### Endpoint /ask - Impl√©mentation Compl√®te

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    context: Optional[dict] = None
    options: Optional[dict] = None

class ComprehensiveAnswer(BaseModel):
    answer: str
    confidence: float
    detected_type: str
    sources: List[dict]
    related_questions: List[str]
    processing_time_ms: int

@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    start_time = time.time()
    
    try:
        # 1. D√©tection du type de requ√™te
        query_type_result = detect_query_type(request.question)
        detected_type = query_type_result['detected_type']
        
        # 2. Recherche vectorielle avec boosting
        search_results = vector_search(
            query=request.question,
            query_type=detected_type,
            top_k=request.options.get('max_chunks', 10) if request.options else 10
        )
        
        # 3. Construction du contexte
        context_string = build_context(search_results['chunks'])
        
        # 4. Prompt engineering
        prompt = generate_prompt(
            question=request.question,
            context=context_string,
            query_type=detected_type
        )
        
        # 5. Appel LLM
        llm_response = await call_claude(prompt)
        
        # 6. Post-processing
        formatted_response = format_response(
            raw_response=llm_response['response'],
            context_chunks=search_results['chunks'],
            query_type=detected_type
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return ComprehensiveAnswer(
            answer=formatted_response['answer'],
            confidence=formatted_response['confidence'],
            detected_type=detected_type,
            sources=formatted_response['sources'],
            related_questions=formatted_response['related_questions'],
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Infrastructure Recommand√©e

### H√©bergement VPS

Pour un d√©ploiement en production, un VPS Debian offre le meilleur compromis performance/co√ªt/contr√¥le.

**Sp√©cifications recommand√©es** :

| Composant | Minimum | Recomman# Comply by Sepefrei

![Comply Logo](comply_logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks

| Composant | Minimum | Recommand√© | Production |
|-----------|---------|------------|------------|
| **CPU** | 2 vCores | 4 vCores | 6 vCores |
| **RAM** | 4 GB | 8 GB | 16 GB |
| **Stockage** | 20 GB SSD | 40 GB SSD | 80 GB SSD |
| **Bande passante** | 100 Mbps | 200 Mbps | 1 Gbps |
| **OS** | Debian 11 | Debian 12 | Debian 12 |

**Fournisseurs VPS Recommand√©s (France)** :

**1. Contabo - VPS S SSD** (Recommandation principale)
- **Prix** : ~5,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 200 GB SSD NVMe
- **Localisation** : N√ºrnberg (Allemagne) ou Paris (France)
- **Avantages** : Excellent rapport qualit√©/prix, ressources g√©n√©reuses
- **Lien** : [https://contabo.com/en/vps/](https://contabo.com/en/vps/)

**2. Hetzner - CX31**
- **Prix** : ~9,50‚Ç¨/mois
- **Config** : 2 vCores, 8 GB RAM, 80 GB SSD
- **Localisation** : Falkenstein ou Helsinki
- **Avantages** : Infrastructure fiable, excellente connectivit√©
- **Lien** : [https://www.hetzner.com/cloud](https://www.hetzner.com/cloud)

**3. OVH - VPS Comfort**
- **Prix** : ~11,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 160 GB SSD
- **Localisation** : Gravelines, Roubaix, Strasbourg (France)
- **Avantages** : Fran√ßais, support fran√ßais, infrastructure r√©siliente
- **Lien** : [https://www.ovhcloud.com/fr/vps/](https://www.ovhcloud.com/fr/vps/)

**4. Scaleway - DEV1-M**
- **Prix** : ~7,99‚Ç¨/mois
- **Config** : 3 vCores, 4 GB RAM, 40 GB SSD
- **Localisation** : Paris, Amsterdam
- **Avantages** : √âcosyst√®me cloud complet, IPv6 natif
- **Lien** : [https://www.scaleway.com/en/pricing/](https://www.scaleway.com/en/pricing/)

**Notre choix pour Junior-Entreprises** : **Contabo VPS S SSD**
- Meilleur compromis co√ªt/performance pour usage Comply
- Ressources largement suffisantes (8 GB RAM = confortable pour l'index)
- Co√ªt mensuel accessible pour budget JE (~72‚Ç¨/an)

### Architecture R√©seau et S√©curit√©

**Configuration pare-feu (UFW)** :
```bash
# Installation UFW
apt install ufw -y

# Configuration par d√©faut
ufw default deny incoming
ufw default allow outgoing

# Autorisation SSH (changez 22 si port custom)
ufw allow 22/tcp

# Autorisation HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# Activation
ufw enable

# V√©rification
ufw status verbose
```

**Configuration SSH s√©curis√©e** (`/etc/ssh/sshd_config`) :
```bash
# D√©sactivation login root
PermitRootLogin no

# Authentification par cl√© uniquement
PasswordAuthentication no
PubkeyAuthentication yes

# D√©sactivation X11 forwarding
X11Forwarding no

# Port custom (optionnel, s√©curit√© par obscurit√©)
Port 2222

# Red√©marrage SSH
systemctl restart sshd
```

**Reverse Proxy Nginx** :
```nginx
# /etc/nginx/sites-available/comply

upstream comply_backend {
    server 127.0.0.1:8000;
    keepalive 32;
}

server {
    listen 80;
    server_name comply.votre-je.fr;
    
    # Redirection HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name comply.votre-je.fr;
    
    # Certificats Let's Encrypt
    ssl_certificate /etc/letsencrypt/live/comply.votre-je.fr/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/comply.votre-je.fr/privkey.pem;
    
    # Configuration SSL moderne
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    
    # Headers de s√©curit√©
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    
    # Logs
    access_log /var/log/nginx/comply_access.log;
    error_log /var/log/nginx/comply_error.log;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=comply_limit:10m rate=10r/s;
    limit_req zone=comply_limit burst=20 nodelay;
    
    # Proxy vers FastAPI
    location / {
        proxy_pass http://comply_backend;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint (pas de rate limit)
    location /health {
        limit_req off;
        proxy_pass http://comply_backend;
    }
}
```

**Certificat SSL Let's Encrypt (gratuit)** :
```bash
# Installation Certbot
apt install certbot python3-certbot-nginx -y

# G√©n√©ration certificat
certbot --nginx -d comply.votre-je.fr

# Renouvellement automatique (cron)
echo "0 3 * * * certbot renew --quiet" | crontab -
```

---

## Pr√©requis Serveur

### Installation de l'Environnement

**Script d'installation compl√®te** :
```bash
#!/bin/bash
# install_comply_environment.sh

set -e

echo "=== COMPLY - Installation de l'environnement ==="

# Mise √† jour syst√®me
echo "[1/8] Mise √† jour du syst√®me..."
apt update && apt upgrade -y

# Installation Python 3.11
echo "[2/8] Installation Python 3.11..."
apt install -y software-properties-common
add-apt-repository ppa:deadsnakes/ppa -y
apt update
apt install -y python3.11 python3.11-venv python3.11-dev python3-pip

# V√©rification Python
python3.11 --version

# Installation Git
echo "[3/8] Installation Git..."
apt install -y git

# Installation Docker (optionnel)
echo "[4/8] Installation Docker..."
apt install -y docker.io docker-compose
systemctl enable docker
systemctl start docker

# Installation d√©pendances syst√®me pour Selenium
echo "[5/8] Installation d√©pendances Selenium..."
apt install -y chromium-browser chromium-chromedriver
apt install -y xvfb  # X Virtual Framebuffer pour headless

# Installation Nginx
echo "[6/8] Installation Nginx..."
apt install -y nginx
systemctl enable nginx

# Installation Certbot
echo "[7/8] Installation Certbot..."
apt install -y certbot python3-certbot-nginx

# Cr√©ation utilisateur d√©di√©
echo "[8/8] Cr√©ation utilisateur comply..."
useradd -m -s /bin/bash comply
usermod -aG sudo comply

echo "=== Installation termin√©e ==="
echo "Prochaine √©tape: Cloner le repository et installer les d√©pendances Python"
```

**Ex√©cution** :
```bash
chmod +x install_comply_environment.sh
sudo ./install_comply_environment.sh
```

### Configuration de l'Application

**Clonage du repository** :
```bash
# Connexion en tant qu'utilisateur comply
su - comply

# Clonage
git clone https://github.com/sepefrei/comply.git
cd comply

# Cr√©ation environnement virtuel
python3.11 -m venv venv
source venv/bin/activate

# Installation d√©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

**Fichier requirements.txt** :
```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Machine Learning & NLP
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3

# LLM
anthropic==0.7.8

# Scraping
selenium==4.15.2
beautifulsoup4==4.12.2
lxml==4.9.3

# Utils
python-dotenv==1.0.0
tenacity==8.2.3
pyyaml==6.0.1

# Logging & Monitoring
loguru==0.7.2

# Testing (dev)
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

**Configuration .env** :
```bash
# Copie du template
cp .env.example .env

# √âdition
nano .env
```

Contenu `.env` :
```bash
# Environment
ENVIRONMENT=production

# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Application
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=4

# Index Configuration
INDEX_FILE_PATH=/home/comply/comply/data/index/kiwi_advanced_index.pkl
MAX_CHUNKS_CONTEXT=10
DEFAULT_TOP_K=10

# LLM Configuration
LLM_MODEL=claude-sonnet-4-5-20250929
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.3

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/comply/app.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Security
ALLOWED_ORIGINS=https://comply.votre-je.fr,https://votre-je.slack.com
API_KEY_ENABLED=false
# API_KEY=your-secret-api-key

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
```

### Service systemd

**Cr√©ation du service** (`/etc/systemd/system/comply.service`) :
```ini
[Unit]
Description=Comply - AI Assistant for Junior-Entreprises
After=network.target

[Service]
Type=simple
User=comply
Group=comply
WorkingDirectory=/home/comply/comply
Environment="PATH=/home/comply/comply/venv/bin"

ExecStart=/home/comply/comply/venv/bin/uvicorn main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --log-level info \
    --access-log \
    --use-colors

Restart=always
RestartSec=5

# Limites ressources
LimitNOFILE=65536
LimitNPROC=4096
MemoryLimit=12G
CPUQuota=400%

# S√©curit√©
PrivateTmp=true
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/comply/comply/data /var/log/comply

[Install]
WantedBy=multi-user.target
```

**Activation et d√©marrage** :
```bash
# Rechargement systemd
sudo systemctl daemon-reload

# Activation au d√©marrage
sudo systemctl enable comply

# D√©marrage du service
sudo systemctl start comply

# V√©rification du statut
sudo systemctl status comply

# Logs en temps r√©el
sudo journalctl -u comply -f
```

### Logging Avanc√©

**Configuration Loguru** :
```python
from loguru import logger
import sys

# Configuration des logs
logger.remove()  # Supprime le handler par d√©faut

# Console (stdout)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
    level="INFO",
    colorize=True
)

# Fichier de logs rotatifs
logger.add(
    "/var/log/comply/app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}",
    level="INFO",
    rotation="10 MB",
    retention="30 days",
    compression="zip"
)

# Fichier d'erreurs s√©par√©
logger.add(
    "/var/log/comply/errors.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}\n{exception}",
    level="ERROR",
    rotation="5 MB",
    retention="60 days",
    backtrace=True,
    diagnose=True
)
```

**Utilisation dans le code** :
```python
# Logs structur√©s
logger.info("Index loaded", version=INDEX['version'], chunks=INDEX['statistics']['n_chunks'])

# Logs de requ√™tes
logger.info("Processing question", 
    question=request.question[:50],
    detected_type=detected_type,
    user_id=user_context.get('id')
)

# Logs d'erreurs avec contexte
try:
    result = vector_search(query)
except Exception as e:
    logger.error("Vector search failed", 
        query=query,
        error=str(e),
        exc_info=True
    )
    raise
```

---

## Roadmap Technique

### Court Terme (Q1-Q2 2025)

#### 1. Automatisation Compl√®te du Scraping

**Objectif** : Supprimer l'intervention humaine du processus de mise √† jour des donn√©es.

**Impl√©mentation** :
```python
# cron_scraper.py
import schedule
import time
from scrapers.kiwi_scraper import KiwiScraper
from utils.diff_detector import DiffDetector

def scheduled_scrape_job():
    """Job de scraping diff√©rentiel automatique"""
    logger.info("Starting scheduled scrape job")
    
    scraper = KiwiScraper()
    diff_detector = DiffDetector()
    
    # Scraping des 3 sources
    sources = ['legal', 'rse', 'faq']
    changes_detected = False
    
    for source in sources:
        logger.info(f"Scraping {source}...")
        new_data = scraper.scrape(source)
        
        # D√©tection de changements (hash comparison)
        has_changes = diff_detector.compare(
            source,
            new_data,
            f'data/raw/{source}_latest.json'
        )
        
        if has_changes:
            logger.info(f"Changes detected in {source}")
            changes_detected = True
            
            # Sauvegarde nouvelle version
            save_json(new_data, f'data/raw/{source}_{date.today()}.json')
            save_json(new_data, f'data/raw/{source}_latest.json')
    
    # Si changements d√©tect√©s ‚Üí r√©indexation automatique
    if changes_detected:
        logger.info("Triggering automatic reindexation")
        trigger_reindex()
        
        # Notification Slack
        send_slack_notification(
            "üîÑ Comply index updated",
            f"New data scraped and indexed. {len(sources)} sources updated."
        )

# Planification : tous les jours √† 3h du matin
schedule.every().day.at("03:00").do(scheduled_scrape_job)

if __name__ == "__main__":
    logger.info("Cron scraper started")
    while True:
        schedule.run_pending()
        time.sleep(60)
```

**Configuration cron syst√®me** :
```bash
# Ajout au crontab de l'utilisateur comply
crontab -e

# Ajout de la ligne
0 3 * * * /home/comply/comply/venv/bin/python /home/comply/comply/cron_scraper.py >> /var/log/comply/cron.log 2>&1
```

**D√©tection diff√©rentielle** :
```python
class DiffDetector:
    def compare(self, source_name, new_data, old_file_path):
        """Compare new data with previous version"""
        if not os.path.exists(old_file_path):
            return True  # Premier scraping
        
        with open(old_file_path, 'r') as f:
            old_data = json.load(f)
        
        # Calcul hash du contenu
        new_hash = self._compute_hash(new_data)
        old_hash = self._compute_hash(old_data)
        
        if new_hash != old_hash:
            # Analyse d√©taill√©e des diff√©rences
            diff_stats = self._compute_diff_stats(old_data, new_data)
            logger.info(f"Diff stats for {source_name}", **diff_stats)
            return True
        
        return False
    
    def _compute_hash(self, data):
        """Compute SHA256 hash of data"""
        import hashlib
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compute_diff_stats(self, old, new):
        """Compute detailed diff statistics"""
        # Logique sp√©cifique selon la structure
        return {
            'added': 0,
            'modified': 0,
            'deleted': 0
        }
```

**R√©indexation incr√©mentale** :
```python
def incremental_reindex(changed_sources):
    """Reindex only modified sources"""
    logger.info(f"Starting incremental reindex for: {changed_sources}")
    
    # Chargement de l'index actuel
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        current_index = pickle.load(f)
    
    # Suppression des chunks des sources modifi√©es
    chunks_to_keep = [
        chunk for chunk in current_index['chunks']
        if chunk['metadata']['source_file'] not in changed_sources
    ]
    
    # Ajout des nouveaux chunks
    for source in changed_sources:
        new_chunks = process_source(source)
        chunks_to_keep.extend(new_chunks)
    
    # R√©indexation compl√®te (vectorisation)
    builder = IndexBuilder()
    new_index = builder.build_index(chunks_to_keep)
    
    # Swap atomique
    backup_index(current_index)
    builder.save_index(new_index)
    
    logger.info("Incremental reindex completed")
```

#### 2. Monitoring et Observabilit√©

**Prometheus metrics** :
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# M√©triques
queries_total = Counter('comply_queries_total', 'Total number of queries', ['type'])
query_duration = Histogram('comply_query_duration_seconds', 'Query duration')
llm_calls_total = Counter('comply_llm_calls_total', 'Total LLM API calls')
llm_tokens_used = Counter('comply_llm_tokens_used', 'LLM tokens consumed', ['type'])
index_size = Gauge('comply_index_size_chunks', 'Number of chunks in index')

# Dans le code
@query_duration.time()
async def ask_question(request):
    queries_total.labels(type=detected_type).inc()
    # ... traitement
    llm_calls_total.inc()
    llm_tokens_used.labels(type='input').inc(usage['input_tokens'])
    llm_tokens_used.labels(type='output').inc(usage['output_tokens'])
```

**Dashboard Grafana** :
- Graphique : Requ√™tes/heure par type
- Graphique : Latence p50, p95, p99
- Graphique : Co√ªt LLM journalier (tokens √ó prix)
- Gauge : Taille de l'index
- Alerte : Latence > 5s
- Alerte : Taux d'erreur > 5%

#### 3. Cache Redis pour Performance

**Impl√©mentation** :
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cached_query(query, ttl=3600):
    """Cache les r√©ponses fr√©quentes"""
    # G√©n√©ration cl√© cache
    query_hash = hashlib.md5(query.encode()).hexdigest()
    cache_key = f"comply:query:{query_hash}"
    
    # Tentative de r√©cup√©ration du cache
    cached = redis_client.get(cache_key)
    if cached:
        logger.info("Cache hit", query=query[:50])
        return json.loads(cached)
    
    # Sinon, traitement normal
    result = process_query(query)
    
    # Mise en cache
    redis_client.setex(
        cache_key,
        ttl,
        json.dumps(result)
    )
    
    return result
```

**Strat√©gie de cache** :
- TTL court (1h) pour questions volatiles
- TTL long (24h) pour FAQ stables
- Invalidation sur r√©indexation
- Cache warming des top 100 questions

### Moyen Terme (Q3-Q4 2025)

#### 1. Migration vers Embeddings Denses

**Objectif** : Am√©liorer la pr√©cision s√©mantique avec des embeddings transformers.

**Impl√©mentation** :
```python
from sentence_transformers import SentenceTransformer

class DenseEmbeddingIndexer:
    def __init__(self):
        # Mod√®le fran√ßais optimis√©
        self.model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')
    
    def encode_chunks(self, chunks):
        texts = [chunk['text'] for chunk in chunks]
        
        # Encoding en batch
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        return embeddings  # Shape: (n_chunks, 1024)
```

**Migration FAISS** :
```python
import faiss

class FAISSIndex:
    def __init__(self, dimension=1024):
        # Index IVF avec quantization
        quantizer = faiss.IndexFlatIP(dimension)  # Inner Product
        self.index = faiss.IndexIVFPQ(
            quantizer,
            dimension,
            nlist=100,  # Nombre de clusters
            m=8,  # Sous-quantizers
            8  # Bits par sous-quantizer
        )
    
    def build(self, embeddings):
        # Entra√Ænement de l'index
        self.index.train(embeddings)
        self.index.add(embeddings)
        
        # Nombre de clusters √† visiter lors de la recherche
        self.index.nprobe = 10
    
    def search(self, query_embedding, k=10):
        distances, indices = self.index.search(query_embedding, k)
        return indices[0], distances[0]
```

**Performance attendue** :
- Pr√©cision : +15-20% (top-5 recall)
- Latence : ~20-30ms (vs 11ms TF-IDF)
- M√©moire : ~800 MB (vs 300 MB)

#### 2. Fine-Tuning Embeddings

**Dataset custom JE** :
```python
# G√©n√©ration de paires positives/n√©gatives
training_data = [
    {
        'query': "Comment d√©clarer la TVA ?",
        'positive': "Les Junior-Entreprises b√©n√©ficient du r√©gime de franchise...",
        'negative': "Pour organiser un √©v√©nement RSE..."
    },
    # ... 10k+ exemples
]

# Fine-tuning avec Sentence Transformers
from sentence_transformers import losses, InputExample

train_examples = [
    InputExample(texts=[item['query'], item['positive']])
    for item in training_data
]

model.fit(
    train_objectives=[(train_dataloader, losses.MultipleNegativesRankingLoss(model))],
    epochs=3,
    warmup_steps=100
)
```

#### 3. Multi-LLM Support

**Abstraction provider** :
```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> dict:
        pass

class ClaudeProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Claude
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation GPT-4
        pass

class MistralProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Mistral
        pass

# Factory
def get_llm_provider(provider_name: str) -> LLMProvider:
    providers = {
        'claude': ClaudeProvider(),
        'openai': OpenAIProvider(),
        'mistral': MistralProvider()
    }
    return providers[provider_name]
```

**Routing intelligent** :
```python
def route_query_to_llm(query_type, complexity):
    """S√©lection du LLM optimal selon le contexte"""
    if query_type == 'juridique' and complexity == 'high':
        return 'claude'  # Meilleur sur le raisonnement complexe
    elif query_type == 'faq' and complexity == 'low':
        return 'mistral'  # Rapide et √©conomique
    else:
        return 'claude'  # Default
```

#### 4. Feedback Loop et Active Learning

**Collecte de feedback** :
```python
class FeedbackCollector:
    def record_feedback(self, query_id, feedback_type, user_comment=None):
        """Enregistre le feedback utilisateur"""
        feedback_data = {
            'query_id': query_id,
            'timestamp': datetime.now().isoformat(),
            'feedback_type': feedback_type,  # 'positive', 'negative', 'neutral'
            'user_comment': user_comment
        }
        
        # Stockage
        save_to_database(feedback_data)
        
        # Si feedback n√©gatif ‚Üí investigation
        if feedback_type == 'negative':
            self.analyze_failure(query_id)
```

**R√©entra√Ænement p√©riodique** :
```python
def monthly_retraining():
    """R√©entra√Ænement mensuel avec les feedbacks"""
    # R√©cup√©ration des feedbacks
    feedbacks = load_feedbacks(last_30_days=True)
    
    # G√©n√©ration de nouveaux exemples d'entra√Ænement
    new_training_data = []
    for feedback in feedbacks:
        if feedback['type'] == 'negative':
            # Analyse de la requ√™te √©chou√©e
            query = get_query(feedback['query_id'])
            correct_chunks = identify_correct_chunks(query, feedback['comment'])
            
            new_training_data.append({
                'query': query,
                'positive': correct_chunks,
                'negative': query['retrieved_chunks']
            })
    
    # Fine-tuning incr√©mental
    if len(new_training_data) > 100:
        fine_tune_model(new_training_data)
        logger.info(f"Model fine-tuned with {len(new_training_data)} examples")
```

### Long Terme (2026+)

#### 1. Multimodalit√©

**Support documents PDF/Images** :
```python
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

class MultimodalProcessor:
    def process_pdf(self, pdf_path):
        """Extraction texte + images d'un PDF"""
        # Conversion PDF ‚Üí images
        images = convert_from_path(pdf_path)
        
        extracted_text = ""
        for image in images:
            # OCR
            text = pytesseract.image_to_string(image, lang='fra')
            extracted_text += text + "\n\n"
        
        return extracted_text
    
    def process_image(self, image_path):
        """Extraction texte d'une image"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='fra')
        return text
```

**Vision LLM pour tableaux complexes** :
```python
async def analyze_table_with_vision(image):
    """Utilise GPT-4 Vision ou Claude pour analyser un tableau"""
    response = await vision_llm.analyze(
        image=image,
        prompt="Extrait les donn√©es de ce tableau sous forme JSON structur√©"
    )
    return response
```

#### 2. G√©n√©ration de Documents

**Templates Jinja2** :
```python
from jinja2 import Template

def generate_contract(template_name, context):
    """G√©n√©ration de contrat personnalis√©"""
    template = load_template(f"templates/{template_name}.j2")
    
    # Enrichissement du contexte via LLM
    enriched_context = llm_enrich_context(context)
    
    # G√©n√©ration
    document = template.render(**enriched_context)
    
    # Conversion Markdown ‚Üí PDF
    pdf = convert_md_to_pdf(document)
    
    return pdf
```

**Exemple** : G√©n√©ration automatique de Convention d'√âtude √† partir d'un brief client.

#### 3. Int√©gration √âtendue

**Plugin Google Workspace** :
- Add-on Google Docs : assistance r√©daction contrat
- Extension Gmail : d√©tection clauses dangereuses emails clients

**Bot Discord**[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks]
    ‚Üì
[Context Building] ‚Üí [Prompt Engineering] ‚Üí [Claude LLM] ‚Üí [Response Formatting]
    ‚Üì
[JSON R√©ponse] ‚Üí [Slack Bot / Web UI / API Client]
```

### Phase 1 : Acquisition des Donn√©es (Scraping)

#### Architecture du Scraping Selenium

Le scraping s'effectue via des scripts Python d√©di√©s par source, utilisant Selenium WebDriver pour g√©rer le JavaScript et les interactions complexes.

**Script principal** : `scrapers/kiwi_scraper.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import json
from datetime import datetime

class KiwiScraper:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def scrape_kiwi_legal(self):
        """Scrape Kiwi Legal documents"""
        base_url = "https://kiwi.cnje.fr/legal"
        self.driver.get(base_url)
        
        # Attente du chargement dynamique
        self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "document-list"))
        )
        
        documents = []
        doc_elements = self.driver.find_elements(By.CLASS_NAME, "document-item")
        
        for element in doc_elements:
            doc_data = self._extract_legal_document(element)
            documents.append(doc_data)
        
        return documents
```

**Gestion de la pagination** :
```python
def scrape_with_pagination(self, url, max_pages=None):
    page = 1
    all_data = []
    
    while True:
        print(f"Scraping page {page}...")
        self.driver.get(f"{url}?page={page}")
        
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "content"))
            )
        except TimeoutException:
            print(f"No more pages after page {page-1}")
            break
        
        page_data = self._extract_page_content()
        if not page_data:
            break
        
        all_data.extend(page_data)
        page += 1
        
        if max_pages and page > max_pages:
            break
    
    return all_data
```

**Gestion des erreurs et retry** :
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def robust_scrape(self, url):
    try:
        self.driver.get(url)
        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        return self._extract_content()
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        raise
```

#### Extraction et Nettoyage HTML

Apr√®s extraction Selenium, parsing avec BeautifulSoup pour nettoyage :

```python
from bs4 import BeautifulSoup
import re

def clean_html_content(raw_html):
    """Nettoyage HTML et extraction texte pertinent"""
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # Suppression √©l√©ments non pertinents
    for element in soup(['script', 'style', 'nav', 'footer', 'header']):
        element.decompose()
    
    # Suppression classes publicitaires
    for ad in soup.find_all(class_=['advertisement', 'popup', 'banner']):
        ad.decompose()
    
    # Extraction texte
    text = soup.get_text(separator='\n', strip=True)
    
    # Nettoyage espaces multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text

def extract_metadata(soup):
    """Extraction m√©tadonn√©es structur√©es"""
    metadata = {}
    
    # Titre
    title_tag = soup.find('h1') or soup.find('title')
    metadata['title'] = title_tag.get_text(strip=True) if title_tag else "Unknown"
    
    # Date publication
    date_tag = soup.find('time') or soup.find(class_='date')
    if date_tag:
        metadata['date'] = date_tag.get('datetime') or date_tag.get_text(strip=True)
    
    # Auteur
    author_tag = soup.find(class_='author') or soup.find(rel='author')
    if author_tag:
        metadata['author'] = author_tag.get_text(strip=True)
    
    # Cat√©gorie
    category_tag = soup.find(class_='category')
    if category_tag:
        metadata['category'] = category_tag.get_text(strip=True)
    
    return metadata
```

#### Structure JSON Standardis√©e

Export dans un format JSON unifi√© facilitant le traitement ult√©rieur :

**Format Legal** :
```json
{
  "source": "kiwi_legal",
  "document_type": "statuts",
  "scraping_metadata": {
    "url": "https://kiwi.cnje.fr/legal/statuts-types-association",
    "date_scraped": "2025-01-15T10:30:00Z",
    "scraper_version": "2.1.0"
  },
  "metadata": {
    "title": "Statuts types Junior-Entreprise association loi 1901",
    "category": "juridique",
    "subcategory": "statuts",
    "publication_date": "2024-06-01",
    "author": "Commission Juridique CNJE"
  },
  "content": {
    "sections": [
      {
        "title": "TITRE I - Dispositions g√©n√©rales",
        "articles": [
          {
            "number": 1,
            "title": "D√©nomination",
            "content": "Il est fond√© entre les adh√©rents aux pr√©sents statuts..."
          }
        ]
      }
    ],
    "full_text": "STATUTS TYPES..."
  }
}
```

**Format RSE** :
```json
{
  "source": "kiwi_rse",
  "document_type": "module_rse",
  "scraping_metadata": {...},
  "metadata": {
    "title": "Module Environnement - Gestion des D√©chets",
    "pilier": "environnemental",
    "odd_concernes": [12, 13],
    "niveau_difficulte": "d√©butant"
  },
  "content": {
    "introduction": "La gestion des d√©chets...",
    "objectifs": ["R√©duire la production", "Recycler"],
    "actions": [
      {
        "titre": "Mise en place du tri s√©lectif",
        "description": "...",
        "indicateurs": ["Taux de recyclage", "Volume d√©chets"]
      }
    ]
  }
}
```

**Format FAQ** :
```json
{
  "source": "kiwi_faq",
  "document_type": "faq",
  "scraping_metadata": {...},
  "metadata": {
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2
  },
  "content": {
    "questions": [
      {
        "id": "compta_tva_001",
        "question": "Comment d√©clarer la TVA en tant que JE ?",
        "reponse": "Les Junior-Entreprises b√©n√©ficient...",
        "tags": ["tva", "d√©claration", "comptabilit√©"],
        "related_questions": ["compta_tva_002", "compta_tva_005"]
      }
    ]
  }
}
```

#### Stockage et Versioning

**Arborescence de stockage** :
```
data/
‚îú‚îÄ‚îÄ raw/                          # Donn√©es brutes apr√®s scraping
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_2025-01-15.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_2025-01-15.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_2025-01-15.json
‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_processed.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_processed.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_processed.json
‚îú‚îÄ‚îÄ index/                        # Index g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îî‚îÄ‚îÄ logs/                         # Logs de scraping
    ‚îî‚îÄ‚îÄ scraping_2025-01-15.log
```

**Logging d√©taill√©** :
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scraping_{datetime.now().date()}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Dans le scraper
logger.info(f"Starting scrape of {url}")
logger.info(f"Extracted {len(documents)} documents")
logger.warning(f"Failed to extract metadata for document {doc_id}")
logger.error(f"Scraping failed: {exception}")
```

### Phase 2 : Preprocessing & Transformation

#### Type Detection Automatique

Algorithme de d√©tection bas√© sur plusieurs signaux :

```python
class DocumentTypeDetector:
    def __init__(self):
        self.type_patterns = {
            'legal': {
                'filename': ['statuts', 'contrat', 'legal', 'juridique'],
                'fields': ['articles', 'sections', 'clauses'],
                'keywords': ['article', 'alin√©a', 'conform√©ment', 'obligation']
            },
            'rse': {
                'filename': ['rse', 'durable', 'environnement'],
                'fields': ['pilier', 'odd', 'actions'],
                'keywords': ['d√©veloppement durable', 'odd', 'responsabilit√©']
            },
            'faq': {
                'filename': ['faq', 'questions'],
                'fields': ['questions', 'reponses'],
                'keywords': ['comment', 'pourquoi', 'qu\'est-ce']
            },
            'je': {
                'filename': ['annuaire', 'je', 'junior'],
                'fields': ['nom', 'ville', 'ecole', 'domaines'],
                'keywords': ['junior-entreprise', '√©cole', 'domaine']
            }
        }
    
    def detect_type(self, document_data, filename):
        scores = {doc_type: 0 for doc_type in self.type_patterns}
        
        # Score filename
        for doc_type, patterns in self.type_patterns.items():
            for pattern in patterns['filename']:
                if pattern in filename.lower():
                    scores[doc_type] += 2
        
        # Score fields pr√©sents
        doc_fields = set(document_data.get('content', {}).keys())
        for doc_type, patterns in self.type_patterns.items():
            matching_fields = doc_fields.intersection(patterns['fields'])
            scores[doc_type] += len(matching_fields) * 3
        
        # Score keywords dans le contenu
        content_text = json.dumps(document_data).lower()
        for doc_type, patterns in self.type_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_text:
                    scores[doc_type] += 1
        
        # S√©lection du type avec le score maximal
        detected_type = max(scores, key=scores.get)
        confidence = scores[detected_type] / sum(scores.values()) if sum(scores.values()) > 0 else 0
        
        return {
            'type': detected_type if confidence > 0.3 else 'general',
            'confidence': confidence,
            'scores': scores
        }
```

#### Extraction Sp√©cialis√©e par Type

**Extracteur Legal** :
```python
class LegalExtractor:
    def extract(self, document):
        extracted_data = []
        
        sections = document['content']['sections']
        for section in sections:
            section_title = section['title']
            
            for article in section.get('articles', []):
                extracted_data.append({
                    'text': f"{article['title']}\n{article['content']}",
                    'type': 'legal',
                    'metadata': {
                        'document_type': document['document_type'],
                        'section': section_title,
                        'article_num': article['number'],
                        'title': article['title']
                    }
                })
        
        return extracted_data
```

**Extracteur FAQ** :
```python
class FAQExtractor:
    def extract(self, document):
        extracted_data = []
        
        category = document['metadata']['category']
        subcategory = document['metadata'].get('subcategory', '')
        level = document['metadata'].get('level', 1)
        
        for qa in document['content']['questions']:
            # Contexte hi√©rarchique
            context_path = f"{category}"
            if subcategory:
                context_path += f" > {subcategory}"
            
            text = f"Question: {qa['question']}\n\nR√©ponse: {qa['reponse']}"
            
            extracted_data.append({
                'text': text,
                'type': 'faq',
                'metadata': {
                    'question': qa['question'],
                    'category': category,
                    'subcategory': subcategory,
                    'level': level,
                    'context_path': context_path,
                    'tags': qa.get('tags', []),
                    'related_questions': qa.get('related_questions', [])
                }
            })
        
        return extracted_data
```

**Extracteur JE** :
```python
class JEExtractor:
    def extract(self, document):
        extracted_data = []
        
        for je in document['content']['junior_entreprises']:
            # Construction texte descriptif
            text = f"""
            Nom: {je['nom']}
            Ville: {je['ville']}
            √âcole: {je['ecole']}
            Domaines d'expertise: {', '.join(je['domaines'])}
            Contact: {je['contact']['email']}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'je',
                'metadata': {
                    'nom': je['nom'],
                    'ville': je['ville'],
                    'ecole': je['ecole'],
                    'domaines': je['domaines'],
                    'contact': je['contact'],
                    'certified': je.get('certified_cnje', False)
                }
            })
        
        return extracted_data
```

**Extracteur RSE** :
```python
class RSEExtractor:
    def extract(self, document):
        extracted_data = []
        
        pilier = document['metadata']['pilier']
        odd = document['metadata']['odd_concernes']
        
        # Extraction par action
        for action in document['content']['actions']:
            text = f"""
            Module RSE: {document['metadata']['title']}
            Pilier: {pilier}
            
            Action: {action['titre']}
            {action['description']}
            
            Indicateurs: {', '.join(action['indicateurs'])}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'rse',
                'metadata': {
                    'module': document['metadata']['title'],
                    'pilier': pilier,
                    'odd': odd,
                    'action_titre': action['titre'],
                    'indicateurs': action['indicateurs']
                }
            })
        
        return extracted_data
```

#### Smart Chunking S√©mantique

Le chunking respecte la logique m√©tier plut√¥t qu'une simple d√©coupe par longueur :

```python
class SemanticChunker:
    def __init__(self, min_length=50, max_length=1000, target_length=300):
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
    
    def chunk_text(self, text, doc_type, metadata):
        if doc_type == 'faq':
            # FAQ: chaque Q/A est un chunk autonome
            return self._chunk_faq(text, metadata)
        elif doc_type == 'legal':
            # Legal: d√©coupage par article/section
            return self._chunk_legal(text, metadata)
        elif doc_type == 'je':
            # JE: entit√© atomique, pas de d√©coupage
            return [self._create_chunk(text, doc_type, metadata)]
        elif doc_type == 'rse':
            # RSE: d√©coupage par action
            return self._chunk_rse(text, metadata)
        else:
            # G√©n√©rique: d√©coupage par paragraphes avec overlap
            return self._chunk_generic(text, doc_type, metadata)
    
    def _chunk_generic(self, text, doc_type, metadata):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_length:
                current_chunk += para + "\n\n"
            else:
                if len(current_chunk) > self.min_length:
                    chunks.append(
                        self._create_chunk(current_chunk.strip(), doc_type, metadata)
                    )
                current_chunk = para + "\n\n"
        
        if len(current_chunk) > self.min_length:
            chunks.append(
                self._create_chunk(current_chunk.strip(), doc_type, metadata)
            )
        
        return chunks
    
    def _create_chunk(self, text, doc_type, metadata):
        return {
            'text': text,
            'type': doc_type,
            'metadata': metadata,
            'length': len(text),
            'word_count': len(text.split())
        }
```

#### Enrichissement M√©tadonn√©es

Chaque chunk est enrichi automatiquement :

```python
class MetadataEnricher:
    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.category_classifier = CategoryClassifier()
    
    def enrich_chunk(self, chunk):
        text = chunk['text']
        
        # Extraction keywords automatique
        keywords = self.keyword_extractor.extract(text, top_n=5)
        chunk['metadata']['keywords'] = keywords
        
        # Classification cat√©gorie fine (si pas d√©j√† pr√©sente)
        if 'category' not in chunk['metadata']:
            category = self.category_classifier.classify(text)
            chunk['metadata']['category'] = category
        
        # Calcul de priorit√© (bas√© sur usage historique si disponible)
        chunk['metadata']['priority'] = self._calculate_priority(chunk)
        
        # Ajout timestamps
        chunk['metadata']['indexed_at'] = datetime.now().isoformat()
        
        # G√©n√©ration d'un hash pour d√©tecter les modifications
        chunk['metadata']['content_hash'] = hashlib.md5(
            text.encode()
        ).hexdigest()
        
        return chunk
    
    def _calculate_priority(self, chunk):
        # Heuristique simple : sources officielles = haute priorit√©
        priority = 0.5
        
        if chunk['type'] == 'legal':
            priority += 0.2
        if 'statuts' in chunk.get('metadata', {}).get('category', '').lower():
            priority += 0.15
        if chunk['metadata'].get('is_featured', False):
            priority += 0.1
        
        return min(priority, 1.0)
```

### Phase 3 : Vectorisation et Indexation

#### Configuration TF-IDF Optimis√©e

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IndexBuilder:
    def __init__(self):
        # Stopwords personnalis√©s JE
        self.custom_stopwords = [
            'junior', 'entreprise', 'je', 'cnje',
            '√©tudiant', '√©tudiante', 'projet', 'mission',
            'conform√©ment', 'article', 'alin√©a', 'paragraphe'
        ]
        
        # Configuration TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8,
            stop_words=self.custom_stopwords,
            sublinear_tf=True,
            norm='l2',
            strip_accents='unicode'
        )
    
    def build_index(self, chunks):
        print(f"Building index from {len(chunks)} chunks...")
        
        # Extraction des textes
        texts = [chunk['text'] for chunk in chunks]
        
        # Vectorisation TF-IDF
        print("Vectorizing with TF-IDF...")
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
        
        # R√©duction dimensionnelle SVD
        print("Applying SVD dimensionality reduction...")
        n_components = min(300, tfidf_matrix.shape[0] - 1)
        svd_model = TruncatedSVD(
            n_components=n_components,
            algorithm='randomized',
            n_iter=7,
            random_state=42
        )
        vectors_reduced = svd_model.fit_transform(tfidf_matrix)
        print(f"Reduced to {n_components} dimensions")
        
        # Construction des index secondaires
        print("Building secondary indexes...")
        metadata_index = self._build_metadata_indexes(chunks)
        
        # Assemblage de l'index complet
        index = {
            'vectorizer': self.vectorizer,
            'svd_model': svd_model,
            'vectors': vectors_reduced,
            'chunks': chunks,
            'metadata_index': metadata_index,
            'version': '2.1.0',
            'build_date': datetime.now().isoformat(),
            'statistics': {
                'n_chunks': len(chunks),
                'n_features': tfidf_matrix.shape[1],
                'n_components': n_components,
                'vocabulary_size': len(self.vectorizer.vocabulary_)
            }
        }
        
        return index
    
    def _build_metadata_indexes(self, chunks):
        by_type = {}
        by_category = {}
        by_source = {}
        
        for idx, chunk in enumerate(chunks):
            # Index by type
            chunk_type = chunk['type']
            if chunk_type not in by_type:
                by_type[chunk_type] = []
            by_type[chunk_type].append(idx)
            
            # Index by category
            category = chunk['metadata'].get('category', 'unknown')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(idx)
            
            # Index by source
            source = chunk['metadata'].get('source_file', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(idx)
        
        return {
            'by_type': by_type,
            'by_category': by_category,
            'by_source': by_source
        }
    
    def save_index(self, index, filepath='data/index/kiwi_advanced_index.pkl'):
        print(f"Saving index to {filepath}...")
        with open(filepath, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"Index saved successfully ({file_size_mb:.2f} MB)")
```

#### Processus Complet d'Indexation

Script principal orchestrant tout le pipeline :

```python
def main_indexation_pipeline():
    print("=== COMPLY INDEXATION PIPELINE ===\n")
    
    # 1. Chargement des donn√©es sources
    print("Step 1: Loading source data...")
    legal_data = load_json('data/processed/kiwi_legal_processed.json')
    rse_data = load_json('data/processed/kiwi_rse_processed.json')
    faq_data = load_json('data/processed/kiwi_faq_processed.json')
    je_data = load_json('data/processed/kiwi_je_processed.json')
    
    all_sources = [
        ('legal', legal_data),
        ('rse', rse_data),
        ('faq', faq_data),
        ('je', je_data)
    ]
    
    # 2. Extraction et chunking
    print("\nStep 2: Extracting and chunking...")
    all_chunks = []
    
    for source_type, data in all_sources:
        extractor = get_extractor(source_type)
        chunks = extractor.extract(data)
        
        # Chunking s√©mantique
        chunker = SemanticChunker()
        chunked_data = []
        for chunk in chunks:
            chunked_data.extend(
                chunker.chunk_text(
                    chunk['text'],
                    chunk['type'],
                    chunk['metadata']
                )
            )
        
        print(f"  - {source_type}: {len(chunked_data)} chunks")
        all_chunks.extend(chunked_data)
    
    print(f"Total chunks: {len(all_chunks)}")
    
    # 3. Enrichissement
    print("\nStep 3: Enriching metadata...")
    enricher = MetadataEnricher()
    enriched_chunks = [enricher.enrich_chunk(c) for c in all_chunks]
    
    # 4. Construction de l'index
    print("\nStep 4: Building vector index...")
    builder = IndexBuilder()
    index = builder.build_index(enriched_chunks)
    
    # 5. Persistance
    print("\nStep 5: Saving index...")
    builder.save_index(index)
    
    # 6. Statistiques finales
    print("\n=== INDEXATION COMPLETE ===")
    print(f"Total chunks indexed: {index['statistics']['n_chunks']}")
    print(f"Vocabulary size: {index['statistics']['vocabulary_size']}")
    print(f"Vector dimensions: {index['statistics']['n_components']}")
    print(f"Index version: {index['version']}")
    
    return index

if __name__ == "__main__":
    main_indexation_pipeline()
```

### Phase 4 : Serving et Recherche

#### Chargement de l'Index au D√©marrage

```python
from fastapi import FastAPI
import pickle

app = FastAPI(title="Comply API", version="2.1.0")

# Chargement de l'index au d√©marrage (√©v√©nement startup)
@app.on_event("startup")
async def load_index():
    global INDEX
    
    print("Loading Comply index...")
    start_time = time.time()
    
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        INDEX = pickle.load(f)
    
    load_time = time.time() - start_time
    print(f"Index loaded in {load_time:.2f}s")
    print(f"  - Version: {INDEX['version']}")
    print(f"  - Chunks: {INDEX['statistics']['n_chunks']}")
    print(f"  - Memory: {sys.getsizeof(INDEX) / (1024**2):.2f} MB")
```

#### Endpoint /ask - Impl√©mentation Compl√®te

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    context: Optional[dict] = None
    options: Optional[dict] = None

class ComprehensiveAnswer(BaseModel):
    answer: str
    confidence: float
    detected_type: str
    sources: List[dict]
    related_questions: List[str]
    processing_time_ms: int

@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    start_time = time.time()
    
    try:
        # 1. D√©tection du type de requ√™te
        query_type_result = detect_query_type(request.question)
        detected_type = query_type_result['detected_type']
        
        # 2. Recherche vectorielle avec boosting
        search_results = vector_search(
            query=request.question,
            query_type=detected_type,
            top_k=request.options.get('max_chunks', 10) if request.options else 10
        )
        
        # 3. Construction du contexte
        context_string = build_context(search_results['chunks'])
        
        # 4. Prompt engineering
        prompt = generate_prompt(
            question=request.question,
            context=context_string,
            query_type=detected_type
        )
        
        # 5. Appel LLM
        llm_response = await call_claude(prompt)
        
        # 6. Post-processing
        formatted_response = format_response(
            raw_response=llm_response['response'],
            context_chunks=search_results['chunks'],
            query_type=detected_type
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return ComprehensiveAnswer(
            answer=formatted_response['answer'],
            confidence=formatted_response['confidence'],
            detected_type=detected_type,
            sources=formatted_response['sources'],
            related_questions=formatted_response['related_questions'],
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Infrastructure Recommand√©e

### H√©bergement VPS

Pour un d√©ploiement en production, un VPS Debian offre le meilleur compromis performance/co√ªt/contr√¥le.

**Sp√©cifications recommand√©es** :

| Composant | Minimum | Recomman# Comply by Sepefrei

![Comply Logo](comply_logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks

**Bot Discord** :
```python
import discord
from discord.ext import commands

class ComplyBot(commands.Bot):
    def __init__(self):
        intents = discord.Intents.default()
        intents.message_content = True
        super().__init__(command_prefix='!comply ', intents=intents)
    
    @commands.command()
    async def ask(self, ctx, *, question):
        """!comply ask Comment d√©clarer la TVA ?"""
        async with ctx.typing():
            response = await call_comply_api(question)
            
            embed = discord.Embed(
                title="üí° Comply",
                description=response['answer'],
                color=discord.Color.blue()
            )
            
            # Ajout des sources
            sources_text = "\n".join([
                f"‚Ä¢ [{s['type']}] {s['source_file']}"
                for s in response['sources'][:3]
            ])
            embed.add_field(name="üìö Sources", value=sources_text, inline=False)
            
            await ctx.send(embed=embed)
```

**Mobile App Native** :
- React Native / Flutter
- Interface conversationnelle
- Mode offline (cache local des FAQ communes)
- Notifications push pour alertes audit/conformit√©

**API Webhooks** :
```python
@router.post("/webhooks/notion")
async def notion_webhook(request: NotionWebhookRequest):
    """Int√©gration Notion : analyse automatique des docs"""
    page_content = request.page_content
    
    # Analyse de conformit√©
    compliance_check = await check_compliance(page_content)
    
    # Update Notion page avec r√©sultats
    update_notion_page(
        page_id=request.page_id,
        compliance_status=compliance_check
    )
```

#### 4. Gouvernance et Audit Trail

**Tra√ßabilit√© compl√®te** :
```python
class AuditLogger:
    def log_query(self, user_id, query, response, context):
        """Log complet de chaque interaction"""
        audit_entry = {
            'id': generate_uuid(),
            'timestamp': datetime.now().isoformat(),
            'user': {
                'id': user_id,
                'role': context.get('role'),
                'je': context.get('je_name')
            },
            'query': {
                'text': query,
                'type': response['detected_type'],
                'hash': hashlib.sha256(query.encode()).hexdigest()
            },
            'response': {
                'answer_preview': response['answer'][:200],
                'confidence': response['confidence'],
                'sources_used': [s['chunk_id'] for s in response['sources']],
                'llm_model': response['metadata']['llm_model'],
                'tokens': {
                    'input': response['metadata']['input_tokens'],
                    'output': response['metadata']['output_tokens']
                }
            },
            'metadata': {
                'ip_address': anonymize_ip(context.get('ip')),
                'user_agent': context.get('user_agent'),| Composant | Minimum | Recommand√© | Production |
|-----------|---------|------------|------------|
| **CPU** | 2 vCores | 4 vCores | 6 vCores |
| **RAM** | 4 GB | 8 GB | 16 GB |
| **Stockage** | 20 GB SSD | 40 GB SSD | 80 GB SSD |
| **Bande passante** | 100 Mbps | 200 Mbps | 1 Gbps |
| **OS** | Debian 11 | Debian 12 | Debian 12 |

**Fournisseurs VPS Recommand√©s (France)** :

**1. Contabo - VPS S SSD** (Recommandation principale)
- **Prix** : ~5,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 200 GB SSD NVMe
- **Localisation** : N√ºrnberg (Allemagne) ou Paris (France)
- **Avantages** : Excellent rapport qualit√©/prix, ressources g√©n√©reuses
- **Lien** : [https://contabo.com/en/vps/](https://contabo.com/en/vps/)

**2. Hetzner - CX31**
- **Prix** : ~9,50‚Ç¨/mois
- **Config** : 2 vCores, 8 GB RAM, 80 GB SSD
- **Localisation** : Falkenstein ou Helsinki
- **Avantages** : Infrastructure fiable, excellente connectivit√©
- **Lien** : [https://www.hetzner.com/cloud](https://www.hetzner.com/cloud)

**3. OVH - VPS Comfort**
- **Prix** : ~11,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 160 GB SSD
- **Localisation** : Gravelines, Roubaix, Strasbourg (France)
- **Avantages** : Fran√ßais, support fran√ßais, infrastructure r√©siliente
- **Lien** : [https://www.ovhcloud.com/fr/vps/](https://www.ovhcloud.com/fr/vps/)

**4. Scaleway - DEV1-M**
- **Prix** : ~7,99‚Ç¨/mois
- **Config** : 3 vCores, 4 GB RAM, 40 GB SSD
- **Localisation** : Paris, Amsterdam
- **Avantages** : √âcosyst√®me cloud complet, IPv6 natif
- **Lien** : [https://www.scaleway.com/en/pricing/](https://www.scaleway.com/en/pricing/)

**Notre choix pour Junior-Entreprises** : **Contabo VPS S SSD**
- Meilleur compromis co√ªt/performance pour usage Comply
- Ressources largement suffisantes (8 GB RAM = confortable pour l'index)
- Co√ªt mensuel accessible pour budget JE (~72‚Ç¨/an)

### Architecture R√©seau et S√©curit√©

**Configuration pare-feu (UFW)** :
```bash
# Installation UFW
apt install ufw -y

# Configuration par d√©faut
ufw default deny incoming
ufw default allow outgoing

# Autorisation SSH (changez 22 si port custom)
ufw allow 22/tcp

# Autorisation HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# Activation
ufw enable

# V√©rification
ufw status verbose
```

**Configuration SSH s√©curis√©e** (`/etc/ssh/sshd_config`) :
```bash
# D√©sactivation login root
PermitRootLogin no

# Authentification par cl√© uniquement
PasswordAuthentication no
PubkeyAuthentication yes

# D√©sactivation X11 forwarding
X11Forwarding no

# Port custom (optionnel, s√©curit√© par obscurit√©)
Port 2222

# Red√©marrage SSH
systemctl restart sshd
```

**Reverse Proxy Nginx** :
```nginx
# /etc/nginx/sites-available/comply

upstream comply_backend {
    server 127.0.0.1:8000;
    keepalive 32;
}

server {
    listen 80;
    server_name comply.votre-je.fr;
    
    # Redirection HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name comply.votre-je.fr;
    
    # Certificats Let's Encrypt
    ssl_certificate /etc/letsencrypt/live/comply.votre-je.fr/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/comply.votre-je.fr/privkey.pem;
    
    # Configuration SSL moderne
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    
    # Headers de s√©curit√©
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    
    # Logs
    access_log /var/log/nginx/comply_access.log;
    error_log /var/log/nginx/comply_error.log;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=comply_limit:10m rate=10r/s;
    limit_req zone=comply_limit burst=20 nodelay;
    
    # Proxy vers FastAPI
    location / {
        proxy_pass http://comply_backend;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint (pas de rate limit)
    location /health {
        limit_req off;
        proxy_pass http://comply_backend;
    }
}
```

**Certificat SSL Let's Encrypt (gratuit)** :
```bash
# Installation Certbot
apt install certbot python3-certbot-nginx -y

# G√©n√©ration certificat
certbot --nginx -d comply.votre-je.fr

# Renouvellement automatique (cron)
echo "0 3 * * * certbot renew --quiet" | crontab -
```

---

## Pr√©requis Serveur

### Installation de l'Environnement

**Script d'installation compl√®te** :
```bash
#!/bin/bash
# install_comply_environment.sh

set -e

echo "=== COMPLY - Installation de l'environnement ==="

# Mise √† jour syst√®me
echo "[1/8] Mise √† jour du syst√®me..."
apt update && apt upgrade -y

# Installation Python 3.11
echo "[2/8] Installation Python 3.11..."
apt install -y software-properties-common
add-apt-repository ppa:deadsnakes/ppa -y
apt update
apt install -y python3.11 python3.11-venv python3.11-dev python3-pip

# V√©rification Python
python3.11 --version

# Installation Git
echo "[3/8] Installation Git..."
apt install -y git

# Installation Docker (optionnel)
echo "[4/8] Installation Docker..."
apt install -y docker.io docker-compose
systemctl enable docker
systemctl start docker

# Installation d√©pendances syst√®me pour Selenium
echo "[5/8] Installation d√©pendances Selenium..."
apt install -y chromium-browser chromium-chromedriver
apt install -y xvfb  # X Virtual Framebuffer pour headless

# Installation Nginx
echo "[6/8] Installation Nginx..."
apt install -y nginx
systemctl enable nginx

# Installation Certbot
echo "[7/8] Installation Certbot..."
apt install -y certbot python3-certbot-nginx

# Cr√©ation utilisateur d√©di√©
echo "[8/8] Cr√©ation utilisateur comply..."
useradd -m -s /bin/bash comply
usermod -aG sudo comply

echo "=== Installation termin√©e ==="
echo "Prochaine √©tape: Cloner le repository et installer les d√©pendances Python"
```

**Ex√©cution** :
```bash
chmod +x install_comply_environment.sh
sudo ./install_comply_environment.sh
```

### Configuration de l'Application

**Clonage du repository** :
```bash
# Connexion en tant qu'utilisateur comply
su - comply

# Clonage
git clone https://github.com/sepefrei/comply.git
cd comply

# Cr√©ation environnement virtuel
python3.11 -m venv venv
source venv/bin/activate

# Installation d√©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

**Fichier requirements.txt** :
```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Machine Learning & NLP
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3

# LLM
anthropic==0.7.8

# Scraping
selenium==4.15.2
beautifulsoup4==4.12.2
lxml==4.9.3

# Utils
python-dotenv==1.0.0
tenacity==8.2.3
pyyaml==6.0.1

# Logging & Monitoring
loguru==0.7.2

# Testing (dev)
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

**Configuration .env** :
```bash
# Copie du template
cp .env.example .env

# √âdition
nano .env
```

Contenu `.env` :
```bash
# Environment
ENVIRONMENT=production

# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Application
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=4

# Index Configuration
INDEX_FILE_PATH=/home/comply/comply/data/index/kiwi_advanced_index.pkl
MAX_CHUNKS_CONTEXT=10
DEFAULT_TOP_K=10

# LLM Configuration
LLM_MODEL=claude-sonnet-4-5-20250929
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.3

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/comply/app.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Security
ALLOWED_ORIGINS=https://comply.votre-je.fr,https://votre-je.slack.com
API_KEY_ENABLED=false
# API_KEY=your-secret-api-key

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
```

### Service systemd

**Cr√©ation du service** (`/etc/systemd/system/comply.service`) :
```ini
[Unit]
Description=Comply - AI Assistant for Junior-Entreprises
After=network.target

[Service]
Type=simple
User=comply
Group=comply
WorkingDirectory=/home/comply/comply
Environment="PATH=/home/comply/comply/venv/bin"

ExecStart=/home/comply/comply/venv/bin/uvicorn main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --log-level info \
    --access-log \
    --use-colors

Restart=always
RestartSec=5

# Limites ressources
LimitNOFILE=65536
LimitNPROC=4096
MemoryLimit=12G
CPUQuota=400%

# S√©curit√©
PrivateTmp=true
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/comply/comply/data /var/log/comply

[Install]
WantedBy=multi-user.target
```

**Activation et d√©marrage** :
```bash
# Rechargement systemd
sudo systemctl daemon-reload

# Activation au d√©marrage
sudo systemctl enable comply

# D√©marrage du service
sudo systemctl start comply

# V√©rification du statut
sudo systemctl status comply

# Logs en temps r√©el
sudo journalctl -u comply -f
```

### Logging Avanc√©

**Configuration Loguru** :
```python
from loguru import logger
import sys

# Configuration des logs
logger.remove()  # Supprime le handler par d√©faut

# Console (stdout)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
    level="INFO",
    colorize=True
)

# Fichier de logs rotatifs
logger.add(
    "/var/log/comply/app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}",
    level="INFO",
    rotation="10 MB",
    retention="30 days",
    compression="zip"
)

# Fichier d'erreurs s√©par√©
logger.add(
    "/var/log/comply/errors.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}\n{exception}",
    level="ERROR",
    rotation="5 MB",
    retention="60 days",
    backtrace=True,
    diagnose=True
)
```

**Utilisation dans le code** :
```python
# Logs structur√©s
logger.info("Index loaded", version=INDEX['version'], chunks=INDEX['statistics']['n_chunks'])

# Logs de requ√™tes
logger.info("Processing question", 
    question=request.question[:50],
    detected_type=detected_type,
    user_id=user_context.get('id')
)

# Logs d'erreurs avec contexte
try:
    result = vector_search(query)
except Exception as e:
    logger.error("Vector search failed", 
        query=query,
        error=str(e),
        exc_info=True
    )
    raise
```

---

## Roadmap Technique

### Court Terme (Q1-Q2 2025)

#### 1. Automatisation Compl√®te du Scraping

**Objectif** : Supprimer l'intervention humaine du processus de mise √† jour des donn√©es.

**Impl√©mentation** :
```python
# cron_scraper.py
import schedule
import time
from scrapers.kiwi_scraper import KiwiScraper
from utils.diff_detector import DiffDetector

def scheduled_scrape_job():
    """Job de scraping diff√©rentiel automatique"""
    logger.info("Starting scheduled scrape job")
    
    scraper = KiwiScraper()
    diff_detector = DiffDetector()
    
    # Scraping des 3 sources
    sources = ['legal', 'rse', 'faq']
    changes_detected = False
    
    for source in sources:
        logger.info(f"Scraping {source}...")
        new_data = scraper.scrape(source)
        
        # D√©tection de changements (hash comparison)
        has_changes = diff_detector.compare(
            source,
            new_data,
            f'data/raw/{source}_latest.json'
        )
        
        if has_changes:
            logger.info(f"Changes detected in {source}")
            changes_detected = True
            
            # Sauvegarde nouvelle version
            save_json(new_data, f'data/raw/{source}_{date.today()}.json')
            save_json(new_data, f'data/raw/{source}_latest.json')
    
    # Si changements d√©tect√©s ‚Üí r√©indexation automatique
    if changes_detected:
        logger.info("Triggering automatic reindexation")
        trigger_reindex()
        
        # Notification Slack
        send_slack_notification(
            "üîÑ Comply index updated",
            f"New data scraped and indexed. {len(sources)} sources updated."
        )

# Planification : tous les jours √† 3h du matin
schedule.every().day.at("03:00").do(scheduled_scrape_job)

if __name__ == "__main__":
    logger.info("Cron scraper started")
    while True:
        schedule.run_pending()
        time.sleep(60)
```

**Configuration cron syst√®me** :
```bash
# Ajout au crontab de l'utilisateur comply
crontab -e

# Ajout de la ligne
0 3 * * * /home/comply/comply/venv/bin/python /home/comply/comply/cron_scraper.py >> /var/log/comply/cron.log 2>&1
```

**D√©tection diff√©rentielle** :
```python
class DiffDetector:
    def compare(self, source_name, new_data, old_file_path):
        """Compare new data with previous version"""
        if not os.path.exists(old_file_path):
            return True  # Premier scraping
        
        with open(old_file_path, 'r') as f:
            old_data = json.load(f)
        
        # Calcul hash du contenu
        new_hash = self._compute_hash(new_data)
        old_hash = self._compute_hash(old_data)
        
        if new_hash != old_hash:
            # Analyse d√©taill√©e des diff√©rences
            diff_stats = self._compute_diff_stats(old_data, new_data)
            logger.info(f"Diff stats for {source_name}", **diff_stats)
            return True
        
        return False
    
    def _compute_hash(self, data):
        """Compute SHA256 hash of data"""
        import hashlib
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compute_diff_stats(self, old, new):
        """Compute detailed diff statistics"""
        # Logique sp√©cifique selon la structure
        return {
            'added': 0,
            'modified': 0,
            'deleted': 0
        }
```

**R√©indexation incr√©mentale** :
```python
def incremental_reindex(changed_sources):
    """Reindex only modified sources"""
    logger.info(f"Starting incremental reindex for: {changed_sources}")
    
    # Chargement de l'index actuel
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        current_index = pickle.load(f)
    
    # Suppression des chunks des sources modifi√©es
    chunks_to_keep = [
        chunk for chunk in current_index['chunks']
        if chunk['metadata']['source_file'] not in changed_sources
    ]
    
    # Ajout des nouveaux chunks
    for source in changed_sources:
        new_chunks = process_source(source)
        chunks_to_keep.extend(new_chunks)
    
    # R√©indexation compl√®te (vectorisation)
    builder = IndexBuilder()
    new_index = builder.build_index(chunks_to_keep)
    
    # Swap atomique
    backup_index(current_index)
    builder.save_index(new_index)
    
    logger.info("Incremental reindex completed")
```

#### 2. Monitoring et Observabilit√©

**Prometheus metrics** :
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# M√©triques
queries_total = Counter('comply_queries_total', 'Total number of queries', ['type'])
query_duration = Histogram('comply_query_duration_seconds', 'Query duration')
llm_calls_total = Counter('comply_llm_calls_total', 'Total LLM API calls')
llm_tokens_used = Counter('comply_llm_tokens_used', 'LLM tokens consumed', ['type'])
index_size = Gauge('comply_index_size_chunks', 'Number of chunks in index')

# Dans le code
@query_duration.time()
async def ask_question(request):
    queries_total.labels(type=detected_type).inc()
    # ... traitement
    llm_calls_total.inc()
    llm_tokens_used.labels(type='input').inc(usage['input_tokens'])
    llm_tokens_used.labels(type='output').inc(usage['output_tokens'])
```

**Dashboard Grafana** :
- Graphique : Requ√™tes/heure par type
- Graphique : Latence p50, p95, p99
- Graphique : Co√ªt LLM journalier (tokens √ó prix)
- Gauge : Taille de l'index
- Alerte : Latence > 5s
- Alerte : Taux d'erreur > 5%

#### 3. Cache Redis pour Performance

**Impl√©mentation** :
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cached_query(query, ttl=3600):
    """Cache les r√©ponses fr√©quentes"""
    # G√©n√©ration cl√© cache
    query_hash = hashlib.md5(query.encode()).hexdigest()
    cache_key = f"comply:query:{query_hash}"
    
    # Tentative de r√©cup√©ration du cache
    cached = redis_client.get(cache_key)
    if cached:
        logger.info("Cache hit", query=query[:50])
        return json.loads(cached)
    
    # Sinon, traitement normal
    result = process_query(query)
    
    # Mise en cache
    redis_client.setex(
        cache_key,
        ttl,
        json.dumps(result)
    )
    
    return result
```

**Strat√©gie de cache** :
- TTL court (1h) pour questions volatiles
- TTL long (24h) pour FAQ stables
- Invalidation sur r√©indexation
- Cache warming des top 100 questions

### Moyen Terme (Q3-Q4 2025)

#### 1. Migration vers Embeddings Denses

**Objectif** : Am√©liorer la pr√©cision s√©mantique avec des embeddings transformers.

**Impl√©mentation** :
```python
from sentence_transformers import SentenceTransformer

class DenseEmbeddingIndexer:
    def __init__(self):
        # Mod√®le fran√ßais optimis√©
        self.model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')
    
    def encode_chunks(self, chunks):
        texts = [chunk['text'] for chunk in chunks]
        
        # Encoding en batch
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        return embeddings  # Shape: (n_chunks, 1024)
```

**Migration FAISS** :
```python
import faiss

class FAISSIndex:
    def __init__(self, dimension=1024):
        # Index IVF avec quantization
        quantizer = faiss.IndexFlatIP(dimension)  # Inner Product
        self.index = faiss.IndexIVFPQ(
            quantizer,
            dimension,
            nlist=100,  # Nombre de clusters
            m=8,  # Sous-quantizers
            8  # Bits par sous-quantizer
        )
    
    def build(self, embeddings):
        # Entra√Ænement de l'index
        self.index.train(embeddings)
        self.index.add(embeddings)
        
        # Nombre de clusters √† visiter lors de la recherche
        self.index.nprobe = 10
    
    def search(self, query_embedding, k=10):
        distances, indices = self.index.search(query_embedding, k)
        return indices[0], distances[0]
```

**Performance attendue** :
- Pr√©cision : +15-20% (top-5 recall)
- Latence : ~20-30ms (vs 11ms TF-IDF)
- M√©moire : ~800 MB (vs 300 MB)

#### 2. Fine-Tuning Embeddings

**Dataset custom JE** :
```python
# G√©n√©ration de paires positives/n√©gatives
training_data = [
    {
        'query': "Comment d√©clarer la TVA ?",
        'positive': "Les Junior-Entreprises b√©n√©ficient du r√©gime de franchise...",
        'negative': "Pour organiser un √©v√©nement RSE..."
    },
    # ... 10k+ exemples
]

# Fine-tuning avec Sentence Transformers
from sentence_transformers import losses, InputExample

train_examples = [
    InputExample(texts=[item['query'], item['positive']])
    for item in training_data
]

model.fit(
    train_objectives=[(train_dataloader, losses.MultipleNegativesRankingLoss(model))],
    epochs=3,
    warmup_steps=100
)
```

#### 3. Multi-LLM Support

**Abstraction provider** :
```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> dict:
        pass

class ClaudeProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Claude
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation GPT-4
        pass

class MistralProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Mistral
        pass

# Factory
def get_llm_provider(provider_name: str) -> LLMProvider:
    providers = {
        'claude': ClaudeProvider(),
        'openai': OpenAIProvider(),
        'mistral': MistralProvider()
    }
    return providers[provider_name]
```

**Routing intelligent** :
```python
def route_query_to_llm(query_type, complexity):
    """S√©lection du LLM optimal selon le contexte"""
    if query_type == 'juridique' and complexity == 'high':
        return 'claude'  # Meilleur sur le raisonnement complexe
    elif query_type == 'faq' and complexity == 'low':
        return 'mistral'  # Rapide et √©conomique
    else:
        return 'claude'  # Default
```

#### 4. Feedback Loop et Active Learning

**Collecte de feedback** :
```python
class FeedbackCollector:
    def record_feedback(self, query_id, feedback_type, user_comment=None):
        """Enregistre le feedback utilisateur"""
        feedback_data = {
            'query_id': query_id,
            'timestamp': datetime.now().isoformat(),
            'feedback_type': feedback_type,  # 'positive', 'negative', 'neutral'
            'user_comment': user_comment
        }
        
        # Stockage
        save_to_database(feedback_data)
        
        # Si feedback n√©gatif ‚Üí investigation
        if feedback_type == 'negative':
            self.analyze_failure(query_id)
```

**R√©entra√Ænement p√©riodique** :
```python
def monthly_retraining():
    """R√©entra√Ænement mensuel avec les feedbacks"""
    # R√©cup√©ration des feedbacks
    feedbacks = load_feedbacks(last_30_days=True)
    
    # G√©n√©ration de nouveaux exemples d'entra√Ænement
    new_training_data = []
    for feedback in feedbacks:
        if feedback['type'] == 'negative':
            # Analyse de la requ√™te √©chou√©e
            query = get_query(feedback['query_id'])
            correct_chunks = identify_correct_chunks(query, feedback['comment'])
            
            new_training_data.append({
                'query': query,
                'positive': correct_chunks,
                'negative': query['retrieved_chunks']
            })
    
    # Fine-tuning incr√©mental
    if len(new_training_data) > 100:
        fine_tune_model(new_training_data)
        logger.info(f"Model fine-tuned with {len(new_training_data)} examples")
```

### Long Terme (2026+)

#### 1. Multimodalit√©

**Support documents PDF/Images** :
```python
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

class MultimodalProcessor:
    def process_pdf(self, pdf_path):
        """Extraction texte + images d'un PDF"""
        # Conversion PDF ‚Üí images
        images = convert_from_path(pdf_path)
        
        extracted_text = ""
        for image in images:
            # OCR
            text = pytesseract.image_to_string(image, lang='fra')
            extracted_text += text + "\n\n"
        
        return extracted_text
    
    def process_image(self, image_path):
        """Extraction texte d'une image"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='fra')
        return text
```

**Vision LLM pour tableaux complexes** :
```python
async def analyze_table_with_vision(image):
    """Utilise GPT-4 Vision ou Claude pour analyser un tableau"""
    response = await vision_llm.analyze(
        image=image,
        prompt="Extrait les donn√©es de ce tableau sous forme JSON structur√©"
    )
    return response
```

#### 2. G√©n√©ration de Documents

**Templates Jinja2** :
```python
from jinja2 import Template

def generate_contract(template_name, context):
    """G√©n√©ration de contrat personnalis√©"""
    template = load_template(f"templates/{template_name}.j2")
    
    # Enrichissement du contexte via LLM
    enriched_context = llm_enrich_context(context)
    
    # G√©n√©ration
    document = template.render(**enriched_context)
    
    # Conversion Markdown ‚Üí PDF
    pdf = convert_md_to_pdf(document)
    
    return pdf
```

**Exemple** : G√©n√©ration automatique de Convention d'√âtude √† partir d'un brief client.

#### 3. Int√©gration √âtendue

**Plugin Google Workspace** :
- Add-on Google Docs : assistance r√©daction contrat
- Extension Gmail : d√©tection clauses dangereuses emails clients

**Bot Discord**[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks]
    ‚Üì
[Context Building] ‚Üí [Prompt Engineering] ‚Üí [Claude LLM] ‚Üí [Response Formatting]
    ‚Üì
[JSON R√©ponse] ‚Üí [Slack Bot / Web UI / API Client]
```

### Phase 1 : Acquisition des Donn√©es (Scraping)

#### Architecture du Scraping Selenium

Le scraping s'effectue via des scripts Python d√©di√©s par source, utilisant Selenium WebDriver pour g√©rer le JavaScript et les interactions complexes.

**Script principal** : `scrapers/kiwi_scraper.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import json
from datetime import datetime

class KiwiScraper:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def scrape_kiwi_legal(self):
        """Scrape Kiwi Legal documents"""
        base_url = "https://kiwi.cnje.fr/legal"
        self.driver.get(base_url)
        
        # Attente du chargement dynamique
        self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "document-list"))
        )
        
        documents = []
        doc_elements = self.driver.find_elements(By.CLASS_NAME, "document-item")
        
        for element in doc_elements:
            doc_data = self._extract_legal_document(element)
            documents.append(doc_data)
        
        return documents
```

**Gestion de la pagination** :
```python
def scrape_with_pagination(self, url, max_pages=None):
    page = 1
    all_data = []
    
    while True:
        print(f"Scraping page {page}...")
        self.driver.get(f"{url}?page={page}")
        
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "content"))
            )
        except TimeoutException:
            print(f"No more pages after page {page-1}")
            break
        
        page_data = self._extract_page_content()
        if not page_data:
            break
        
        all_data.extend(page_data)
        page += 1
        
        if max_pages and page > max_pages:
            break
    
    return all_data
```

**Gestion des erreurs et retry** :
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def robust_scrape(self, url):
    try:
        self.driver.get(url)
        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        return self._extract_content()
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        raise
```

#### Extraction et Nettoyage HTML

Apr√®s extraction Selenium, parsing avec BeautifulSoup pour nettoyage :

```python
from bs4 import BeautifulSoup
import re

def clean_html_content(raw_html):
    """Nettoyage HTML et extraction texte pertinent"""
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # Suppression √©l√©ments non pertinents
    for element in soup(['script', 'style', 'nav', 'footer', 'header']):
        element.decompose()
    
    # Suppression classes publicitaires
    for ad in soup.find_all(class_=['advertisement', 'popup', 'banner']):
        ad.decompose()
    
    # Extraction texte
    text = soup.get_text(separator='\n', strip=True)
    
    # Nettoyage espaces multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text

def extract_metadata(soup):
    """Extraction m√©tadonn√©es structur√©es"""
    metadata = {}
    
    # Titre
    title_tag = soup.find('h1') or soup.find('title')
    metadata['title'] = title_tag.get_text(strip=True) if title_tag else "Unknown"
    
    # Date publication
    date_tag = soup.find('time') or soup.find(class_='date')
    if date_tag:
        metadata['date'] = date_tag.get('datetime') or date_tag.get_text(strip=True)
    
    # Auteur
    author_tag = soup.find(class_='author') or soup.find(rel='author')
    if author_tag:
        metadata['author'] = author_tag.get_text(strip=True)
    
    # Cat√©gorie
    category_tag = soup.find(class_='category')
    if category_tag:
        metadata['category'] = category_tag.get_text(strip=True)
    
    return metadata
```

#### Structure JSON Standardis√©e

Export dans un format JSON unifi√© facilitant le traitement ult√©rieur :

**Format Legal** :
```json
{
  "source": "kiwi_legal",
  "document_type": "statuts",
  "scraping_metadata": {
    "url": "https://kiwi.cnje.fr/legal/statuts-types-association",
    "date_scraped": "2025-01-15T10:30:00Z",
    "scraper_version": "2.1.0"
  },
  "metadata": {
    "title": "Statuts types Junior-Entreprise association loi 1901",
    "category": "juridique",
    "subcategory": "statuts",
    "publication_date": "2024-06-01",
    "author": "Commission Juridique CNJE"
  },
  "content": {
    "sections": [
      {
        "title": "TITRE I - Dispositions g√©n√©rales",
        "articles": [
          {
            "number": 1,
            "title": "D√©nomination",
            "content": "Il est fond√© entre les adh√©rents aux pr√©sents statuts..."
          }
        ]
      }
    ],
    "full_text": "STATUTS TYPES..."
  }
}
```

**Format RSE** :
```json
{
  "source": "kiwi_rse",
  "document_type": "module_rse",
  "scraping_metadata": {...},
  "metadata": {
    "title": "Module Environnement - Gestion des D√©chets",
    "pilier": "environnemental",
    "odd_concernes": [12, 13],
    "niveau_difficulte": "d√©butant"
  },
  "content": {
    "introduction": "La gestion des d√©chets...",
    "objectifs": ["R√©duire la production", "Recycler"],
    "actions": [
      {
        "titre": "Mise en place du tri s√©lectif",
        "description": "...",
        "indicateurs": ["Taux de recyclage", "Volume d√©chets"]
      }
    ]
  }
}
```

**Format FAQ** :
```json
{
  "source": "kiwi_faq",
  "document_type": "faq",
  "scraping_metadata": {...},
  "metadata": {
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2
  },
  "content": {
    "questions": [
      {
        "id": "compta_tva_001",
        "question": "Comment d√©clarer la TVA en tant que JE ?",
        "reponse": "Les Junior-Entreprises b√©n√©ficient...",
        "tags": ["tva", "d√©claration", "comptabilit√©"],
        "related_questions": ["compta_tva_002", "compta_tva_005"]
      }
    ]
  }
}
```

#### Stockage et Versioning

**Arborescence de stockage** :
```
data/
‚îú‚îÄ‚îÄ raw/                          # Donn√©es brutes apr√®s scraping
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_2025-01-15.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_2025-01-15.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_2025-01-15.json
‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_processed.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_processed.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_processed.json
‚îú‚îÄ‚îÄ index/                        # Index g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îî‚îÄ‚îÄ logs/                         # Logs de scraping
    ‚îî‚îÄ‚îÄ scraping_2025-01-15.log
```

**Logging d√©taill√©** :
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scraping_{datetime.now().date()}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Dans le scraper
logger.info(f"Starting scrape of {url}")
logger.info(f"Extracted {len(documents)} documents")
logger.warning(f"Failed to extract metadata for document {doc_id}")
logger.error(f"Scraping failed: {exception}")
```

### Phase 2 : Preprocessing & Transformation

#### Type Detection Automatique

Algorithme de d√©tection bas√© sur plusieurs signaux :

```python
class DocumentTypeDetector:
    def __init__(self):
        self.type_patterns = {
            'legal': {
                'filename': ['statuts', 'contrat', 'legal', 'juridique'],
                'fields': ['articles', 'sections', 'clauses'],
                'keywords': ['article', 'alin√©a', 'conform√©ment', 'obligation']
            },
            'rse': {
                'filename': ['rse', 'durable', 'environnement'],
                'fields': ['pilier', 'odd', 'actions'],
                'keywords': ['d√©veloppement durable', 'odd', 'responsabilit√©']
            },
            'faq': {
                'filename': ['faq', 'questions'],
                'fields': ['questions', 'reponses'],
                'keywords': ['comment', 'pourquoi', 'qu\'est-ce']
            },
            'je': {
                'filename': ['annuaire', 'je', 'junior'],
                'fields': ['nom', 'ville', 'ecole', 'domaines'],
                'keywords': ['junior-entreprise', '√©cole', 'domaine']
            }
        }
    
    def detect_type(self, document_data, filename):
        scores = {doc_type: 0 for doc_type in self.type_patterns}
        
        # Score filename
        for doc_type, patterns in self.type_patterns.items():
            for pattern in patterns['filename']:
                if pattern in filename.lower():
                    scores[doc_type] += 2
        
        # Score fields pr√©sents
        doc_fields = set(document_data.get('content', {}).keys())
        for doc_type, patterns in self.type_patterns.items():
            matching_fields = doc_fields.intersection(patterns['fields'])
            scores[doc_type] += len(matching_fields) * 3
        
        # Score keywords dans le contenu
        content_text = json.dumps(document_data).lower()
        for doc_type, patterns in self.type_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_text:
                    scores[doc_type] += 1
        
        # S√©lection du type avec le score maximal
        detected_type = max(scores, key=scores.get)
        confidence = scores[detected_type] / sum(scores.values()) if sum(scores.values()) > 0 else 0
        
        return {
            'type': detected_type if confidence > 0.3 else 'general',
            'confidence': confidence,
            'scores': scores
        }
```

#### Extraction Sp√©cialis√©e par Type

**Extracteur Legal** :
```python
class LegalExtractor:
    def extract(self, document):
        extracted_data = []
        
        sections = document['content']['sections']
        for section in sections:
            section_title = section['title']
            
            for article in section.get('articles', []):
                extracted_data.append({
                    'text': f"{article['title']}\n{article['content']}",
                    'type': 'legal',
                    'metadata': {
                        'document_type': document['document_type'],
                        'section': section_title,
                        'article_num': article['number'],
                        'title': article['title']
                    }
                })
        
        return extracted_data
```

**Extracteur FAQ** :
```python
class FAQExtractor:
    def extract(self, document):
        extracted_data = []
        
        category = document['metadata']['category']
        subcategory = document['metadata'].get('subcategory', '')
        level = document['metadata'].get('level', 1)
        
        for qa in document['content']['questions']:
            # Contexte hi√©rarchique
            context_path = f"{category}"
            if subcategory:
                context_path += f" > {subcategory}"
            
            text = f"Question: {qa['question']}\n\nR√©ponse: {qa['reponse']}"
            
            extracted_data.append({
                'text': text,
                'type': 'faq',
                'metadata': {
                    'question': qa['question'],
                    'category': category,
                    'subcategory': subcategory,
                    'level': level,
                    'context_path': context_path,
                    'tags': qa.get('tags', []),
                    'related_questions': qa.get('related_questions', [])
                }
            })
        
        return extracted_data
```

**Extracteur JE** :
```python
class JEExtractor:
    def extract(self, document):
        extracted_data = []
        
        for je in document['content']['junior_entreprises']:
            # Construction texte descriptif
            text = f"""
            Nom: {je['nom']}
            Ville: {je['ville']}
            √âcole: {je['ecole']}
            Domaines d'expertise: {', '.join(je['domaines'])}
            Contact: {je['contact']['email']}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'je',
                'metadata': {
                    'nom': je['nom'],
                    'ville': je['ville'],
                    'ecole': je['ecole'],
                    'domaines': je['domaines'],
                    'contact': je['contact'],
                    'certified': je.get('certified_cnje', False)
                }
            })
        
        return extracted_data
```

**Extracteur RSE** :
```python
class RSEExtractor:
    def extract(self, document):
        extracted_data = []
        
        pilier = document['metadata']['pilier']
        odd = document['metadata']['odd_concernes']
        
        # Extraction par action
        for action in document['content']['actions']:
            text = f"""
            Module RSE: {document['metadata']['title']}
            Pilier: {pilier}
            
            Action: {action['titre']}
            {action['description']}
            
            Indicateurs: {', '.join(action['indicateurs'])}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'rse',
                'metadata': {
                    'module': document['metadata']['title'],
                    'pilier': pilier,
                    'odd': odd,
                    'action_titre': action['titre'],
                    'indicateurs': action['indicateurs']
                }
            })
        
        return extracted_data
```

#### Smart Chunking S√©mantique

Le chunking respecte la logique m√©tier plut√¥t qu'une simple d√©coupe par longueur :

```python
class SemanticChunker:
    def __init__(self, min_length=50, max_length=1000, target_length=300):
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
    
    def chunk_text(self, text, doc_type, metadata):
        if doc_type == 'faq':
            # FAQ: chaque Q/A est un chunk autonome
            return self._chunk_faq(text, metadata)
        elif doc_type == 'legal':
            # Legal: d√©coupage par article/section
            return self._chunk_legal(text, metadata)
        elif doc_type == 'je':
            # JE: entit√© atomique, pas de d√©coupage
            return [self._create_chunk(text, doc_type, metadata)]
        elif doc_type == 'rse':
            # RSE: d√©coupage par action
            return self._chunk_rse(text, metadata)
        else:
            # G√©n√©rique: d√©coupage par paragraphes avec overlap
            return self._chunk_generic(text, doc_type, metadata)
    
    def _chunk_generic(self, text, doc_type, metadata):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_length:
                current_chunk += para + "\n\n"
            else:
                if len(current_chunk) > self.min_length:
                    chunks.append(
                        self._create_chunk(current_chunk.strip(), doc_type, metadata)
                    )
                current_chunk = para + "\n\n"
        
        if len(current_chunk) > self.min_length:
            chunks.append(
                self._create_chunk(current_chunk.strip(), doc_type, metadata)
            )
        
        return chunks
    
    def _create_chunk(self, text, doc_type, metadata):
        return {
            'text': text,
            'type': doc_type,
            'metadata': metadata,
            'length': len(text),
            'word_count': len(text.split())
        }
```

#### Enrichissement M√©tadonn√©es

Chaque chunk est enrichi automatiquement :

```python
class MetadataEnricher:
    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.category_classifier = CategoryClassifier()
    
    def enrich_chunk(self, chunk):
        text = chunk['text']
        
        # Extraction keywords automatique
        keywords = self.keyword_extractor.extract(text, top_n=5)
        chunk['metadata']['keywords'] = keywords
        
        # Classification cat√©gorie fine (si pas d√©j√† pr√©sente)
        if 'category' not in chunk['metadata']:
            category = self.category_classifier.classify(text)
            chunk['metadata']['category'] = category
        
        # Calcul de priorit√© (bas√© sur usage historique si disponible)
        chunk['metadata']['priority'] = self._calculate_priority(chunk)
        
        # Ajout timestamps
        chunk['metadata']['indexed_at'] = datetime.now().isoformat()
        
        # G√©n√©ration d'un hash pour d√©tecter les modifications
        chunk['metadata']['content_hash'] = hashlib.md5(
            text.encode()
        ).hexdigest()
        
        return chunk
    
    def _calculate_priority(self, chunk):
        # Heuristique simple : sources officielles = haute priorit√©
        priority = 0.5
        
        if chunk['type'] == 'legal':
            priority += 0.2
        if 'statuts' in chunk.get('metadata', {}).get('category', '').lower():
            priority += 0.15
        if chunk['metadata'].get('is_featured', False):
            priority += 0.1
        
        return min(priority, 1.0)
```

### Phase 3 : Vectorisation et Indexation

#### Configuration TF-IDF Optimis√©e

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IndexBuilder:
    def __init__(self):
        # Stopwords personnalis√©s JE
        self.custom_stopwords = [
            'junior', 'entreprise', 'je', 'cnje',
            '√©tudiant', '√©tudiante', 'projet', 'mission',
            'conform√©ment', 'article', 'alin√©a', 'paragraphe'
        ]
        
        # Configuration TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8,
            stop_words=self.custom_stopwords,
            sublinear_tf=True,
            norm='l2',
            strip_accents='unicode'
        )
    
    def build_index(self, chunks):
        print(f"Building index from {len(chunks)} chunks...")
        
        # Extraction des textes
        texts = [chunk['text'] for chunk in chunks]
        
        # Vectorisation TF-IDF
        print("Vectorizing with TF-IDF...")
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
        
        # R√©duction dimensionnelle SVD
        print("Applying SVD dimensionality reduction...")
        n_components = min(300, tfidf_matrix.shape[0] - 1)
        svd_model = TruncatedSVD(
            n_components=n_components,
            algorithm='randomized',
            n_iter=7,
            random_state=42
        )
        vectors_reduced = svd_model.fit_transform(tfidf_matrix)
        print(f"Reduced to {n_components} dimensions")
        
        # Construction des index secondaires
        print("Building secondary indexes...")
        metadata_index = self._build_metadata_indexes(chunks)
        
        # Assemblage de l'index complet
        index = {
            'vectorizer': self.vectorizer,
            'svd_model': svd_model,
            'vectors': vectors_reduced,
            'chunks': chunks,
            'metadata_index': metadata_index,
            'version': '2.1.0',
            'build_date': datetime.now().isoformat(),
            'statistics': {
                'n_chunks': len(chunks),
                'n_features': tfidf_matrix.shape[1],
                'n_components': n_components,
                'vocabulary_size': len(self.vectorizer.vocabulary_)
            }
        }
        
        return index
    
    def _build_metadata_indexes(self, chunks):
        by_type = {}
        by_category = {}
        by_source = {}
        
        for idx, chunk in enumerate(chunks):
            # Index by type
            chunk_type = chunk['type']
            if chunk_type not in by_type:
                by_type[chunk_type] = []
            by_type[chunk_type].append(idx)
            
            # Index by category
            category = chunk['metadata'].get('category', 'unknown')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(idx)
            
            # Index by source
            source = chunk['metadata'].get('source_file', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(idx)
        
        return {
            'by_type': by_type,
            'by_category': by_category,
            'by_source': by_source
        }
    
    def save_index(self, index, filepath='data/index/kiwi_advanced_index.pkl'):
        print(f"Saving index to {filepath}...")
        with open(filepath, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"Index saved successfully ({file_size_mb:.2f} MB)")
```

#### Processus Complet d'Indexation

Script principal orchestrant tout le pipeline :

```python
def main_indexation_pipeline():
    print("=== COMPLY INDEXATION PIPELINE ===\n")
    
    # 1. Chargement des donn√©es sources
    print("Step 1: Loading source data...")
    legal_data = load_json('data/processed/kiwi_legal_processed.json')
    rse_data = load_json('data/processed/kiwi_rse_processed.json')
    faq_data = load_json('data/processed/kiwi_faq_processed.json')
    je_data = load_json('data/processed/kiwi_je_processed.json')
    
    all_sources = [
        ('legal', legal_data),
        ('rse', rse_data),
        ('faq', faq_data),
        ('je', je_data)
    ]
    
    # 2. Extraction et chunking
    print("\nStep 2: Extracting and chunking...")
    all_chunks = []
    
    for source_type, data in all_sources:
        extractor = get_extractor(source_type)
        chunks = extractor.extract(data)
        
        # Chunking s√©mantique
        chunker = SemanticChunker()
        chunked_data = []
        for chunk in chunks:
            chunked_data.extend(
                chunker.chunk_text(
                    chunk['text'],
                    chunk['type'],
                    chunk['metadata']
                )
            )
        
        print(f"  - {source_type}: {len(chunked_data)} chunks")
        all_chunks.extend(chunked_data)
    
    print(f"Total chunks: {len(all_chunks)}")
    
    # 3. Enrichissement
    print("\nStep 3: Enriching metadata...")
    enricher = MetadataEnricher()
    enriched_chunks = [enricher.enrich_chunk(c) for c in all_chunks]
    
    # 4. Construction de l'index
    print("\nStep 4: Building vector index...")
    builder = IndexBuilder()
    index = builder.build_index(enriched_chunks)
    
    # 5. Persistance
    print("\nStep 5: Saving index...")
    builder.save_index(index)
    
    # 6. Statistiques finales
    print("\n=== INDEXATION COMPLETE ===")
    print(f"Total chunks indexed: {index['statistics']['n_chunks']}")
    print(f"Vocabulary size: {index['statistics']['vocabulary_size']}")
    print(f"Vector dimensions: {index['statistics']['n_components']}")
    print(f"Index version: {index['version']}")
    
    return index

if __name__ == "__main__":
    main_indexation_pipeline()
```

### Phase 4 : Serving et Recherche

#### Chargement de l'Index au D√©marrage

```python
from fastapi import FastAPI
import pickle

app = FastAPI(title="Comply API", version="2.1.0")

# Chargement de l'index au d√©marrage (√©v√©nement startup)
@app.on_event("startup")
async def load_index():
    global INDEX
    
    print("Loading Comply index...")
    start_time = time.time()
    
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        INDEX = pickle.load(f)
    
    load_time = time.time() - start_time
    print(f"Index loaded in {load_time:.2f}s")
    print(f"  - Version: {INDEX['version']}")
    print(f"  - Chunks: {INDEX['statistics']['n_chunks']}")
    print(f"  - Memory: {sys.getsizeof(INDEX) / (1024**2):.2f} MB")
```

#### Endpoint /ask - Impl√©mentation Compl√®te

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    context: Optional[dict] = None
    options: Optional[dict] = None

class ComprehensiveAnswer(BaseModel):
    answer: str
    confidence: float
    detected_type: str
    sources: List[dict]
    related_questions: List[str]
    processing_time_ms: int

@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    start_time = time.time()
    
    try:
        # 1. D√©tection du type de requ√™te
        query_type_result = detect_query_type(request.question)
        detected_type = query_type_result['detected_type']
        
        # 2. Recherche vectorielle avec boosting
        search_results = vector_search(
            query=request.question,
            query_type=detected_type,
            top_k=request.options.get('max_chunks', 10) if request.options else 10
        )
        
        # 3. Construction du contexte
        context_string = build_context(search_results['chunks'])
        
        # 4. Prompt engineering
        prompt = generate_prompt(
            question=request.question,
            context=context_string,
            query_type=detected_type
        )
        
        # 5. Appel LLM
        llm_response = await call_claude(prompt)
        
        # 6. Post-processing
        formatted_response = format_response(
            raw_response=llm_response['response'],
            context_chunks=search_results['chunks'],
            query_type=detected_type
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return ComprehensiveAnswer(
            answer=formatted_response['answer'],
            confidence=formatted_response['confidence'],
            detected_type=detected_type,
            sources=formatted_response['sources'],
            related_questions=formatted_response['related_questions'],
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Infrastructure Recommand√©e

### H√©bergement VPS

Pour un d√©ploiement en production, un VPS Debian offre le meilleur compromis performance/co√ªt/contr√¥le.

**Sp√©cifications recommand√©es** :

| Composant | Minimum | Recomman# Comply by Sepefrei

![Comply Logo](comply_logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks

'ip_address': anonymize_ip(context.get('ip')),
                'user_agent': context.get('user_agent'),
                'processing_time_ms': response['processing_time_ms']
            }
        }
        
        # Stockage s√©curis√©
        store_audit_entry(audit_entry)
        
        return audit_entry['id']
```

**Export audit pour RGPD** :
```python
def export_user_data(user_id):
    """Export de toutes les donn√©es d'un utilisateur"""
    queries = get_user_queries(user_id)
    
    export_data = {
        'user_id': user_id,
        'export_date': datetime.now().isoformat(),
        'queries': queries,
        'statistics': {
            'total_queries': len(queries),
            'date_range': {
                'first': queries[0]['timestamp'],
                'last': queries[-1]['timestamp']
            }
        }
    }
    
    return export_data

def anonymize_user_data(user_id):
    """Anonymisation compl√®te des donn√©es utilisateur"""
    # Remplacement user_id par hash irr√©versible
    anonymized_id = hashlib.sha256(f"{user_id}_{SECRET_SALT}".encode()).hexdigest()
    
    # Update de tous les logs
    update_audit_logs(user_id, anonymized_id)
```

**Certification ISO 27001** :
- Chiffrement at-rest (disques)
- Chiffrement in-transit (TLS)
- Rotation des secrets tous les 90 jours
- Backup chiffr√© quotidien
- Disaster recovery plan (RTO < 4h, RPO < 1h)

---

## Architecture D√©taill√©e

### Diagramme de S√©quence Complet

**[IMAGE REQUISE : Diagramme de s√©quence d√©taill√© d'une requ√™te]**

```mermaid
sequenceDiagram
    actor User
    participant Slack
    participant Nginx
    participant FastAPI
    participant TypeDetector
    participant IndexLoader
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter
    participant AuditLogger

    User->>Slack: @comply Comment modifier les statuts ?
    Slack->>Nginx: POST /api/slack/events
    Nginx->>FastAPI: Forward request
    
    FastAPI->>FastAPI: Parse Slack event
    FastAPI->>TypeDetector: Analyze query
    Note over TypeDetector: NLP classification<br/>Keywords matching<br/>Score calculation
    TypeDetector-->>FastAPI: {type: "juridique", confidence: 0.89}
    
    FastAPI->>IndexLoader: Get index
    Note over IndexLoader: Index already in RAM<br/>No I/O needed
    IndexLoader-->>FastAPI: Index reference
    
    FastAPI->>VectorSearch: Search(query, type="juridique")
    Note over VectorSearch: TF-IDF transform<br/>SVD transform<br/>Cosine similarity
    VectorSearch-->>FastAPI: Top 100 candidates
    
    FastAPI->>Booster: Apply boosting
    Note over Booster: Type boost √ó1.3<br/>Category boost √ó1.2<br/>Source boost √ó1.15<br/>Recency boost √ó1.1
    Booster-->>FastAPI: Top 10 final chunks
    
    FastAPI->>ContextBuilder: Build context
    Note over ContextBuilder: Aggregate chunks<br/>Deduplicate<br/>Format with metadata
    ContextBuilder-->>FastAPI: Structured context string
    
    FastAPI->>PromptEngine: Generate prompt
    Note over PromptEngine: Select legal template<br/>Inject context<br/>Add instructions
    PromptEngine-->>FastAPI: Final prompt (1423 tokens)
    
    FastAPI->>Claude: POST /v1/messages
    Note over Claude: Claude Sonnet 4.5<br/>Temperature: 0.3<br/>Max tokens: 2000
    Claude-->>FastAPI: Generated response (487 tokens)
    
    FastAPI->>ResponseFormatter: Format response
    Note over ResponseFormatter: Extract sources<br/>Calculate confidence<br/>Generate related questions<br/>Structure JSON
    ResponseFormatter-->>FastAPI: Formatted response
    
    FastAPI->>AuditLogger: Log interaction
    Note over AuditLogger: Store query/response<br/>Anonymize user data<br/>Compliance tracking
    AuditLogger-->>FastAPI: Audit ID
    
    FastAPI-->>Nginx: JSON response
    Nginx-->>Slack: Forward response
    Slack-->>User: Display formatted answer with sources
```

### Architecture en Couches D√©taill√©e

**Layer 1 : Data Sources**
```
Sources Externes
‚îú‚îÄ‚îÄ Kiwi Legal (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Statuts types (10 documents)
‚îÇ   ‚îú‚îÄ‚îÄ Contrats mod√®les (25 templates)
‚îÇ   ‚îú‚îÄ‚îÄ R√®glements int√©rieurs (8 versions)
‚îÇ   ‚îî‚îÄ‚îÄ Documentation juridique (50+ articles)
‚îú‚îÄ‚îÄ Kiwi RSE (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Modules environnementaux (15)
‚îÇ   ‚îú‚îÄ‚îÄ Modules sociaux (12)
‚îÇ   ‚îú‚îÄ‚îÄ Modules gouvernance (8)
‚îÇ   ‚îî‚îÄ‚îÄ Mapping ODD (17 objectifs)
‚îú‚îÄ‚îÄ Kiwi FAQ (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Cat√©gories niveau 1 (12)
‚îÇ   ‚îú‚îÄ‚îÄ Sous-cat√©gories niveau 2 (47)
‚îÇ   ‚îî‚îÄ‚îÄ Questions/r√©ponses niveau 3 (320+)
‚îî‚îÄ‚îÄ Base Junior-Entreprises (JSON/CSV)
    ‚îú‚îÄ‚îÄ JE fran√ßaises (~200)
    ‚îú‚îÄ‚îÄ M√©tadonn√©es (√©cole, ville, domaines)
    ‚îî‚îÄ‚îÄ Contacts (email, t√©l√©phone, site)
```

**Layer 2 : Acquisition Pipeline**
```
Selenium WebDriver
‚îú‚îÄ‚îÄ Chrome Headless (configur√© via ChromeOptions)
‚îú‚îÄ‚îÄ WebDriverWait (timeout 10s)
‚îú‚îÄ‚îÄ Navigation programmatique
‚îÇ   ‚îú‚îÄ‚îÄ Login automatique (si n√©cessaire)
‚îÇ   ‚îú‚îÄ‚îÄ Parcours des menus
‚îÇ   ‚îî‚îÄ‚îÄ Gestion de la pagination
‚îî‚îÄ‚îÄ Extraction HTML
    ‚îú‚îÄ‚îÄ BeautifulSoup4 parsing
    ‚îú‚îÄ‚îÄ Nettoyage (scripts, styles, nav)
    ‚îî‚îÄ‚îÄ Normalisation (encoding UTF-8)

Scripts de Nettoyage Python
‚îú‚îÄ‚îÄ Suppression √©l√©ments non pertinents
‚îú‚îÄ‚îÄ Extraction m√©tadonn√©es (auteur, date, cat√©gorie)
‚îú‚îÄ‚îÄ Structuration s√©mantique (sections, articles)
‚îî‚îÄ‚îÄ Export JSON standardis√©
    ‚îú‚îÄ‚îÄ Format Legal
    ‚îú‚îÄ‚îÄ Format RSE
    ‚îú‚îÄ‚îÄ Format FAQ
    ‚îî‚îÄ‚îÄ Format JE
```

**Layer 3 : Processing Engine**
```
Type Detection
‚îú‚îÄ‚îÄ Analyse filename (regex patterns)
‚îú‚îÄ‚îÄ Inspection structure JSON (champs pr√©sents)
‚îú‚îÄ‚îÄ Analyse s√©mantique contenu (keywords)
‚îî‚îÄ‚îÄ Score de confiance (threshold 0.3)

Extraction Sp√©cialis√©e
‚îú‚îÄ‚îÄ LegalExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Parse sections/articles
‚îÇ   ‚îú‚îÄ‚îÄ Extraction num√©ros articles
‚îÇ   ‚îî‚îÄ‚îÄ D√©tection r√©f√©rences crois√©es
‚îú‚îÄ‚îÄ RSEExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Identification pilier (env/social/gouv)
‚îÇ   ‚îú‚îÄ‚îÄ Extraction actions concr√®tes
‚îÇ   ‚îî‚îÄ‚îÄ Mapping ODD
‚îú‚îÄ‚îÄ FAQExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Parse Q/A hi√©rarchiques
‚îÇ   ‚îú‚îÄ‚îÄ Pr√©servation contexte parent
‚îÇ   ‚îî‚îÄ‚îÄ Extraction tags/keywords
‚îî‚îÄ‚îÄ JEExtractor
    ‚îú‚îÄ‚îÄ Extraction champs structur√©s
    ‚îú‚îÄ‚îÄ Normalisation (ville, √©cole)
    ‚îî‚îÄ‚îÄ Validation contacts

Smart Chunking
‚îú‚îÄ‚îÄ Chunking par type (logique m√©tier)
‚îÇ   ‚îú‚îÄ‚îÄ FAQ : 1 chunk = 1 Q/A
‚îÇ   ‚îú‚îÄ‚îÄ Legal : 1 chunk = 1 article/section
‚îÇ   ‚îú‚îÄ‚îÄ JE : 1 chunk = 1 fiche compl√®te
‚îÇ   ‚îî‚îÄ‚îÄ RSE : 1 chunk = 1 action/module
‚îú‚îÄ‚îÄ Respect taille (50-1000 mots, cible 300)
‚îî‚îÄ‚îÄ Pr√©servation coh√©rence s√©mantique

Enrichissement M√©tadonn√©es
‚îú‚îÄ‚îÄ Extraction keywords automatique (RAKE/YAKE)
‚îú‚îÄ‚îÄ Classification cat√©gorie (si absente)
‚îú‚îÄ‚îÄ Calcul priorit√© (heuristique)
‚îú‚îÄ‚îÄ Ajout timestamps (indexed_at)
‚îî‚îÄ‚îÄ Hash contenu (d√©tection modifications)
```

**Layer 4 : Vectorisation & Indexation**
```
TF-IDF Vectorizer (Scikit-Learn)
‚îú‚îÄ‚îÄ Configuration
‚îÇ   ‚îú‚îÄ‚îÄ max_features: 5000
‚îÇ   ‚îú‚îÄ‚îÄ ngram_range: (1, 3)
‚îÇ   ‚îú‚îÄ‚îÄ min_df: 2
‚îÇ   ‚îú‚îÄ‚îÄ max_df: 0.8
‚îÇ   ‚îú‚îÄ‚îÄ stop_words: custom JE
‚îÇ   ‚îú‚îÄ‚îÄ sublinear_tf: True
‚îÇ   ‚îî‚îÄ‚îÄ norm: 'l2'
‚îú‚îÄ‚îÄ Fit sur corpus complet (~8500 chunks)
‚îî‚îÄ‚îÄ Transform ‚Üí Matrice sparse (8500, 5000)

Truncated SVD (Scikit-Learn)
‚îú‚îÄ‚îÄ n_components: 300
‚îú‚îÄ‚îÄ algorithm: 'randomized'
‚îú‚îÄ‚îÄ n_iter: 7
‚îú‚îÄ‚îÄ Fit + Transform ‚Üí Matrice dense (8500, 300)
‚îî‚îÄ‚îÄ Variance expliqu√©e: ~85%

Multi-Index Construction
‚îú‚îÄ‚îÄ Index Vectoriel Principal
‚îÇ   ‚îî‚îÄ‚îÄ NumPy array (8500, 300) float32
‚îú‚îÄ‚îÄ Index by_type
‚îÇ   ‚îú‚îÄ‚îÄ 'faq' ‚Üí [0, 1, 15, 23, ...]
‚îÇ   ‚îú‚îÄ‚îÄ 'legal' ‚Üí [2, 5, 8, 11, ...]
‚îÇ   ‚îú‚îÄ‚îÄ 'je' ‚Üí [3, 7, 12, 19, ...]
‚îÇ   ‚îî‚îÄ‚îÄ 'rse' ‚Üí [4, 9, 14, 18, ...]
‚îú‚îÄ‚îÄ Index by_category
‚îÇ   ‚îú‚îÄ‚îÄ 'comptabilit√©' ‚Üí [...]
‚îÇ   ‚îú‚îÄ‚îÄ 'contrats' ‚Üí [...]
‚îÇ   ‚îú‚îÄ‚îÄ 'rh' ‚Üí [...]
‚îÇ   ‚îî‚îÄ‚îÄ ... (27 cat√©gories)
‚îú‚îÄ‚îÄ Index by_source
‚îÇ   ‚îî‚îÄ‚îÄ Par fichier JSON source
‚îî‚îÄ‚îÄ Index by_priority
    ‚îî‚îÄ‚îÄ Liste tri√©e par score priorit√©

Persistance Pickle
‚îú‚îÄ‚îÄ Fichier: kiwi_advanced_index.pkl
‚îú‚îÄ‚îÄ Taille: ~118 MB
‚îú‚îÄ‚îÄ Contenu complet:
‚îÇ   ‚îú‚îÄ‚îÄ vectorizer (fitted TfidfVectorizer)
‚îÇ   ‚îú‚îÄ‚îÄ svd_model (fitted TruncatedSVD)
‚îÇ   ‚îú‚îÄ‚îÄ vectors (NumPy array dense)
‚îÇ   ‚îú‚îÄ‚îÄ chunks (liste de dicts)
‚îÇ   ‚îú‚îÄ‚îÄ metadata_index (multi-index)
‚îÇ   ‚îú‚îÄ‚îÄ version (string)
‚îÇ   ‚îú‚îÄ‚îÄ build_date (ISO datetime)
‚îÇ   ‚îî‚îÄ‚îÄ statistics (dict)
‚îî‚îÄ‚îÄ Chargement: < 1 seconde
```

**Layer 5 : API Serving (FastAPI)**
```
main.py (Application Entry Point)
‚îú‚îÄ‚îÄ FastAPI app instance
‚îú‚îÄ‚îÄ CORS middleware (origins configur√©s)
‚îú‚îÄ‚îÄ Exception handlers
‚îú‚îÄ‚îÄ Startup event: load_index()
‚îî‚îÄ‚îÄ Shutdown event: cleanup()

Routers (Modularit√©)
‚îú‚îÄ‚îÄ /ask (main Q/A endpoint)
‚îú‚îÄ‚îÄ /search/* (recherche sp√©cialis√©e)
‚îú‚îÄ‚îÄ /admin/* (r√©indexation, stats)
‚îî‚îÄ‚îÄ /health (healthcheck)

Services Layer
‚îú‚îÄ‚îÄ vector_search.py
‚îÇ   ‚îú‚îÄ‚îÄ vectorize_query()
‚îÇ   ‚îú‚îÄ‚îÄ cosine_similarity_search()
‚îÇ   ‚îî‚îÄ‚îÄ filter_by_metadata()
‚îú‚îÄ‚îÄ llm_service.py
‚îÇ   ‚îú‚îÄ‚îÄ call_claude()
‚îÇ   ‚îú‚îÄ‚îÄ handle_rate_limits()
‚îÇ   ‚îî‚îÄ‚îÄ retry_logic()
‚îú‚îÄ‚îÄ type_detector.py
‚îÇ   ‚îú‚îÄ‚îÄ detect_query_type()
‚îÇ   ‚îî‚îÄ‚îÄ calculate_confidence()
‚îî‚îÄ‚îÄ boosting.py
    ‚îú‚îÄ‚îÄ apply_type_boost()
    ‚îú‚îÄ‚îÄ apply_category_boost()
    ‚îú‚îÄ‚îÄ apply_source_boost()
    ‚îî‚îÄ‚îÄ apply_recency_boost()

Models (Pydantic)
‚îú‚îÄ‚îÄ Request Models
‚îÇ   ‚îú‚îÄ‚îÄ QuestionRequest
‚îÇ   ‚îú‚îÄ‚îÄ AdvancedSearchRequest
‚îÇ   ‚îî‚îÄ‚îÄ ReindexRequest
‚îî‚îÄ‚îÄ Response Models
    ‚îú‚îÄ‚îÄ ComprehensiveAnswer
    ‚îú‚îÄ‚îÄ SearchResults
    ‚îî‚îÄ‚îÄ ReindexStatus

Configuration
‚îú‚îÄ‚îÄ Environment variables (.env)
‚îú‚îÄ‚îÄ Config class (Pydantic BaseSettings)
‚îî‚îÄ‚îÄ Secrets management
```

**Layer 6 : LLM Orchestration**
```
Query Processing Pipeline
‚îú‚îÄ‚îÄ 1. Type Detection
‚îÇ   ‚îú‚îÄ‚îÄ Keyword matching
‚îÇ   ‚îú‚îÄ‚îÄ TF-IDF scoring
‚îÇ   ‚îî‚îÄ‚îÄ Confidence threshold
‚îú‚îÄ‚îÄ 2. Vector Search
‚îÇ   ‚îú‚îÄ‚îÄ Query vectorization
‚îÇ   ‚îú‚îÄ‚îÄ Cosine similarity (all chunks)
‚îÇ   ‚îî‚îÄ‚îÄ Top 100 retrieval
‚îú‚îÄ‚îÄ 3. Contextual Boosting
‚îÇ   ‚îú‚îÄ‚îÄ Type match (√ó1.30)
‚îÇ   ‚îú‚îÄ‚îÄ Category match (√ó1.20)
‚îÇ   ‚îú‚îÄ‚îÄ Source authority (√ó1.15)
‚îÇ   ‚îî‚îÄ‚îÄ Recency (√ó1.10)
‚îú‚îÄ‚îÄ 4. Context Building
‚îÇ   ‚îú‚îÄ‚îÄ Aggregate top 10 chunks
‚îÇ   ‚îú‚îÄ‚îÄ Deduplicate similar (cosine > 0.85)
‚îÇ   ‚îú‚îÄ‚îÄ Format with metadata
‚îÇ   ‚îî‚îÄ‚îÄ Limit to ~8k tokens
‚îú‚îÄ‚îÄ 5. Prompt Engineering
‚îÇ   ‚îú‚îÄ‚îÄ Select template (by type)
‚îÇ   ‚îú‚îÄ‚îÄ Inject context
‚îÇ   ‚îú‚îÄ‚îÄ Add instructions
‚îÇ   ‚îî‚îÄ‚îÄ Final prompt assembly
‚îú‚îÄ‚îÄ 6. LLM Invocation
‚îÇ   ‚îú‚îÄ‚îÄ Anthropic API call
‚îÇ   ‚îú‚îÄ‚îÄ Model: claude-sonnet-4-5-20250929
‚îÇ   ‚îú‚îÄ‚îÄ Temperature: 0.3
‚îÇ   ‚îî‚îÄ‚îÄ Max tokens: 2000
‚îî‚îÄ‚îÄ 7. Response Formatting
    ‚îú‚îÄ‚îÄ Parse LLM output
    ‚îú‚îÄ‚îÄ Extract source references
    ‚îú‚îÄ‚îÄ Calculate confidence
    ‚îú‚îÄ‚îÄ Generate related questions
    ‚îî‚îÄ‚îÄ Structure JSON response

Prompt Templates
‚îú‚îÄ‚îÄ LEGAL_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Expert juridique JE
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: citer sources, alerter risques
‚îú‚îÄ‚îÄ RSE_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Consultant RSE
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: actions concr√®tes, ODD
‚îú‚îÄ‚îÄ FAQ_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Assistant p√©dagogique
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: clart√©, exemples
‚îî‚îÄ‚îÄ GENERAL_TEMPLATE
    ‚îú‚îÄ‚îÄ Role: Assistant Comply
    ‚îú‚îÄ‚îÄ Context injection point
    ‚îî‚îÄ‚îÄ Instructions: pr√©cision, sources

Claude API Integration
‚îú‚îÄ‚îÄ AsyncAnthropic client
‚îú‚îÄ‚îÄ Message creation
‚îú‚îÄ‚îÄ Error handling
‚îÇ   ‚îú‚îÄ‚îÄ Rate limiting (exponential backoff)
‚îÇ   ‚îú‚îÄ‚îÄ Timeout (60s)
‚îÇ   ‚îî‚îÄ‚îÄ API errors (502 fallback)
‚îî‚îÄ‚îÄ Usage tracking
    ‚îú‚îÄ‚îÄ Input tokens
    ‚îú‚îÄ‚îÄ Output tokens
    ‚îî‚îÄ‚îÄ Cost calculation
```

---

## Choix Techniques et Justifications

### TF-IDF + SVD vs Embeddings Transformers

**Pourquoi TF-IDF + SVD ?**

**Avantages** :
1. **Performance brute** : Vectorisation query < 2ms, recherche < 10ms pour 8500 chunks
2. **Empreinte m√©moire r√©duite** : ~300 MB en RAM vs ~2-3 GB pour embeddings denses
3. **Pas de d√©pendance GPU** : Fonctionne parfaitement sur CPU standard
4. **Interpr√©tabilit√©** : On sait exactement quels termes matchent (vocabulaire explicite)
5. **Co√ªt computationnel minimal** : Entra√Ænement < 30s, pas de fine-tuning n√©cessaire
6. **D√©ploiement simple** : Pas de mod√®le lourd √† charger (BERT = 400+ MB)

**Inconv√©nients** :
1. **Pr√©cision s√©mantique limit√©e** : "v√©hicule" et "voiture" ne matchent pas (pas de synonymie)
2. **Sensibilit√© au vocabulaire exact** : Requ√™te doit contenir les bons mots-cl√©s
3. **Pas de compr√©hension contextuelle** : "banque" (finance) vs "banque" (si√®ge) non distingu√©s

**Pourquoi √ßa suffit pour Comply** :
- Corpus m√©tier avec vocabulaire stable et technique
- Utilisateurs JE connaissent la terminologie (pas de langage naturel casual)
- Performance critique (latence < 2s exig√©e)
- Infrastructure l√©g√®re (VPS entr√©e de gamme)
- Pr√©cision actuelle ~75% top-1, ~92% top-5 (suffisant avec LLM derri√®re)

**Migration future vers embeddings** :
Pr√©vue Q3 2025 avec sentence-transformers fran√ßais (Solon, CamemBERT) pour am√©liorer la pr√©cision de 15-20% tout en gardant TF-IDF en fallback.

### Pickle vs Base de Donn√©es Vectorielle

**Pourquoi Pickle ?**

**Avantages** :
1. **Simplicit√© extr√™me** : Un seul fichier, aucune infrastructure externe
2. **Chargement ultra-rapide** : < 1s pour 118 MB, tout en RAM
3. **Pas de r√©seau** : Pas de latence r√©seau, pas de connexion √† g√©rer
4. **Atomic swap** : R√©indexation = swap de fichier (zero downtime)
5. **Backup trivial** : Simple `cp` du fichier
6. **Pas de d√©pendance** : Pas de service externe √† maintenir/monitorer

**Inconv√©nients** :
1. **Pas de recherche distribu√©e** : Scaling horizontal impossible
2. **Update non incr√©mentale** : Modification = r√©indexation compl√®te
3. **Pas de queries complexes** : Pas de filtres SQL-like sophistiqu√©s
4. **Limite de taille** : Probl√©matique au-del√† de 100k chunks (~1.5 GB RAM)

**Pourquoi √ßa suffit pour Comply v1** :
- Corpus stable ~8500 chunks (croissance lente, +10-15% par an)
- Usage mono-serveur (pas de distribution n√©cessaire)
- R√©indexation rare (1-2 fois par mois max)
- Latence critique (base distante = +10-50ms minimum)

**Migration future** :
- **Phase 1 (Q2 2025)** : FAISS local (m√™me serveur, API compatible)
- **Phase 2 (Q4 2025)** : Milvus/Qdrant si scaling n√©cessaire (multi-JE mutualis√©)

### FastAPI vs Flask/Django

**Pourquoi FastAPI ?**

**Avantages techniques** :
1. **Performance async native** : 3-4x plus rapide que Flask sur requ√™tes I/O-bound
2. **Type safety** : Pydantic validation automatique, pas de bugs runtime sur types
3. **Documentation auto** : OpenAPI/Swagger UI sans code additionnel
4. **Standards modernes** : ASGI, async/await natif, Python 3.9+ type hints
5. **√âcosyst√®me mature** : Starlette (robuste), Uvicorn (performant)

**Comparaison benchmarks** (requ√™tes/seconde sur VPS 4 cores) :
- FastAPI (async) : ~1200 req/s
- Flask (sync) : ~400 req/s
- Django (sync) : ~300 req/s
- Django (async) : ~800 req/s

**Pour Comply** :
- Appels LLM = I/O-bound (attente r√©seau 1-3s)
- Async permet de g√©rer 10-20 requ√™tes simultan√©es sans bloquer
- Validation Pydantic critique (s√©curit√©, fiabilit√©)
- Doc OpenAPI = indispensable pour int√©grations tierces

### Claude vs GPT-4 vs Mistral

**Pourquoi Claude Sonnet 4.5 ?**

**Comparaison qualitative** :

| Crit√®re | Claude Sonnet 4.5 | GPT-4 Turbo | Mistral Large |
|---------|-------------------|-------------|---------------|
| **Adh√©rence instructions** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Context window** | 200k tokens | 128k tokens | 32k tokens |
| **Hallucinations** | Tr√®s peu | Mod√©r√©es | Fr√©quentes |
| **Citations sources** | Excellent | Bon | Moyen |
| **Raisonnement complexe** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Latence** | 1-3s | 2-4s | 0.5-1.5s |
| **Co√ªt (1M tokens)** | $3 input/$15 output | $10 input/$30 output | $2 input/$6 output |

**Pour Comply** :
- **Adh√©rence critique** : Doit respecter strictement les instructions (citer sources, pas inventer)
- **Context window** : Permet d'injecter 10-15 chunks riches sans truncation
- **Hallucinations** : Inacceptable en contexte juridique
- **Co√ªt ma√Ætris√©** : ~$0.012/requ√™te vs $0.025 avec GPT-4

**Tests internes** :
- Claude : 87% de r√©ponses jug√©es "excellentes" (citations correctes, pas d'hallucination)
- GPT-4 : 82% (invente parfois des articles de loi)
- Mistral : 71% (manque de pr√©cision, citations approximatives)

### Python vs Node.js vs Go

**Pourquoi Python ?**

**Avantages pour Comply** :
1. **√âcosyst√®me ML/NLP** : Scikit-learn, NumPy, Pandas = stack standard
2. **Productivit√© d√©veloppement** : Syntaxe claire, prototypage rapide
3. **Biblioth√®ques scraping** : Selenium, BeautifulSoup = r√©f√©rences
4. **Type hints (3.9+)** : Robustesse comparable aux langages typ√©s
5. **Communaut√© data science** : Ressources, tutoriels, support

**Inconv√©nients** :
1. **Performance brute** : Plus lent que Go/Rust (mais non critique ici)
2. **GIL** : Threading limit√© (compens√© par async/await)

**Pourquoi pas Node.js** :
- √âcosyst√®me ML immature (TensorFlow.js limit√©)
- Pas de NumPy/Scikit-learn √©quivalents
- Moins adapt√© au traitement de donn√©es scientifiques

**Pourquoi pas Go** :
- Pas d'√©cosyst√®me ML/NLP mature
- D√©veloppement plus verbeux
- Moins de d√©veloppeurs data dans l'√©quipe

**Verdict** :
Python = choix √©vident pour un projet ML/NLP avec scraping. Performance suffisante avec FastAPI async. Possibilit√© de r√©√©crire les parties critiques en Rust/Cython si n√©cessaire (non pr√©vu).

---

## M√©triques et Performance

### Benchmarks Actuels (Production)

**Latence end-to-end** :
```
P50 (m√©diane)  : 1.8s
P95            : 3.2s
P99            : 4.5s
Max observ√©    : 6.2s
```

**D√©composition latence (P50)** :
```
Type detection  :   15ms  (0.8%)
Vector search   :   11ms  (0.6%)
Boosting        :    3ms  (0.2%)
Context build   :    5ms  (0.3%)
Prompt gen      :    2ms  (0.1%)
Claude API call : 1720ms (95.6%)
Response format :    8ms  (0.4%)
Logging         :    6ms  (0.3%)
Network         :   30ms  (1.7%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total           : 1800ms (100%)
```

**Observation** : Le LLM repr√©sente 95%+ de la latence. Optimisations internes < 100ms d'impact total.

**Throughput** :
- Serveur mono-instance (4 workers Uvicorn) : ~15 requ√™tes/s
- Limit√© par appels LLM s√©quentiels (pas de batch)
- Avec cache Redis (20% hit rate) : ~18 requ√™tes/s

**Pr√©cision** :
```
Top-1 accuracy : 75.3%  (chunk pertinent en 1√®re position)
Top-5 recall   : 92.1%  (chunk pertinent dans top 5)
Top-10 recall  : 96.8%  (chunk pertinent dans top 10)
```

M√©thodologie : √âvaluation manuelle sur 200 requ√™tes test par 3 experts JE.

**Satisfaction utilisateur** :
```
Feedback Slack (üëç/üëé) :
  Positif (üëç) : 85.3%
  N√©gatif (üëé) : 8.7%
  Neutre      : 6.0%
```

### Consommation Ressources

**VPS Production (Contabo VPS S)** :
```
CPU (moyenne)      : 8-12%  (pics √† 35% lors de r√©indexation)
RAM utilis√©e       : 2.1 GB / 8 GB (26%)
  - Index en m√©moire : 312 MB
  - Python runtime   : 180 MB
  - FastAPI          : 95 MB
  - Uvicorn workers  : 420 MB (4√ó105 MB)
  - OS + services    : 1.1 GB

Disque utilis√©    : 8.7 GB / 200 GB
  - Application      : 450 MB
  - Index + data     : 580 MB
  - Logs (30 jours)  : 1.2 GB
  - Docker images    : 2.8 GB
  - OS               : 3.7 GB

Bande passante (mois) : ~42 GB
  - API calls in      : 18 GB
  - API calls out     : 21 GB
  - Claude API        : 3 GB
```

**Co√ªt LLM** :
```
Usage mensuel moyen :
  - Requ√™tes/jour  : 147
  - Requ√™tes/mois  : ~4400

Tokens consomm√©s :
  - Input  : 6.2M tokens/mois (~1400 tokens/req)
  - Output : 2.1M tokens/mois (~480 tokens/req)

Co√ªt Claude :
  - Input  : 6.2M √ó $3/1M  = $18.60
  - Output : 2.1M √ó $15/1M = $31.50
  - Total                  = $50.10/mois
```

---

## S√©curit√© et Conformit√©

### Mesures de S√©curit√© Impl√©ment√©es

**Infrastructure** :
- ‚úÖ Pare-feu UFW (ports 22, 80, 443 uniquement)
- ‚úÖ SSH par cl√© uniquement (password auth disabled)
- ‚úÖ Fail2ban (bannissement auto apr√®s 5 tentatives)
- ‚úÖ Nginx reverse proxy avec rate limiting
- ‚úÖ HTTPS obligatoire (Let's Encrypt)
- ‚úÖ Headers de s√©curit√© (HSTS, CSP, X-Frame-Options)

**Application** :
- ‚úÖ Validation Pydantic de tous les inputs
- ‚úÖ Pas d'ex√©cution de code user (pas d'eval, pas d'exec)
- ‚úÖ Sanitization des queries (injection prevention)
- ‚úÖ Secrets en variables d'environnement (.env gitignored)
- ‚úÖ API key rotation tous les 90 jours

**Donn√©es** :
- ‚úÖ Logs anonymis√©s (IP hash√©s)
- ‚úÖ Pas de stockage de donn√©es sensibles utilisateur
- ‚úÖ Chiffrement TLS in-transit
- ‚ö†Ô∏è **TODO** : Chiffrement at-rest (disques)

### Conformit√© RGPD

**Donn√©es personnelles collect√©es** :
- User ID Slack (pseudonyme)
- Timestamps des requ√™tes
- IP address (anonymis√©e apr√®s 24h)

**Droits utilisateurs** :
- ‚úÖ Droit d'acc√®s : Export JSON de toutes les requ√™tes via `/api/gdpr/export`
- ‚úÖ Droit de rectification : Modification du user_id via API admin
- ‚úÖ Droit √† l'oubli : Anonymisation compl√®te via `/api/gdpr/anonymize`
- ‚úÖ Droit √† la portabilit√© : Format JSON standardis√©

**Base l√©gale** :
- Int√©r√™t l√©gitime (am√©lioration du service, statistiques)
- Dur√©e de conservation : 12 mois (logs), 6 mois (audit)

**DPO** : Contact SEPEFREI pour toute question RGPD.

---

## Limitations et Consid√©rations

### Limitations Techniques Actuelles

**1. Pas de m√©moire conversationnelle**
- Chaque question trait√©e ind√©pendamment
- Pas de contexte multi-turn ("Et pour une SASU ?" apr√®s "Comment cr√©er une JE ?")
- **Impact** : Utilisateur doit reformuler compl√®tement chaque question
- **Workaround** : Stocker historique c√¥t√© client (Slack thread)

**2. Recherche non distribu√©e**
- Index entier sur un seul serveur
- Pas de sharding possible
- **Impact** : Scaling limit√© √† ~100k chunks max
- **Mitigation** : Suffisant pour 5-10 ans de croissance

**3. Pas de cache intelligent**
- Questions identiques recalcul√©es
- Pas de cache s√©mantique (questions similaires)
- **Impact** : Latence et co√ªt LLM non optimis√©s
- **Mitigation** : Redis cache basique en Q1 2025

**4. Scraping manuel trigger**
- N√©cessite intervention humaine pour update
- Pas de d√©tection auto des changements sources
- **Impact** : Index peut √™tre obsol√®te
- **Mitigation** : Cron automation en Q1 2025

### Limitations Fonctionnelles

**1. Texte uniquement**
- Pas de traitement d'images, PDFs scann√©s, tableaux complexes
- **Impact** : Certains documents non indexables
- **Workaround** : OCR manuel + ajout au corpus

**2. Pas de g√©n√©ration de documents**
- Comply r√©pond mais ne cr√©e pas de contrats/rapports
- **Impact** : Utilisateur doit r√©diger lui-m√™me
- **√âvolution** : Templates + g√©n√©ration en Q3 2025

**3. D√©pendance totale Claude**
- Si API Anthropic down ‚Üí service inop√©rant
- Pas de fallback provider
- **Impact** : Single point of failure
- **Mitigation** : Multi-LLM support en Q2 2025

### Limitations Organisationnelles

**1. Pas de gestion de versions du corpus****Bot Discord** :
```python
import discord
from discord.ext import commands

class ComplyBot(commands.Bot):
    def __init__(self):
        intents = discord.Intents.default()
        intents.message_content = True
        super().__init__(command_prefix='!comply ', intents=intents)
    
    @commands.command()
    async def ask(self, ctx, *, question):
        """!comply ask Comment d√©clarer la TVA ?"""
        async with ctx.typing():
            response = await call_comply_api(question)
            
            embed = discord.Embed(
                title="üí° Comply",
                description=response['answer'],
                color=discord.Color.blue()
            )
            
            # Ajout des sources
            sources_text = "\n".join([
                f"‚Ä¢ [{s['type']}] {s['source_file']}"
                for s in response['sources'][:3]
            ])
            embed.add_field(name="üìö Sources", value=sources_text, inline=False)
            
            await ctx.send(embed=embed)
```

**Mobile App Native** :
- React Native / Flutter
- Interface conversationnelle
- Mode offline (cache local des FAQ communes)
- Notifications push pour alertes audit/conformit√©

**API Webhooks** :
```python
@router.post("/webhooks/notion")
async def notion_webhook(request: NotionWebhookRequest):
    """Int√©gration Notion : analyse automatique des docs"""
    page_content = request.page_content
    
    # Analyse de conformit√©
    compliance_check = await check_compliance(page_content)
    
    # Update Notion page avec r√©sultats
    update_notion_page(
        page_id=request.page_id,
        compliance_status=compliance_check
    )
```

#### 4. Gouvernance et Audit Trail

**Tra√ßabilit√© compl√®te** :
```python
class AuditLogger:
    def log_query(self, user_id, query, response, context):
        """Log complet de chaque interaction"""
        audit_entry = {
            'id': generate_uuid(),
            'timestamp': datetime.now().isoformat(),
            'user': {
                'id': user_id,
                'role': context.get('role'),
                'je': context.get('je_name')
            },
            'query': {
                'text': query,
                'type': response['detected_type'],
                'hash': hashlib.sha256(query.encode()).hexdigest()
            },
            'response': {
                'answer_preview': response['answer'][:200],
                'confidence': response['confidence'],
                'sources_used': [s['chunk_id'] for s in response['sources']],
                'llm_model': response['metadata']['llm_model'],
                'tokens': {
                    'input': response['metadata']['input_tokens'],
                    'output': response['metadata']['output_tokens']
                }
            },
            'metadata': {
                'ip_address': anonymize_ip(context.get('ip')),
                'user_agent': context.get('user_agent'),| Composant | Minimum | Recommand√© | Production |
|-----------|---------|------------|------------|
| **CPU** | 2 vCores | 4 vCores | 6 vCores |
| **RAM** | 4 GB | 8 GB | 16 GB |
| **Stockage** | 20 GB SSD | 40 GB SSD | 80 GB SSD |
| **Bande passante** | 100 Mbps | 200 Mbps | 1 Gbps |
| **OS** | Debian 11 | Debian 12 | Debian 12 |

**Fournisseurs VPS Recommand√©s (France)** :

**1. Contabo - VPS S SSD** (Recommandation principale)
- **Prix** : ~5,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 200 GB SSD NVMe
- **Localisation** : N√ºrnberg (Allemagne) ou Paris (France)
- **Avantages** : Excellent rapport qualit√©/prix, ressources g√©n√©reuses
- **Lien** : [https://contabo.com/en/vps/](https://contabo.com/en/vps/)

**2. Hetzner - CX31**
- **Prix** : ~9,50‚Ç¨/mois
- **Config** : 2 vCores, 8 GB RAM, 80 GB SSD
- **Localisation** : Falkenstein ou Helsinki
- **Avantages** : Infrastructure fiable, excellente connectivit√©
- **Lien** : [https://www.hetzner.com/cloud](https://www.hetzner.com/cloud)

**3. OVH - VPS Comfort**
- **Prix** : ~11,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 160 GB SSD
- **Localisation** : Gravelines, Roubaix, Strasbourg (France)
- **Avantages** : Fran√ßais, support fran√ßais, infrastructure r√©siliente
- **Lien** : [https://www.ovhcloud.com/fr/vps/](https://www.ovhcloud.com/fr/vps/)

**4. Scaleway - DEV1-M**
- **Prix** : ~7,99‚Ç¨/mois
- **Config** : 3 vCores, 4 GB RAM, 40 GB SSD
- **Localisation** : Paris, Amsterdam
- **Avantages** : √âcosyst√®me cloud complet, IPv6 natif
- **Lien** : [https://www.scaleway.com/en/pricing/](https://www.scaleway.com/en/pricing/)

**Notre choix pour Junior-Entreprises** : **Contabo VPS S SSD**
- Meilleur compromis co√ªt/performance pour usage Comply
- Ressources largement suffisantes (8 GB RAM = confortable pour l'index)
- Co√ªt mensuel accessible pour budget JE (~72‚Ç¨/an)

### Architecture R√©seau et S√©curit√©

**Configuration pare-feu (UFW)** :
```bash
# Installation UFW
apt install ufw -y

# Configuration par d√©faut
ufw default deny incoming
ufw default allow outgoing

# Autorisation SSH (changez 22 si port custom)
ufw allow 22/tcp

# Autorisation HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# Activation
ufw enable

# V√©rification
ufw status verbose
```

**Configuration SSH s√©curis√©e** (`/etc/ssh/sshd_config`) :
```bash
# D√©sactivation login root
PermitRootLogin no

# Authentification par cl√© uniquement
PasswordAuthentication no
PubkeyAuthentication yes

# D√©sactivation X11 forwarding
X11Forwarding no

# Port custom (optionnel, s√©curit√© par obscurit√©)
Port 2222

# Red√©marrage SSH
systemctl restart sshd
```

**Reverse Proxy Nginx** :
```nginx
# /etc/nginx/sites-available/comply

upstream comply_backend {
    server 127.0.0.1:8000;
    keepalive 32;
}

server {
    listen 80;
    server_name comply.votre-je.fr;
    
    # Redirection HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name comply.votre-je.fr;
    
    # Certificats Let's Encrypt
    ssl_certificate /etc/letsencrypt/live/comply.votre-je.fr/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/comply.votre-je.fr/privkey.pem;
    
    # Configuration SSL moderne
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    
    # Headers de s√©curit√©
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    
    # Logs
    access_log /var/log/nginx/comply_access.log;
    error_log /var/log/nginx/comply_error.log;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=comply_limit:10m rate=10r/s;
    limit_req zone=comply_limit burst=20 nodelay;
    
    # Proxy vers FastAPI
    location / {
        proxy_pass http://comply_backend;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint (pas de rate limit)
    location /health {
        limit_req off;
        proxy_pass http://comply_backend;
    }
}
```

**Certificat SSL Let's Encrypt (gratuit)** :
```bash
# Installation Certbot
apt install certbot python3-certbot-nginx -y

# G√©n√©ration certificat
certbot --nginx -d comply.votre-je.fr

# Renouvellement automatique (cron)
echo "0 3 * * * certbot renew --quiet" | crontab -
```

---

## Pr√©requis Serveur

### Installation de l'Environnement

**Script d'installation compl√®te** :
```bash
#!/bin/bash
# install_comply_environment.sh

set -e

echo "=== COMPLY - Installation de l'environnement ==="

# Mise √† jour syst√®me
echo "[1/8] Mise √† jour du syst√®me..."
apt update && apt upgrade -y

# Installation Python 3.11
echo "[2/8] Installation Python 3.11..."
apt install -y software-properties-common
add-apt-repository ppa:deadsnakes/ppa -y
apt update
apt install -y python3.11 python3.11-venv python3.11-dev python3-pip

# V√©rification Python
python3.11 --version

# Installation Git
echo "[3/8] Installation Git..."
apt install -y git

# Installation Docker (optionnel)
echo "[4/8] Installation Docker..."
apt install -y docker.io docker-compose
systemctl enable docker
systemctl start docker

# Installation d√©pendances syst√®me pour Selenium
echo "[5/8] Installation d√©pendances Selenium..."
apt install -y chromium-browser chromium-chromedriver
apt install -y xvfb  # X Virtual Framebuffer pour headless

# Installation Nginx
echo "[6/8] Installation Nginx..."
apt install -y nginx
systemctl enable nginx

# Installation Certbot
echo "[7/8] Installation Certbot..."
apt install -y certbot python3-certbot-nginx

# Cr√©ation utilisateur d√©di√©
echo "[8/8] Cr√©ation utilisateur comply..."
useradd -m -s /bin/bash comply
usermod -aG sudo comply

echo "=== Installation termin√©e ==="
echo "Prochaine √©tape: Cloner le repository et installer les d√©pendances Python"
```

**Ex√©cution** :
```bash
chmod +x install_comply_environment.sh
sudo ./install_comply_environment.sh
```

### Configuration de l'Application

**Clonage du repository** :
```bash
# Connexion en tant qu'utilisateur comply
su - comply

# Clonage
git clone https://github.com/sepefrei/comply.git
cd comply

# Cr√©ation environnement virtuel
python3.11 -m venv venv
source venv/bin/activate

# Installation d√©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

**Fichier requirements.txt** :
```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Machine Learning & NLP
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3

# LLM
anthropic==0.7.8

# Scraping
selenium==4.15.2
beautifulsoup4==4.12.2
lxml==4.9.3

# Utils
python-dotenv==1.0.0
tenacity==8.2.3
pyyaml==6.0.1

# Logging & Monitoring
loguru==0.7.2

# Testing (dev)
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

**Configuration .env** :
```bash
# Copie du template
cp .env.example .env

# √âdition
nano .env
```

Contenu `.env` :
```bash
# Environment
ENVIRONMENT=production

# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Application
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=4

# Index Configuration
INDEX_FILE_PATH=/home/comply/comply/data/index/kiwi_advanced_index.pkl
MAX_CHUNKS_CONTEXT=10
DEFAULT_TOP_K=10

# LLM Configuration
LLM_MODEL=claude-sonnet-4-5-20250929
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.3

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/comply/app.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Security
ALLOWED_ORIGINS=https://comply.votre-je.fr,https://votre-je.slack.com
API_KEY_ENABLED=false
# API_KEY=your-secret-api-key

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
```

### Service systemd

**Cr√©ation du service** (`/etc/systemd/system/comply.service`) :
```ini
[Unit]
Description=Comply - AI Assistant for Junior-Entreprises
After=network.target

[Service]
Type=simple
User=comply
Group=comply
WorkingDirectory=/home/comply/comply
Environment="PATH=/home/comply/comply/venv/bin"

ExecStart=/home/comply/comply/venv/bin/uvicorn main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --log-level info \
    --access-log \
    --use-colors

Restart=always
RestartSec=5

# Limites ressources
LimitNOFILE=65536
LimitNPROC=4096
MemoryLimit=12G
CPUQuota=400%

# S√©curit√©
PrivateTmp=true
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/comply/comply/data /var/log/comply

[Install]
WantedBy=multi-user.target
```

**Activation et d√©marrage** :
```bash
# Rechargement systemd
sudo systemctl daemon-reload

# Activation au d√©marrage
sudo systemctl enable comply

# D√©marrage du service
sudo systemctl start comply

# V√©rification du statut
sudo systemctl status comply

# Logs en temps r√©el
sudo journalctl -u comply -f
```

### Logging Avanc√©

**Configuration Loguru** :
```python
from loguru import logger
import sys

# Configuration des logs
logger.remove()  # Supprime le handler par d√©faut

# Console (stdout)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
    level="INFO",
    colorize=True
)

# Fichier de logs rotatifs
logger.add(
    "/var/log/comply/app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}",
    level="INFO",
    rotation="10 MB",
    retention="30 days",
    compression="zip"
)

# Fichier d'erreurs s√©par√©
logger.add(
    "/var/log/comply/errors.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}\n{exception}",
    level="ERROR",
    rotation="5 MB",
    retention="60 days",
    backtrace=True,
    diagnose=True
)
```

**Utilisation dans le code** :
```python
# Logs structur√©s
logger.info("Index loaded", version=INDEX['version'], chunks=INDEX['statistics']['n_chunks'])

# Logs de requ√™tes
logger.info("Processing question", 
    question=request.question[:50],
    detected_type=detected_type,
    user_id=user_context.get('id')
)

# Logs d'erreurs avec contexte
try:
    result = vector_search(query)
except Exception as e:
    logger.error("Vector search failed", 
        query=query,
        error=str(e),
        exc_info=True
    )
    raise
```

---

## Roadmap Technique

### Court Terme (Q1-Q2 2025)

#### 1. Automatisation Compl√®te du Scraping

**Objectif** : Supprimer l'intervention humaine du processus de mise √† jour des donn√©es.

**Impl√©mentation** :
```python
# cron_scraper.py
import schedule
import time
from scrapers.kiwi_scraper import KiwiScraper
from utils.diff_detector import DiffDetector

def scheduled_scrape_job():
    """Job de scraping diff√©rentiel automatique"""
    logger.info("Starting scheduled scrape job")
    
    scraper = KiwiScraper()
    diff_detector = DiffDetector()
    
    # Scraping des 3 sources
    sources = ['legal', 'rse', 'faq']
    changes_detected = False
    
    for source in sources:
        logger.info(f"Scraping {source}...")
        new_data = scraper.scrape(source)
        
        # D√©tection de changements (hash comparison)
        has_changes = diff_detector.compare(
            source,
            new_data,
            f'data/raw/{source}_latest.json'
        )
        
        if has_changes:
            logger.info(f"Changes detected in {source}")
            changes_detected = True
            
            # Sauvegarde nouvelle version
            save_json(new_data, f'data/raw/{source}_{date.today()}.json')
            save_json(new_data, f'data/raw/{source}_latest.json')
    
    # Si changements d√©tect√©s ‚Üí r√©indexation automatique
    if changes_detected:
        logger.info("Triggering automatic reindexation")
        trigger_reindex()
        
        # Notification Slack
        send_slack_notification(
            "üîÑ Comply index updated",
            f"New data scraped and indexed. {len(sources)} sources updated."
        )

# Planification : tous les jours √† 3h du matin
schedule.every().day.at("03:00").do(scheduled_scrape_job)

if __name__ == "__main__":
    logger.info("Cron scraper started")
    while True:
        schedule.run_pending()
        time.sleep(60)
```

**Configuration cron syst√®me** :
```bash
# Ajout au crontab de l'utilisateur comply
crontab -e

# Ajout de la ligne
0 3 * * * /home/comply/comply/venv/bin/python /home/comply/comply/cron_scraper.py >> /var/log/comply/cron.log 2>&1
```

**D√©tection diff√©rentielle** :
```python
class DiffDetector:
    def compare(self, source_name, new_data, old_file_path):
        """Compare new data with previous version"""
        if not os.path.exists(old_file_path):
            return True  # Premier scraping
        
        with open(old_file_path, 'r') as f:
            old_data = json.load(f)
        
        # Calcul hash du contenu
        new_hash = self._compute_hash(new_data)
        old_hash = self._compute_hash(old_data)
        
        if new_hash != old_hash:
            # Analyse d√©taill√©e des diff√©rences
            diff_stats = self._compute_diff_stats(old_data, new_data)
            logger.info(f"Diff stats for {source_name}", **diff_stats)
            return True
        
        return False
    
    def _compute_hash(self, data):
        """Compute SHA256 hash of data"""
        import hashlib
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compute_diff_stats(self, old, new):
        """Compute detailed diff statistics"""
        # Logique sp√©cifique selon la structure
        return {
            'added': 0,
            'modified': 0,
            'deleted': 0
        }
```

**R√©indexation incr√©mentale** :
```python
def incremental_reindex(changed_sources):
    """Reindex only modified sources"""
    logger.info(f"Starting incremental reindex for: {changed_sources}")
    
    # Chargement de l'index actuel
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        current_index = pickle.load(f)
    
    # Suppression des chunks des sources modifi√©es
    chunks_to_keep = [
        chunk for chunk in current_index['chunks']
        if chunk['metadata']['source_file'] not in changed_sources
    ]
    
    # Ajout des nouveaux chunks
    for source in changed_sources:
        new_chunks = process_source(source)
        chunks_to_keep.extend(new_chunks)
    
    # R√©indexation compl√®te (vectorisation)
    builder = IndexBuilder()
    new_index = builder.build_index(chunks_to_keep)
    
    # Swap atomique
    backup_index(current_index)
    builder.save_index(new_index)
    
    logger.info("Incremental reindex completed")
```

#### 2. Monitoring et Observabilit√©

**Prometheus metrics** :
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# M√©triques
queries_total = Counter('comply_queries_total', 'Total number of queries', ['type'])
query_duration = Histogram('comply_query_duration_seconds', 'Query duration')
llm_calls_total = Counter('comply_llm_calls_total', 'Total LLM API calls')
llm_tokens_used = Counter('comply_llm_tokens_used', 'LLM tokens consumed', ['type'])
index_size = Gauge('comply_index_size_chunks', 'Number of chunks in index')

# Dans le code
@query_duration.time()
async def ask_question(request):
    queries_total.labels(type=detected_type).inc()
    # ... traitement
    llm_calls_total.inc()
    llm_tokens_used.labels(type='input').inc(usage['input_tokens'])
    llm_tokens_used.labels(type='output').inc(usage['output_tokens'])
```

**Dashboard Grafana** :
- Graphique : Requ√™tes/heure par type
- Graphique : Latence p50, p95, p99
- Graphique : Co√ªt LLM journalier (tokens √ó prix)
- Gauge : Taille de l'index
- Alerte : Latence > 5s
- Alerte : Taux d'erreur > 5%

#### 3. Cache Redis pour Performance

**Impl√©mentation** :
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cached_query(query, ttl=3600):
    """Cache les r√©ponses fr√©quentes"""
    # G√©n√©ration cl√© cache
    query_hash = hashlib.md5(query.encode()).hexdigest()
    cache_key = f"comply:query:{query_hash}"
    
    # Tentative de r√©cup√©ration du cache
    cached = redis_client.get(cache_key)
    if cached:
        logger.info("Cache hit", query=query[:50])
        return json.loads(cached)
    
    # Sinon, traitement normal
    result = process_query(query)
    
    # Mise en cache
    redis_client.setex(
        cache_key,
        ttl,
        json.dumps(result)
    )
    
    return result
```

**Strat√©gie de cache** :
- TTL court (1h) pour questions volatiles
- TTL long (24h) pour FAQ stables
- Invalidation sur r√©indexation
- Cache warming des top 100 questions

### Moyen Terme (Q3-Q4 2025)

#### 1. Migration vers Embeddings Denses

**Objectif** : Am√©liorer la pr√©cision s√©mantique avec des embeddings transformers.

**Impl√©mentation** :
```python
from sentence_transformers import SentenceTransformer

class DenseEmbeddingIndexer:
    def __init__(self):
        # Mod√®le fran√ßais optimis√©
        self.model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')
    
    def encode_chunks(self, chunks):
        texts = [chunk['text'] for chunk in chunks]
        
        # Encoding en batch
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        return embeddings  # Shape: (n_chunks, 1024)
```

**Migration FAISS** :
```python
import faiss

class FAISSIndex:
    def __init__(self, dimension=1024):
        # Index IVF avec quantization
        quantizer = faiss.IndexFlatIP(dimension)  # Inner Product
        self.index = faiss.IndexIVFPQ(
            quantizer,
            dimension,
            nlist=100,  # Nombre de clusters
            m=8,  # Sous-quantizers
            8  # Bits par sous-quantizer
        )
    
    def build(self, embeddings):
        # Entra√Ænement de l'index
        self.index.train(embeddings)
        self.index.add(embeddings)
        
        # Nombre de clusters √† visiter lors de la recherche
        self.index.nprobe = 10
    
    def search(self, query_embedding, k=10):
        distances, indices = self.index.search(query_embedding, k)
        return indices[0], distances[0]
```

**Performance attendue** :
- Pr√©cision : +15-20% (top-5 recall)
- Latence : ~20-30ms (vs 11ms TF-IDF)
- M√©moire : ~800 MB (vs 300 MB)

#### 2. Fine-Tuning Embeddings

**Dataset custom JE** :
```python
# G√©n√©ration de paires positives/n√©gatives
training_data = [
    {
        'query': "Comment d√©clarer la TVA ?",
        'positive': "Les Junior-Entreprises b√©n√©ficient du r√©gime de franchise...",
        'negative': "Pour organiser un √©v√©nement RSE..."
    },
    # ... 10k+ exemples
]

# Fine-tuning avec Sentence Transformers
from sentence_transformers import losses, InputExample

train_examples = [
    InputExample(texts=[item['query'], item['positive']])
    for item in training_data
]

model.fit(
    train_objectives=[(train_dataloader, losses.MultipleNegativesRankingLoss(model))],
    epochs=3,
    warmup_steps=100
)
```

#### 3. Multi-LLM Support

**Abstraction provider** :
```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> dict:
        pass

class ClaudeProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Claude
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation GPT-4
        pass

class MistralProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Mistral
        pass

# Factory
def get_llm_provider(provider_name: str) -> LLMProvider:
    providers = {
        'claude': ClaudeProvider(),
        'openai': OpenAIProvider(),
        'mistral': MistralProvider()
    }
    return providers[provider_name]
```

**Routing intelligent** :
```python
def route_query_to_llm(query_type, complexity):
    """S√©lection du LLM optimal selon le contexte"""
    if query_type == 'juridique' and complexity == 'high':
        return 'claude'  # Meilleur sur le raisonnement complexe
    elif query_type == 'faq' and complexity == 'low':
        return 'mistral'  # Rapide et √©conomique
    else:
        return 'claude'  # Default
```

#### 4. Feedback Loop et Active Learning

**Collecte de feedback** :
```python
class FeedbackCollector:
    def record_feedback(self, query_id, feedback_type, user_comment=None):
        """Enregistre le feedback utilisateur"""
        feedback_data = {
            'query_id': query_id,
            'timestamp': datetime.now().isoformat(),
            'feedback_type': feedback_type,  # 'positive', 'negative', 'neutral'
            'user_comment': user_comment
        }
        
        # Stockage
        save_to_database(feedback_data)
        
        # Si feedback n√©gatif ‚Üí investigation
        if feedback_type == 'negative':
            self.analyze_failure(query_id)
```

**R√©entra√Ænement p√©riodique** :
```python
def monthly_retraining():
    """R√©entra√Ænement mensuel avec les feedbacks"""
    # R√©cup√©ration des feedbacks
    feedbacks = load_feedbacks(last_30_days=True)
    
    # G√©n√©ration de nouveaux exemples d'entra√Ænement
    new_training_data = []
    for feedback in feedbacks:
        if feedback['type'] == 'negative':
            # Analyse de la requ√™te √©chou√©e
            query = get_query(feedback['query_id'])
            correct_chunks = identify_correct_chunks(query, feedback['comment'])
            
            new_training_data.append({
                'query': query,
                'positive': correct_chunks,
                'negative': query['retrieved_chunks']
            })
    
    # Fine-tuning incr√©mental
    if len(new_training_data) > 100:
        fine_tune_model(new_training_data)
        logger.info(f"Model fine-tuned with {len(new_training_data)} examples")
```

### Long Terme (2026+)

#### 1. Multimodalit√©

**Support documents PDF/Images** :
```python
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

class MultimodalProcessor:
    def process_pdf(self, pdf_path):
        """Extraction texte + images d'un PDF"""
        # Conversion PDF ‚Üí images
        images = convert_from_path(pdf_path)
        
        extracted_text = ""
        for image in images:
            # OCR
            text = pytesseract.image_to_string(image, lang='fra')
            extracted_text += text + "\n\n"
        
        return extracted_text
    
    def process_image(self, image_path):
        """Extraction texte d'une image"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='fra')
        return text
```

**Vision LLM pour tableaux complexes** :
```python
async def analyze_table_with_vision(image):
    """Utilise GPT-4 Vision ou Claude pour analyser un tableau"""
    response = await vision_llm.analyze(
        image=image,
        prompt="Extrait les donn√©es de ce tableau sous forme JSON structur√©"
    )
    return response
```

#### 2. G√©n√©ration de Documents

**Templates Jinja2** :
```python
from jinja2 import Template

def generate_contract(template_name, context):
    """G√©n√©ration de contrat personnalis√©"""
    template = load_template(f"templates/{template_name}.j2")
    
    # Enrichissement du contexte via LLM
    enriched_context = llm_enrich_context(context)
    
    # G√©n√©ration
    document = template.render(**enriched_context)
    
    # Conversion Markdown ‚Üí PDF
    pdf = convert_md_to_pdf(document)
    
    return pdf
```

**Exemple** : G√©n√©ration automatique de Convention d'√âtude √† partir d'un brief client.

#### 3. Int√©gration √âtendue

**Plugin Google Workspace** :
- Add-on Google Docs : assistance r√©daction contrat
- Extension Gmail : d√©tection clauses dangereuses emails clients

**Bot Discord**[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks]
    ‚Üì
[Context Building] ‚Üí [Prompt Engineering] ‚Üí [Claude LLM] ‚Üí [Response Formatting]
    ‚Üì
[JSON R√©ponse] ‚Üí [Slack Bot / Web UI / API Client]
```

### Phase 1 : Acquisition des Donn√©es (Scraping)

#### Architecture du Scraping Selenium

Le scraping s'effectue via des scripts Python d√©di√©s par source, utilisant Selenium WebDriver pour g√©rer le JavaScript et les interactions complexes.

**Script principal** : `scrapers/kiwi_scraper.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import json
from datetime import datetime

class KiwiScraper:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def scrape_kiwi_legal(self):
        """Scrape Kiwi Legal documents"""
        base_url = "https://kiwi.cnje.fr/legal"
        self.driver.get(base_url)
        
        # Attente du chargement dynamique
        self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "document-list"))
        )
        
        documents = []
        doc_elements = self.driver.find_elements(By.CLASS_NAME, "document-item")
        
        for element in doc_elements:
            doc_data = self._extract_legal_document(element)
            documents.append(doc_data)
        
        return documents
```

**Gestion de la pagination** :
```python
def scrape_with_pagination(self, url, max_pages=None):
    page = 1
    all_data = []
    
    while True:
        print(f"Scraping page {page}...")
        self.driver.get(f"{url}?page={page}")
        
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "content"))
            )
        except TimeoutException:
            print(f"No more pages after page {page-1}")
            break
        
        page_data = self._extract_page_content()
        if not page_data:
            break
        
        all_data.extend(page_data)
        page += 1
        
        if max_pages and page > max_pages:
            break
    
    return all_data
```

**Gestion des erreurs et retry** :
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def robust_scrape(self, url):
    try:
        self.driver.get(url)
        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        return self._extract_content()
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        raise
```

#### Extraction et Nettoyage HTML

Apr√®s extraction Selenium, parsing avec BeautifulSoup pour nettoyage :

```python
from bs4 import BeautifulSoup
import re

def clean_html_content(raw_html):
    """Nettoyage HTML et extraction texte pertinent"""
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # Suppression √©l√©ments non pertinents
    for element in soup(['script', 'style', 'nav', 'footer', 'header']):
        element.decompose()
    
    # Suppression classes publicitaires
    for ad in soup.find_all(class_=['advertisement', 'popup', 'banner']):
        ad.decompose()
    
    # Extraction texte
    text = soup.get_text(separator='\n', strip=True)
    
    # Nettoyage espaces multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text

def extract_metadata(soup):
    """Extraction m√©tadonn√©es structur√©es"""
    metadata = {}
    
    # Titre
    title_tag = soup.find('h1') or soup.find('title')
    metadata['title'] = title_tag.get_text(strip=True) if title_tag else "Unknown"
    
    # Date publication
    date_tag = soup.find('time') or soup.find(class_='date')
    if date_tag:
        metadata['date'] = date_tag.get('datetime') or date_tag.get_text(strip=True)
    
    # Auteur
    author_tag = soup.find(class_='author') or soup.find(rel='author')
    if author_tag:
        metadata['author'] = author_tag.get_text(strip=True)
    
    # Cat√©gorie
    category_tag = soup.find(class_='category')
    if category_tag:
        metadata['category'] = category_tag.get_text(strip=True)
    
    return metadata
```

#### Structure JSON Standardis√©e

Export dans un format JSON unifi√© facilitant le traitement ult√©rieur :

**Format Legal** :
```json
{
  "source": "kiwi_legal",
  "document_type": "statuts",
  "scraping_metadata": {
    "url": "https://kiwi.cnje.fr/legal/statuts-types-association",
    "date_scraped": "2025-01-15T10:30:00Z",
    "scraper_version": "2.1.0"
  },
  "metadata": {
    "title": "Statuts types Junior-Entreprise association loi 1901",
    "category": "juridique",
    "subcategory": "statuts",
    "publication_date": "2024-06-01",
    "author": "Commission Juridique CNJE"
  },
  "content": {
    "sections": [
      {
        "title": "TITRE I - Dispositions g√©n√©rales",
        "articles": [
          {
            "number": 1,
            "title": "D√©nomination",
            "content": "Il est fond√© entre les adh√©rents aux pr√©sents statuts..."
          }
        ]
      }
    ],
    "full_text": "STATUTS TYPES..."
  }
}
```

**Format RSE** :
```json
{
  "source": "kiwi_rse",
  "document_type": "module_rse",
  "scraping_metadata": {...},
  "metadata": {
    "title": "Module Environnement - Gestion des D√©chets",
    "pilier": "environnemental",
    "odd_concernes": [12, 13],
    "niveau_difficulte": "d√©butant"
  },
  "content": {
    "introduction": "La gestion des d√©chets...",
    "objectifs": ["R√©duire la production", "Recycler"],
    "actions": [
      {
        "titre": "Mise en place du tri s√©lectif",
        "description": "...",
        "indicateurs": ["Taux de recyclage", "Volume d√©chets"]
      }
    ]
  }
}
```

**Format FAQ** :
```json
{
  "source": "kiwi_faq",
  "document_type": "faq",
  "scraping_metadata": {...},
  "metadata": {
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2
  },
  "content": {
    "questions": [
      {
        "id": "compta_tva_001",
        "question": "Comment d√©clarer la TVA en tant que JE ?",
        "reponse": "Les Junior-Entreprises b√©n√©ficient...",
        "tags": ["tva", "d√©claration", "comptabilit√©"],
        "related_questions": ["compta_tva_002", "compta_tva_005"]
      }
    ]
  }
}
```

#### Stockage et Versioning

**Arborescence de stockage** :
```
data/
‚îú‚îÄ‚îÄ raw/                          # Donn√©es brutes apr√®s scraping
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_2025-01-15.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_2025-01-15.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_2025-01-15.json
‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_processed.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_processed.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_processed.json
‚îú‚îÄ‚îÄ index/                        # Index g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îî‚îÄ‚îÄ logs/                         # Logs de scraping
    ‚îî‚îÄ‚îÄ scraping_2025-01-15.log
```

**Logging d√©taill√©** :
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scraping_{datetime.now().date()}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Dans le scraper
logger.info(f"Starting scrape of {url}")
logger.info(f"Extracted {len(documents)} documents")
logger.warning(f"Failed to extract metadata for document {doc_id}")
logger.error(f"Scraping failed: {exception}")
```

### Phase 2 : Preprocessing & Transformation

#### Type Detection Automatique

Algorithme de d√©tection bas√© sur plusieurs signaux :

```python
class DocumentTypeDetector:
    def __init__(self):
        self.type_patterns = {
            'legal': {
                'filename': ['statuts', 'contrat', 'legal', 'juridique'],
                'fields': ['articles', 'sections', 'clauses'],
                'keywords': ['article', 'alin√©a', 'conform√©ment', 'obligation']
            },
            'rse': {
                'filename': ['rse', 'durable', 'environnement'],
                'fields': ['pilier', 'odd', 'actions'],
                'keywords': ['d√©veloppement durable', 'odd', 'responsabilit√©']
            },
            'faq': {
                'filename': ['faq', 'questions'],
                'fields': ['questions', 'reponses'],
                'keywords': ['comment', 'pourquoi', 'qu\'est-ce']
            },
            'je': {
                'filename': ['annuaire', 'je', 'junior'],
                'fields': ['nom', 'ville', 'ecole', 'domaines'],
                'keywords': ['junior-entreprise', '√©cole', 'domaine']
            }
        }
    
    def detect_type(self, document_data, filename):
        scores = {doc_type: 0 for doc_type in self.type_patterns}
        
        # Score filename
        for doc_type, patterns in self.type_patterns.items():
            for pattern in patterns['filename']:
                if pattern in filename.lower():
                    scores[doc_type] += 2
        
        # Score fields pr√©sents
        doc_fields = set(document_data.get('content', {}).keys())
        for doc_type, patterns in self.type_patterns.items():
            matching_fields = doc_fields.intersection(patterns['fields'])
            scores[doc_type] += len(matching_fields) * 3
        
        # Score keywords dans le contenu
        content_text = json.dumps(document_data).lower()
        for doc_type, patterns in self.type_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_text:
                    scores[doc_type] += 1
        
        # S√©lection du type avec le score maximal
        detected_type = max(scores, key=scores.get)
        confidence = scores[detected_type] / sum(scores.values()) if sum(scores.values()) > 0 else 0
        
        return {
            'type': detected_type if confidence > 0.3 else 'general',
            'confidence': confidence,
            'scores': scores
        }
```

#### Extraction Sp√©cialis√©e par Type

**Extracteur Legal** :
```python
class LegalExtractor:
    def extract(self, document):
        extracted_data = []
        
        sections = document['content']['sections']
        for section in sections:
            section_title = section['title']
            
            for article in section.get('articles', []):
                extracted_data.append({
                    'text': f"{article['title']}\n{article['content']}",
                    'type': 'legal',
                    'metadata': {
                        'document_type': document['document_type'],
                        'section': section_title,
                        'article_num': article['number'],
                        'title': article['title']
                    }
                })
        
        return extracted_data
```

**Extracteur FAQ** :
```python
class FAQExtractor:
    def extract(self, document):
        extracted_data = []
        
        category = document['metadata']['category']
        subcategory = document['metadata'].get('subcategory', '')
        level = document['metadata'].get('level', 1)
        
        for qa in document['content']['questions']:
            # Contexte hi√©rarchique
            context_path = f"{category}"
            if subcategory:
                context_path += f" > {subcategory}"
            
            text = f"Question: {qa['question']}\n\nR√©ponse: {qa['reponse']}"
            
            extracted_data.append({
                'text': text,
                'type': 'faq',
                'metadata': {
                    'question': qa['question'],
                    'category': category,
                    'subcategory': subcategory,
                    'level': level,
                    'context_path': context_path,
                    'tags': qa.get('tags', []),
                    'related_questions': qa.get('related_questions', [])
                }
            })
        
        return extracted_data
```

**Extracteur JE** :
```python
class JEExtractor:
    def extract(self, document):
        extracted_data = []
        
        for je in document['content']['junior_entreprises']:
            # Construction texte descriptif
            text = f"""
            Nom: {je['nom']}
            Ville: {je['ville']}
            √âcole: {je['ecole']}
            Domaines d'expertise: {', '.join(je['domaines'])}
            Contact: {je['contact']['email']}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'je',
                'metadata': {
                    'nom': je['nom'],
                    'ville': je['ville'],
                    'ecole': je['ecole'],
                    'domaines': je['domaines'],
                    'contact': je['contact'],
                    'certified': je.get('certified_cnje', False)
                }
            })
        
        return extracted_data
```

**Extracteur RSE** :
```python
class RSEExtractor:
    def extract(self, document):
        extracted_data = []
        
        pilier = document['metadata']['pilier']
        odd = document['metadata']['odd_concernes']
        
        # Extraction par action
        for action in document['content']['actions']:
            text = f"""
            Module RSE: {document['metadata']['title']}
            Pilier: {pilier}
            
            Action: {action['titre']}
            {action['description']}
            
            Indicateurs: {', '.join(action['indicateurs'])}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'rse',
                'metadata': {
                    'module': document['metadata']['title'],
                    'pilier': pilier,
                    'odd': odd,
                    'action_titre': action['titre'],
                    'indicateurs': action['indicateurs']
                }
            })
        
        return extracted_data
```

#### Smart Chunking S√©mantique

Le chunking respecte la logique m√©tier plut√¥t qu'une simple d√©coupe par longueur :

```python
class SemanticChunker:
    def __init__(self, min_length=50, max_length=1000, target_length=300):
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
    
    def chunk_text(self, text, doc_type, metadata):
        if doc_type == 'faq':
            # FAQ: chaque Q/A est un chunk autonome
            return self._chunk_faq(text, metadata)
        elif doc_type == 'legal':
            # Legal: d√©coupage par article/section
            return self._chunk_legal(text, metadata)
        elif doc_type == 'je':
            # JE: entit√© atomique, pas de d√©coupage
            return [self._create_chunk(text, doc_type, metadata)]
        elif doc_type == 'rse':
            # RSE: d√©coupage par action
            return self._chunk_rse(text, metadata)
        else:
            # G√©n√©rique: d√©coupage par paragraphes avec overlap
            return self._chunk_generic(text, doc_type, metadata)
    
    def _chunk_generic(self, text, doc_type, metadata):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_length:
                current_chunk += para + "\n\n"
            else:
                if len(current_chunk) > self.min_length:
                    chunks.append(
                        self._create_chunk(current_chunk.strip(), doc_type, metadata)
                    )
                current_chunk = para + "\n\n"
        
        if len(current_chunk) > self.min_length:
            chunks.append(
                self._create_chunk(current_chunk.strip(), doc_type, metadata)
            )
        
        return chunks
    
    def _create_chunk(self, text, doc_type, metadata):
        return {
            'text': text,
            'type': doc_type,
            'metadata': metadata,
            'length': len(text),
            'word_count': len(text.split())
        }
```

#### Enrichissement M√©tadonn√©es

Chaque chunk est enrichi automatiquement :

```python
class MetadataEnricher:
    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.category_classifier = CategoryClassifier()
    
    def enrich_chunk(self, chunk):
        text = chunk['text']
        
        # Extraction keywords automatique
        keywords = self.keyword_extractor.extract(text, top_n=5)
        chunk['metadata']['keywords'] = keywords
        
        # Classification cat√©gorie fine (si pas d√©j√† pr√©sente)
        if 'category' not in chunk['metadata']:
            category = self.category_classifier.classify(text)
            chunk['metadata']['category'] = category
        
        # Calcul de priorit√© (bas√© sur usage historique si disponible)
        chunk['metadata']['priority'] = self._calculate_priority(chunk)
        
        # Ajout timestamps
        chunk['metadata']['indexed_at'] = datetime.now().isoformat()
        
        # G√©n√©ration d'un hash pour d√©tecter les modifications
        chunk['metadata']['content_hash'] = hashlib.md5(
            text.encode()
        ).hexdigest()
        
        return chunk
    
    def _calculate_priority(self, chunk):
        # Heuristique simple : sources officielles = haute priorit√©
        priority = 0.5
        
        if chunk['type'] == 'legal':
            priority += 0.2
        if 'statuts' in chunk.get('metadata', {}).get('category', '').lower():
            priority += 0.15
        if chunk['metadata'].get('is_featured', False):
            priority += 0.1
        
        return min(priority, 1.0)
```

### Phase 3 : Vectorisation et Indexation

#### Configuration TF-IDF Optimis√©e

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IndexBuilder:
    def __init__(self):
        # Stopwords personnalis√©s JE
        self.custom_stopwords = [
            'junior', 'entreprise', 'je', 'cnje',
            '√©tudiant', '√©tudiante', 'projet', 'mission',
            'conform√©ment', 'article', 'alin√©a', 'paragraphe'
        ]
        
        # Configuration TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8,
            stop_words=self.custom_stopwords,
            sublinear_tf=True,
            norm='l2',
            strip_accents='unicode'
        )
    
    def build_index(self, chunks):
        print(f"Building index from {len(chunks)} chunks...")
        
        # Extraction des textes
        texts = [chunk['text'] for chunk in chunks]
        
        # Vectorisation TF-IDF
        print("Vectorizing with TF-IDF...")
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
        
        # R√©duction dimensionnelle SVD
        print("Applying SVD dimensionality reduction...")
        n_components = min(300, tfidf_matrix.shape[0] - 1)
        svd_model = TruncatedSVD(
            n_components=n_components,
            algorithm='randomized',
            n_iter=7,
            random_state=42
        )
        vectors_reduced = svd_model.fit_transform(tfidf_matrix)
        print(f"Reduced to {n_components} dimensions")
        
        # Construction des index secondaires
        print("Building secondary indexes...")
        metadata_index = self._build_metadata_indexes(chunks)
        
        # Assemblage de l'index complet
        index = {
            'vectorizer': self.vectorizer,
            'svd_model': svd_model,
            'vectors': vectors_reduced,
            'chunks': chunks,
            'metadata_index': metadata_index,
            'version': '2.1.0',
            'build_date': datetime.now().isoformat(),
            'statistics': {
                'n_chunks': len(chunks),
                'n_features': tfidf_matrix.shape[1],
                'n_components': n_components,
                'vocabulary_size': len(self.vectorizer.vocabulary_)
            }
        }
        
        return index
    
    def _build_metadata_indexes(self, chunks):
        by_type = {}
        by_category = {}
        by_source = {}
        
        for idx, chunk in enumerate(chunks):
            # Index by type
            chunk_type = chunk['type']
            if chunk_type not in by_type:
                by_type[chunk_type] = []
            by_type[chunk_type].append(idx)
            
            # Index by category
            category = chunk['metadata'].get('category', 'unknown')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(idx)
            
            # Index by source
            source = chunk['metadata'].get('source_file', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(idx)
        
        return {
            'by_type': by_type,
            'by_category': by_category,
            'by_source': by_source
        }
    
    def save_index(self, index, filepath='data/index/kiwi_advanced_index.pkl'):
        print(f"Saving index to {filepath}...")
        with open(filepath, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"Index saved successfully ({file_size_mb:.2f} MB)")
```

#### Processus Complet d'Indexation

Script principal orchestrant tout le pipeline :

```python
def main_indexation_pipeline():
    print("=== COMPLY INDEXATION PIPELINE ===\n")
    
    # 1. Chargement des donn√©es sources
    print("Step 1: Loading source data...")
    legal_data = load_json('data/processed/kiwi_legal_processed.json')
    rse_data = load_json('data/processed/kiwi_rse_processed.json')
    faq_data = load_json('data/processed/kiwi_faq_processed.json')
    je_data = load_json('data/processed/kiwi_je_processed.json')
    
    all_sources = [
        ('legal', legal_data),
        ('rse', rse_data),
        ('faq', faq_data),
        ('je', je_data)
    ]
    
    # 2. Extraction et chunking
    print("\nStep 2: Extracting and chunking...")
    all_chunks = []
    
    for source_type, data in all_sources:
        extractor = get_extractor(source_type)
        chunks = extractor.extract(data)
        
        # Chunking s√©mantique
        chunker = SemanticChunker()
        chunked_data = []
        for chunk in chunks:
            chunked_data.extend(
                chunker.chunk_text(
                    chunk['text'],
                    chunk['type'],
                    chunk['metadata']
                )
            )
        
        print(f"  - {source_type}: {len(chunked_data)} chunks")
        all_chunks.extend(chunked_data)
    
    print(f"Total chunks: {len(all_chunks)}")
    
    # 3. Enrichissement
    print("\nStep 3: Enriching metadata...")
    enricher = MetadataEnricher()
    enriched_chunks = [enricher.enrich_chunk(c) for c in all_chunks]
    
    # 4. Construction de l'index
    print("\nStep 4: Building vector index...")
    builder = IndexBuilder()
    index = builder.build_index(enriched_chunks)
    
    # 5. Persistance
    print("\nStep 5: Saving index...")
    builder.save_index(index)
    
    # 6. Statistiques finales
    print("\n=== INDEXATION COMPLETE ===")
    print(f"Total chunks indexed: {index['statistics']['n_chunks']}")
    print(f"Vocabulary size: {index['statistics']['vocabulary_size']}")
    print(f"Vector dimensions: {index['statistics']['n_components']}")
    print(f"Index version: {index['version']}")
    
    return index

if __name__ == "__main__":
    main_indexation_pipeline()
```

### Phase 4 : Serving et Recherche

#### Chargement de l'Index au D√©marrage

```python
from fastapi import FastAPI
import pickle

app = FastAPI(title="Comply API", version="2.1.0")

# Chargement de l'index au d√©marrage (√©v√©nement startup)
@app.on_event("startup")
async def load_index():
    global INDEX
    
    print("Loading Comply index...")
    start_time = time.time()
    
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        INDEX = pickle.load(f)
    
    load_time = time.time() - start_time
    print(f"Index loaded in {load_time:.2f}s")
    print(f"  - Version: {INDEX['version']}")
    print(f"  - Chunks: {INDEX['statistics']['n_chunks']}")
    print(f"  - Memory: {sys.getsizeof(INDEX) / (1024**2):.2f} MB")
```

#### Endpoint /ask - Impl√©mentation Compl√®te

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    context: Optional[dict] = None
    options: Optional[dict] = None

class ComprehensiveAnswer(BaseModel):
    answer: str
    confidence: float
    detected_type: str
    sources: List[dict]
    related_questions: List[str]
    processing_time_ms: int

@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    start_time = time.time()
    
    try:
        # 1. D√©tection du type de requ√™te
        query_type_result = detect_query_type(request.question)
        detected_type = query_type_result['detected_type']
        
        # 2. Recherche vectorielle avec boosting
        search_results = vector_search(
            query=request.question,
            query_type=detected_type,
            top_k=request.options.get('max_chunks', 10) if request.options else 10
        )
        
        # 3. Construction du contexte
        context_string = build_context(search_results['chunks'])
        
        # 4. Prompt engineering
        prompt = generate_prompt(
            question=request.question,
            context=context_string,
            query_type=detected_type
        )
        
        # 5. Appel LLM
        llm_response = await call_claude(prompt)
        
        # 6. Post-processing
        formatted_response = format_response(
            raw_response=llm_response['response'],
            context_chunks=search_results['chunks'],
            query_type=detected_type
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return ComprehensiveAnswer(
            answer=formatted_response['answer'],
            confidence=formatted_response['confidence'],
            detected_type=detected_type,
            sources=formatted_response['sources'],
            related_questions=formatted_response['related_questions'],
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Infrastructure Recommand√©e

### H√©bergement VPS

Pour un d√©ploiement en production, un VPS Debian offre le meilleur compromis performance/co√ªt/contr√¥le.

**Sp√©cifications recommand√©es** :

| Composant | Minimum | Recomman# Comply by Sepefrei

![Comply Logo](comply_logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks

**1. Pas de gestion de versions du corpus**
- Impossible de savoir quelle version de la doc a g√©n√©r√© une r√©ponse pass√©e
- Pas de rollback possible vers version ant√©rieure
- **Impact** : Tra√ßabilit√© limit√©e, audit complexe
- **Mitigation** : Versioning Git des JSON sources

**2. Pas de workflow de validation**
- R√©ponses g√©n√©r√©es sans review humaine
- Pas de processus d'approbation avant publication
- **Impact** : Risque de r√©ponses inexactes en production
- **Mitigation** : Feedback loop + monitoring alerting

**3. Pas de feedback loop formalis√©**
- Feedback collect√© mais pas exploit√© automatiquement
- Pas de r√©entra√Ænement bas√© sur les erreurs
- **Impact** : Am√©lioration continue manuelle
- **Mitigation** : Active learning pipeline en Q3 2025

---

## Annexes

### Glossaire Technique

**Chunk** : Unit√© s√©mantique de texte index√©e s√©par√©ment (200-500 mots typiquement). Correspond √† une question/r√©ponse FAQ, un article de statuts, une action RSE, ou une fiche JE.

**Cosine Similarity** : Mesure de similarit√© entre deux vecteurs bas√©e sur l'angle entre eux (valeur entre 0 et 1). Utilis√©e pour comparer la requ√™te utilisateur avec tous les chunks index√©s.

**Embedding / Vectorisation** : Transformation d'un texte en vecteur num√©rique haute dimension capturant sa s√©mantique. TF-IDF produit des embeddings sparse, les transformers des embeddings denses.

**LLM (Large Language Model)** : Mod√®le de langage de grande taille (milliards de param√®tres) capable de g√©n√©rer du texte coh√©rent. Exemples : Claude, GPT-4, Mistral.

**Prompt Engineering** : Art de concevoir des instructions optimales pour un LLM afin de maximiser la qualit√© et la pertinence de ses r√©ponses.

**RAG (Retrieval-Augmented Generation)** : Architecture combinant recherche documentaire (retrieval) et g√©n√©ration par LLM. Le LLM g√©n√®re une r√©ponse bas√©e sur des documents pertinents r√©cup√©r√©s.

**SVD (Singular Value Decomposition)** : Technique d'alg√®bre lin√©aire pour d√©composer et compresser des matrices. Utilis√©e ici pour r√©duire la dimensionnalit√© des vecteurs TF-IDF.

**TF-IDF (Term Frequency - Inverse Document Frequency)** : M√©thode de vectorisation textuelle donnant plus de poids aux termes rares et discriminants. Alternative l√©g√®re aux embeddings transformers.

**Top-K** : S√©lection des K √©l√©ments ayant les scores les plus √©lev√©s. Ici, r√©cup√©ration des K chunks les plus similaires √† la requ√™te.

**Boosting** : Technique consistant √† multiplier le score de pertinence d'un r√©sultat selon des crit√®res contextuels (type, cat√©gorie, source, r√©cence).

### Structure du Repository

```
comply/
‚îú‚îÄ‚îÄ README.md                          # Ce fichier
‚îú‚îÄ‚îÄ requirements.txt                    # D√©pendances Python
‚îú‚îÄ‚îÄ .env.example                        # Template de configuration
‚îú‚îÄ‚îÄ .gitignore                          
‚îú‚îÄ‚îÄ main.py                             # Point d'entr√©e FastAPI
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ask.py                      # Endpoint /ask
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py                   # Endpoints /search/*
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin.py                    # Endpoints /admin/*
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ slack.py                    # Int√©gration Slack
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py            # Recherche vectorielle
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py              # Orchestration LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py            # D√©tection type requ√™te
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ boosting.py                 # Calculs boosting
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ context_builder.py          # Construction contexte
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ request_models.py           # Mod√®les Pydantic requests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_models.py          # Mod√®les Pydantic responses
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Configuration centralis√©e
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py             # Chargement index Pickle
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py                   # Configuration logs
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ prompt_templates.py         # Templates prompts LLM
‚îÇ       ‚îú‚îÄ‚îÄ text_processing.py          # Utils traitement texte
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py                  # M√©triques Prometheus
‚îÇ
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_scraper.py                 # Scraper principal
‚îÇ   ‚îú‚îÄ‚îÄ legal_parser.py                 # Parser documents l√©gaux
‚îÇ   ‚îú‚îÄ‚îÄ rse_parser.py                   # Parser modules RSE
‚îÇ   ‚îú‚îÄ‚îÄ faq_parser.py                   # Parser FAQ
‚îÇ   ‚îî‚îÄ‚îÄ je_parser.py                    # Parser base JE
‚îÇ
‚îú‚îÄ‚îÄ indexing/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py                # D√©tection type documents
‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ legal_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rse_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faq_extractor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ je_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py                      # Smart chunking
‚îÇ   ‚îú‚îÄ‚îÄ enricher.py                     # Enrichissement m√©tadonn√©es
‚îÇ   ‚îú‚îÄ‚îÄ vectorizer.py                   # Vectorisation TF-IDF + SVD
‚îÇ   ‚îî‚îÄ‚îÄ index_builder.py                # Construction index complet
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                            # Donn√©es scrap√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/                      # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ index/                          # Index Pickle
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îÇ   ‚îî‚îÄ‚îÄ logs/                           # Logs de scraping
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ scrape_all.py                   # Script scraping complet
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py                  # Script indexation
‚îÇ   ‚îú‚îÄ‚îÄ test_query.py                   # Test de requ√™tes
‚îÇ   ‚îî‚îÄ‚îÄ cron_scraper.py                 # Job cron automatique
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_vector_search.py
‚îÇ   ‚îú‚îÄ‚îÄ test_type_detection.py
‚îÇ   ‚îú‚îÄ‚îÄ test_chunking.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îÇ
‚îú‚îÄ‚îÄ deployment/
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf                      # Config Nginx
‚îÇ   ‚îú‚îÄ‚îÄ comply.service                  # Service systemd
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml              # Docker (dev)
‚îÇ   ‚îî‚îÄ‚îÄ install.sh                      # Script d'installation
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ API.md                          # Documentation API
    ‚îú‚îÄ‚îÄ DEPLOYMENT.md                   # Guide de d√©ploiement
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md                 # Ce document
    ‚îî‚îÄ‚îÄ CONTRIBUTING.md                 # Guide de contribution
```

### Commandes Utiles

**D√©veloppement** :
```bash
# Activation environnement virtuel
source venv/bin/activate

# Lancement en mode dev (hot reload)
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Tests
pytest tests/ -v

# Linting
black app/ scrapers/ indexing/
flake8 app/ scrapers/ indexing/
mypy app/ scrapers/ indexing/
```

**Scraping** :
```bash
# Scraping complet de toutes les sources
python scripts/scrape_all.py

# Scraping d'une source sp√©cifique
python scripts/scrape_all.py --source legal
python scripts/scrape_all.py --source rse

# Scraping avec sauvegarde horodat√©e
python scripts/scrape_all.py --timestamp
```

**Indexation** :
```bash
# Construction compl√®te de l'index
python scripts/build_index.py

# Indexation incr√©mentale (sources modifi√©es uniquement)
python scripts/build_index.py --incremental

# Indexation avec stats d√©taill√©es
python scripts/build_index.py --verbose

# Test de l'index
python scripts/test_index.py
```

**Production** :
```bash
# D√©marrage du service
sudo systemctl start comply

# Arr√™t du service
sudo systemctl stop comply

# Red√©marrage
sudo systemctl restart comply

# Statut
sudo systemctl status comply

# Logs temps r√©el
sudo journalctl -u comply -f

# Logs avec filtre
sudo journalctl -u comply --since "1 hour ago" | grep ERROR

# R√©indexation sans downtime
curl -X POST https://comply.votre-je.fr/admin/reindex \
  -H "Authorization: Bearer $API_KEY"

# Statistiques de l'index
curl https://comply.votre-je.fr/stats/advanced
```

### FAQ Technique

**Q : Pourquoi le chargement de l'index est-il si rapide ?**
R : L'index complet est charg√© en RAM au d√©marrage (< 1s). Ensuite, toutes les recherches se font en m√©moire sans I/O disque. C'est une architecture "in-memory" classique pour la performance.

**Q : Que se passe-t-il si le serveur manque de RAM ?**
R : Avec 8 GB de RAM, on a une marge confortable (utilisation actuelle ~2 GB). Si l'index grossit au-del√† de 5-6 GB, il faudra upgrader le VPS ou passer √† une architecture distribu√©e (FAISS, Milvus).

**Q : Peut-on d√©ployer plusieurs instances pour la haute disponibilit√© ?**
R : Oui, en utilisant un load balancer (HAProxy, Nginx) devant plusieurs instances FastAPI. Chaque instance charge son propre index en RAM. Attention au co√ªt (chaque instance = 1 VPS).

**Q : Comment g√©rer les mises √† jour de l'index sans downtime ?**
R : M√©thode actuelle : construire le nouvel index dans un fichier temporaire, puis swap atomique via `mv`. FastAPI rechargera l'index au prochain restart (< 1s downtime). Alternative : hot reload avec signal SIGHUP custom.

**Q : Pourquoi ne pas utiliser une vraie base de donn√©es ?**
R : Pour un index de recherche vectorielle, une DB relationnelle (PostgreSQL) serait 10-100x plus lente. Les DB vectorielles (Pinecone, Milvus) ajoutent de la complexit√© et du co√ªt pour un b√©n√©fice limit√© √† ce stade. Pickle = simplicit√© maximale.

**Q : Le syst√®me est-il pr√™t pour plusieurs Junior-Entreprises simultan√©ment ?**
R : Oui, l'architecture est multi-tenant ready. Il suffit d'ajouter un champ `je_id` dans les m√©tadonn√©es des chunks et filtrer les recherches par JE. Actuellement d√©ploy√© pour SEPEFREI uniquement.

**Q : Quelle est la dur√©e de vie d'un index avant r√©indexation ?**
R : D√©pend de la fr√©quence de mise √† jour des sources. Recommandation : r√©indexation mensuelle pour les sources stables (statuts), hebdomadaire pour les sources dynamiques (FAQ).

**Q : Comment d√©bugger une r√©ponse incorrecte ?**
R : Logs d√©taill√©s disponibles avec `LOG_LEVEL=DEBUG`. Chaque requ√™te trace : type d√©tect√©, chunks r√©cup√©r√©s, scores, prompt g√©n√©r√©, r√©ponse LLM. Permet d'identifier si le probl√®me vient de la recherche ou du LLM.

**Q : Peut-on utiliser Comply offline (sans connexion internet) ?**
R : Non, car d√©pendance √† l'API Claude (cloud). Pour un usage offline, il faudrait d√©ployer un LLM local (Llama, Mistral) avec une carte GPU. Complexit√© et co√ªt significativement plus √©lev√©s.

**Q : Comment contribuer au projet ?**
R : Voir `CONTRIBUTING.md`. En r√©sum√© : fork ‚Üí branche feature ‚Üí PR vers develop. Tests obligatoires. Code review par l'√©quipe SI SEPEFREI.

---

## Contacts et Support

### √âquipe Technique

**Lucas Lantrua** - RAG Engineering & Data Pipeline  
- Email : lucas.lantrua@sepefrei.fr  
- GitHub : [@lucaslantrua](https://github.com/lucaslantrua)  
- Expertise : Scraping, vectorisation, indexation, ML/NLP

**Matteo Bonnet** - Backend & API Development  
- Email : matteo.bonnet@sepefrei.fr  
- GitHub : [@matteobonnet](https://github.com/matteobonnet)  
- Expertise : FastAPI, architecture, int√©gration LLM, DevOps

**Victoria Breuling** - Product Management  
- Email : victoria.breuling@sepefrei.fr  
- Expertise : Vision produit, analyse besoins, coordination

### Support Technique

**Pour les Junior-Entreprises utilisatrices** :
- Slack : Channel #comply-support
- Email : comply-support@sepefrei.fr
- Documentation : https://docs.comply.sepefrei.fr (√† venir)

**Pour les d√©veloppeurs** :
- GitHub Issues : https://github.com/sepefrei/comply/issues
- Pull Requests : Bienvenues sur develop
- Discord Tech SEPEFREI : Channel #comply-dev

### Ressources Externes

**Documentation des technologies utilis√©es** :
- FastAPI : https://fastapi.tiangolo.com
- Scikit-Learn : https://scikit-learn.org/stable/
- Anthropic Claude : https://docs.anthropic.com/claude/docs
- Selenium : https://www.selenium.dev/documentation/
- Pydantic : https://docs.pydantic.dev

**Articles et ressources RAG** :
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al.)
- "Building RAG-based LLM Applications" (Pinecone Learning Center)
- "Advanced RAG Techniques" (LlamaIndex Docs)

---

## Remerciements

**Comply** n'aurait pas pu voir le jour sans :

- **La Conf√©d√©ration Nationale des Junior-Entreprises (CNJE)** pour la mise √† disposition des ressources documentaires (Kiwi Legal, Kiwi RSE, base JE)
- **SEPEFREI** pour le financement du projet et le soutien organisationnel
- **Les Junior-Entrepreneurs beta-testeurs** qui ont fourni des retours pr√©cieux durant le d√©veloppement
- **La communaut√© open-source** derri√®re FastAPI, Scikit-Learn, et toutes les biblioth√®ques utilis√©es

---

## Licence

**Comply** est un projet propri√©taire de **SEPEFREI - Conf√©d√©ration Nationale des Junior-Entreprises**.

Utilisation r√©serv√©e aux Junior-Entreprises membres de la CNJE dans le cadre de leurs activit√©s statutaires.

Toute utilisation, reproduction ou distribution en dehors de ce cadre n√©cessite une autorisation √©crite pr√©alable de SEPEFREI.

¬© 2025 SEPEFREI - Tous droits r√©serv√©s

---

## Changelog

### Version 2.1.0 (Janvier 2025) - Current

**Nouvelles fonctionnalit√©s** :
- ‚ú® Support des requ√™tes RSE avanc√©es avec mapping ODD
- ‚ú® Endpoint `/legal/guidance` pour assistance juridique sp√©cialis√©e
- ‚ú® G√©n√©ration automatique de questions li√©es
- ‚ú® Calcul de score de confiance des r√©ponses

**Am√©liorations** :
- ‚ö° Optimisation du boosting contextuel (+12% pr√©cision)
- ‚ö° R√©duction latence moyenne de 2.3s √† 1.8s
- üìù Documentation technique compl√®te (ce README)
- üîß Configuration via .env (pas de hardcoding)

**Corrections** :
- üêõ Fix crash lors de requ√™tes vides
- üêõ Gestion des timeouts Claude API
- üêõ Encoding UTF-8 corrig√© pour caract√®res sp√©ciaux

### Version 2.0.0 (D√©cembre 2024)

**Breaking changes** :
- üîÑ Migration vers Claude Sonnet 4.5 (incompatible v1)
- üîÑ Nouveau format index (r√©indexation obligatoire)
- üîÑ API endpoints restructur√©s

**Nouvelles fonctionnalit√©s** :
- ‚ú® Multi-index par type/cat√©gorie/source
- ‚ú® Boosting contextuel intelligent
- ‚ú® Int√©gration Slack officielle
- ‚ú® Logs structur√©s avec Loguru

### Version 1.0.0 (Octobre 2024)

**Release initiale** :
- ‚ú® Pipeline complet scraping ‚Üí indexation ‚Üí API
- ‚ú® Vectorisation TF-IDF + SVD
- ‚ú® Int√©gration Claude 3 Sonnet
- ‚ú® Endpoints FastAPI de base
- ‚ú® Support FAQ, Legal, JE

---

## Conclusion

**Comply** repr√©sente une avanc√©e significative dans l'automatisation du knowledge management pour les Junior-Entreprises. En combinant des techniques √©prouv√©es de recherche vectorielle (TF-IDF, SVD) avec la puissance des mod√®les de langage de derni√®re g√©n√©ration (Claude), le syst√®me offre une assistance intelligente accessible √† tous les membres d'une JE, quel que soit leur niveau d'expertise.

L'architecture modulaire et l'approche API-first garantissent une √©volutivit√© technique √† long terme. Les choix techniques (Python, FastAPI, Scikit-Learn, Pickle) privil√©gient la simplicit√© de d√©ploiement et la maintenabilit√© plut√¥t que la sophistication excessive. Cette philosophie "start simple, scale smart" permet un d√©ploiement rapide sur infrastructure l√©g√®re tout en conservant des marges d'am√©lioration importantes.

La roadmap ambitieuse (scraping automatique, embeddings denses, multi-LLM, feedback loop) assure que Comply continuera d'√©voluer pour r√©pondre aux besoins croissants des Junior-Entreprises en mati√®re de conformit√©, formation et efficacit√© op√©rationnelle.

**Comply n'est pas qu'un chatbot, c'est une infrastructure de connaissance r√©utilisable et extensible, pens√©e pour durer.**

---

**üöÄ Pr√™t √† d√©ployer Comply dans votre Junior-Entreprise ?**

Consultez le guide d'installation complet dans `DEPLOYMENT.md` ou contactez l'√©quipe technique SEPEFREI pour un accompagnement personnalis√©.

**Pour toute question technique, am√©lioration sugg√©r√©e ou bug report** : comply-support@sepefrei.fr

---

*Document r√©dig√© par l'√©quipe P√¥le SI & Performance SEPEFREI - Janvier 2025*  
*Version du document : 1.0*  
*Derni√®re mise √† jour : 15 janvier 2025*                'ip_address': anonymize_ip(context.get('ip')),
                'user_agent': context.get('user_agent'),
                'processing_time_ms': response['processing_time_ms']
            }
        }
        
        # Stockage s√©curis√©
        store_audit_entry(audit_entry)
        
        return audit_entry['id']
```

**Export audit pour RGPD** :
```python
def export_user_data(user_id):
    """Export de toutes les donn√©es d'un utilisateur"""
    queries = get_user_queries(user_id)
    
    export_data = {
        'user_id': user_id,
        'export_date': datetime.now().isoformat(),
        'queries': queries,
        'statistics': {
            'total_queries': len(queries),
            'date_range': {
                'first': queries[0]['timestamp'],
                'last': queries[-1]['timestamp']
            }
        }
    }
    
    return export_data

def anonymize_user_data(user_id):
    """Anonymisation compl√®te des donn√©es utilisateur"""
    # Remplacement user_id par hash irr√©versible
    anonymized_id = hashlib.sha256(f"{user_id}_{SECRET_SALT}".encode()).hexdigest()
    
    # Update de tous les logs
    update_audit_logs(user_id, anonymized_id)
```

**Certification ISO 27001** :
- Chiffrement at-rest (disques)
- Chiffrement in-transit (TLS)
- Rotation des secrets tous les 90 jours
- Backup chiffr√© quotidien
- Disaster recovery plan (RTO < 4h, RPO < 1h)

---

## Architecture D√©taill√©e

### Diagramme de S√©quence Complet

**[IMAGE REQUISE : Diagramme de s√©quence d√©taill√© d'une requ√™te]**

```mermaid
sequenceDiagram
    actor User
    participant Slack
    participant Nginx
    participant FastAPI
    participant TypeDetector
    participant IndexLoader
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter
    participant AuditLogger

    User->>Slack: @comply Comment modifier les statuts ?
    Slack->>Nginx: POST /api/slack/events
    Nginx->>FastAPI: Forward request
    
    FastAPI->>FastAPI: Parse Slack event
    FastAPI->>TypeDetector: Analyze query
    Note over TypeDetector: NLP classification<br/>Keywords matching<br/>Score calculation
    TypeDetector-->>FastAPI: {type: "juridique", confidence: 0.89}
    
    FastAPI->>IndexLoader: Get index
    Note over IndexLoader: Index already in RAM<br/>No I/O needed
    IndexLoader-->>FastAPI: Index reference
    
    FastAPI->>VectorSearch: Search(query, type="juridique")
    Note over VectorSearch: TF-IDF transform<br/>SVD transform<br/>Cosine similarity
    VectorSearch-->>FastAPI: Top 100 candidates
    
    FastAPI->>Booster: Apply boosting
    Note over Booster: Type boost √ó1.3<br/>Category boost √ó1.2<br/>Source boost √ó1.15<br/>Recency boost √ó1.1
    Booster-->>FastAPI: Top 10 final chunks
    
    FastAPI->>ContextBuilder: Build context
    Note over ContextBuilder: Aggregate chunks<br/>Deduplicate<br/>Format with metadata
    ContextBuilder-->>FastAPI: Structured context string
    
    FastAPI->>PromptEngine: Generate prompt
    Note over PromptEngine: Select legal template<br/>Inject context<br/>Add instructions
    PromptEngine-->>FastAPI: Final prompt (1423 tokens)
    
    FastAPI->>Claude: POST /v1/messages
    Note over Claude: Claude Sonnet 4.5<br/>Temperature: 0.3<br/>Max tokens: 2000
    Claude-->>FastAPI: Generated response (487 tokens)
    
    FastAPI->>ResponseFormatter: Format response
    Note over ResponseFormatter: Extract sources<br/>Calculate confidence<br/>Generate related questions<br/>Structure JSON
    ResponseFormatter-->>FastAPI: Formatted response
    
    FastAPI->>AuditLogger: Log interaction
    Note over AuditLogger: Store query/response<br/>Anonymize user data<br/>Compliance tracking
    AuditLogger-->>FastAPI: Audit ID
    
    FastAPI-->>Nginx: JSON response
    Nginx-->>Slack: Forward response
    Slack-->>User: Display formatted answer with sources
```

### Architecture en Couches D√©taill√©e

**Layer 1 : Data Sources**
```
Sources Externes
‚îú‚îÄ‚îÄ Kiwi Legal (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Statuts types (10 documents)
‚îÇ   ‚îú‚îÄ‚îÄ Contrats mod√®les (25 templates)
‚îÇ   ‚îú‚îÄ‚îÄ R√®glements int√©rieurs (8 versions)
‚îÇ   ‚îî‚îÄ‚îÄ Documentation juridique (50+ articles)
‚îú‚îÄ‚îÄ Kiwi RSE (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Modules environnementaux (15)
‚îÇ   ‚îú‚îÄ‚îÄ Modules sociaux (12)
‚îÇ   ‚îú‚îÄ‚îÄ Modules gouvernance (8)
‚îÇ   ‚îî‚îÄ‚îÄ Mapping ODD (17 objectifs)
‚îú‚îÄ‚îÄ Kiwi FAQ (HTTPS scraping)
‚îÇ   ‚îú‚îÄ‚îÄ Cat√©gories niveau 1 (12)
‚îÇ   ‚îú‚îÄ‚îÄ Sous-cat√©gories niveau 2 (47)
‚îÇ   ‚îî‚îÄ‚îÄ Questions/r√©ponses niveau 3 (320+)
‚îî‚îÄ‚îÄ Base Junior-Entreprises (JSON/CSV)
    ‚îú‚îÄ‚îÄ JE fran√ßaises (~200)
    ‚îú‚îÄ‚îÄ M√©tadonn√©es (√©cole, ville, domaines)
    ‚îî‚îÄ‚îÄ Contacts (email, t√©l√©phone, site)
```

**Layer 2 : Acquisition Pipeline**
```
Selenium WebDriver
‚îú‚îÄ‚îÄ Chrome Headless (configur√© via ChromeOptions)
‚îú‚îÄ‚îÄ WebDriverWait (timeout 10s)
‚îú‚îÄ‚îÄ Navigation programmatique
‚îÇ   ‚îú‚îÄ‚îÄ Login automatique (si n√©cessaire)
‚îÇ   ‚îú‚îÄ‚îÄ Parcours des menus
‚îÇ   ‚îî‚îÄ‚îÄ Gestion de la pagination
‚îî‚îÄ‚îÄ Extraction HTML
    ‚îú‚îÄ‚îÄ BeautifulSoup4 parsing
    ‚îú‚îÄ‚îÄ Nettoyage (scripts, styles, nav)
    ‚îî‚îÄ‚îÄ Normalisation (encoding UTF-8)

Scripts de Nettoyage Python
‚îú‚îÄ‚îÄ Suppression √©l√©ments non pertinents
‚îú‚îÄ‚îÄ Extraction m√©tadonn√©es (auteur, date, cat√©gorie)
‚îú‚îÄ‚îÄ Structuration s√©mantique (sections, articles)
‚îî‚îÄ‚îÄ Export JSON standardis√©
    ‚îú‚îÄ‚îÄ Format Legal
    ‚îú‚îÄ‚îÄ Format RSE
    ‚îú‚îÄ‚îÄ Format FAQ
    ‚îî‚îÄ‚îÄ Format JE
```

**Layer 3 : Processing Engine**
```
Type Detection
‚îú‚îÄ‚îÄ Analyse filename (regex patterns)
‚îú‚îÄ‚îÄ Inspection structure JSON (champs pr√©sents)
‚îú‚îÄ‚îÄ Analyse s√©mantique contenu (keywords)
‚îî‚îÄ‚îÄ Score de confiance (threshold 0.3)

Extraction Sp√©cialis√©e
‚îú‚îÄ‚îÄ LegalExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Parse sections/articles
‚îÇ   ‚îú‚îÄ‚îÄ Extraction num√©ros articles
‚îÇ   ‚îî‚îÄ‚îÄ D√©tection r√©f√©rences crois√©es
‚îú‚îÄ‚îÄ RSEExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Identification pilier (env/social/gouv)
‚îÇ   ‚îú‚îÄ‚îÄ Extraction actions concr√®tes
‚îÇ   ‚îî‚îÄ‚îÄ Mapping ODD
‚îú‚îÄ‚îÄ FAQExtractor
‚îÇ   ‚îú‚îÄ‚îÄ Parse Q/A hi√©rarchiques
‚îÇ   ‚îú‚îÄ‚îÄ Pr√©servation contexte parent
‚îÇ   ‚îî‚îÄ‚îÄ Extraction tags/keywords
‚îî‚îÄ‚îÄ JEExtractor
    ‚îú‚îÄ‚îÄ Extraction champs structur√©s
    ‚îú‚îÄ‚îÄ Normalisation (ville, √©cole)
    ‚îî‚îÄ‚îÄ Validation contacts

Smart Chunking
‚îú‚îÄ‚îÄ Chunking par type (logique m√©tier)
‚îÇ   ‚îú‚îÄ‚îÄ FAQ : 1 chunk = 1 Q/A
‚îÇ   ‚îú‚îÄ‚îÄ Legal : 1 chunk = 1 article/section
‚îÇ   ‚îú‚îÄ‚îÄ JE : 1 chunk = 1 fiche compl√®te
‚îÇ   ‚îî‚îÄ‚îÄ RSE : 1 chunk = 1 action/module
‚îú‚îÄ‚îÄ Respect taille (50-1000 mots, cible 300)
‚îî‚îÄ‚îÄ Pr√©servation coh√©rence s√©mantique

Enrichissement M√©tadonn√©es
‚îú‚îÄ‚îÄ Extraction keywords automatique (RAKE/YAKE)
‚îú‚îÄ‚îÄ Classification cat√©gorie (si absente)
‚îú‚îÄ‚îÄ Calcul priorit√© (heuristique)
‚îú‚îÄ‚îÄ Ajout timestamps (indexed_at)
‚îî‚îÄ‚îÄ Hash contenu (d√©tection modifications)
```

**Layer 4 : Vectorisation & Indexation**
```
TF-IDF Vectorizer (Scikit-Learn)
‚îú‚îÄ‚îÄ Configuration
‚îÇ   ‚îú‚îÄ‚îÄ max_features: 5000
‚îÇ   ‚îú‚îÄ‚îÄ ngram_range: (1, 3)
‚îÇ   ‚îú‚îÄ‚îÄ min_df: 2
‚îÇ   ‚îú‚îÄ‚îÄ max_df: 0.8
‚îÇ   ‚îú‚îÄ‚îÄ stop_words: custom JE
‚îÇ   ‚îú‚îÄ‚îÄ sublinear_tf: True
‚îÇ   ‚îî‚îÄ‚îÄ norm: 'l2'
‚îú‚îÄ‚îÄ Fit sur corpus complet (~8500 chunks)
‚îî‚îÄ‚îÄ Transform ‚Üí Matrice sparse (8500, 5000)

Truncated SVD (Scikit-Learn)
‚îú‚îÄ‚îÄ n_components: 300
‚îú‚îÄ‚îÄ algorithm: 'randomized'
‚îú‚îÄ‚îÄ n_iter: 7
‚îú‚îÄ‚îÄ Fit + Transform ‚Üí Matrice dense (8500, 300)
‚îî‚îÄ‚îÄ Variance expliqu√©e: ~85%

Multi-Index Construction
‚îú‚îÄ‚îÄ Index Vectoriel Principal
‚îÇ   ‚îî‚îÄ‚îÄ NumPy array (8500, 300) float32
‚îú‚îÄ‚îÄ Index by_type
‚îÇ   ‚îú‚îÄ‚îÄ 'faq' ‚Üí [0, 1, 15, 23, ...]
‚îÇ   ‚îú‚îÄ‚îÄ 'legal' ‚Üí [2, 5, 8, 11, ...]
‚îÇ   ‚îú‚îÄ‚îÄ 'je' ‚Üí [3, 7, 12, 19, ...]
‚îÇ   ‚îî‚îÄ‚îÄ 'rse' ‚Üí [4, 9, 14, 18, ...]
‚îú‚îÄ‚îÄ Index by_category
‚îÇ   ‚îú‚îÄ‚îÄ 'comptabilit√©' ‚Üí [...]
‚îÇ   ‚îú‚îÄ‚îÄ 'contrats' ‚Üí [...]
‚îÇ   ‚îú‚îÄ‚îÄ 'rh' ‚Üí [...]
‚îÇ   ‚îî‚îÄ‚îÄ ... (27 cat√©gories)
‚îú‚îÄ‚îÄ Index by_source
‚îÇ   ‚îî‚îÄ‚îÄ Par fichier JSON source
‚îî‚îÄ‚îÄ Index by_priority
    ‚îî‚îÄ‚îÄ Liste tri√©e par score priorit√©

Persistance Pickle
‚îú‚îÄ‚îÄ Fichier: kiwi_advanced_index.pkl
‚îú‚îÄ‚îÄ Taille: ~118 MB
‚îú‚îÄ‚îÄ Contenu complet:
‚îÇ   ‚îú‚îÄ‚îÄ vectorizer (fitted TfidfVectorizer)
‚îÇ   ‚îú‚îÄ‚îÄ svd_model (fitted TruncatedSVD)
‚îÇ   ‚îú‚îÄ‚îÄ vectors (NumPy array dense)
‚îÇ   ‚îú‚îÄ‚îÄ chunks (liste de dicts)
‚îÇ   ‚îú‚îÄ‚îÄ metadata_index (multi-index)
‚îÇ   ‚îú‚îÄ‚îÄ version (string)
‚îÇ   ‚îú‚îÄ‚îÄ build_date (ISO datetime)
‚îÇ   ‚îî‚îÄ‚îÄ statistics (dict)
‚îî‚îÄ‚îÄ Chargement: < 1 seconde
```

**Layer 5 : API Serving (FastAPI)**
```
main.py (Application Entry Point)
‚îú‚îÄ‚îÄ FastAPI app instance
‚îú‚îÄ‚îÄ CORS middleware (origins configur√©s)
‚îú‚îÄ‚îÄ Exception handlers
‚îú‚îÄ‚îÄ Startup event: load_index()
‚îî‚îÄ‚îÄ Shutdown event: cleanup()

Routers (Modularit√©)
‚îú‚îÄ‚îÄ /ask (main Q/A endpoint)
‚îú‚îÄ‚îÄ /search/* (recherche sp√©cialis√©e)
‚îú‚îÄ‚îÄ /admin/* (r√©indexation, stats)
‚îî‚îÄ‚îÄ /health (healthcheck)

Services Layer
‚îú‚îÄ‚îÄ vector_search.py
‚îÇ   ‚îú‚îÄ‚îÄ vectorize_query()
‚îÇ   ‚îú‚îÄ‚îÄ cosine_similarity_search()
‚îÇ   ‚îî‚îÄ‚îÄ filter_by_metadata()
‚îú‚îÄ‚îÄ llm_service.py
‚îÇ   ‚îú‚îÄ‚îÄ call_claude()
‚îÇ   ‚îú‚îÄ‚îÄ handle_rate_limits()
‚îÇ   ‚îî‚îÄ‚îÄ retry_logic()
‚îú‚îÄ‚îÄ type_detector.py
‚îÇ   ‚îú‚îÄ‚îÄ detect_query_type()
‚îÇ   ‚îî‚îÄ‚îÄ calculate_confidence()
‚îî‚îÄ‚îÄ boosting.py
    ‚îú‚îÄ‚îÄ apply_type_boost()
    ‚îú‚îÄ‚îÄ apply_category_boost()
    ‚îú‚îÄ‚îÄ apply_source_boost()
    ‚îî‚îÄ‚îÄ apply_recency_boost()

Models (Pydantic)
‚îú‚îÄ‚îÄ Request Models
‚îÇ   ‚îú‚îÄ‚îÄ QuestionRequest
‚îÇ   ‚îú‚îÄ‚îÄ AdvancedSearchRequest
‚îÇ   ‚îî‚îÄ‚îÄ ReindexRequest
‚îî‚îÄ‚îÄ Response Models
    ‚îú‚îÄ‚îÄ ComprehensiveAnswer
    ‚îú‚îÄ‚îÄ SearchResults
    ‚îî‚îÄ‚îÄ ReindexStatus

Configuration
‚îú‚îÄ‚îÄ Environment variables (.env)
‚îú‚îÄ‚îÄ Config class (Pydantic BaseSettings)
‚îî‚îÄ‚îÄ Secrets management
```

**Layer 6 : LLM Orchestration**
```
Query Processing Pipeline
‚îú‚îÄ‚îÄ 1. Type Detection
‚îÇ   ‚îú‚îÄ‚îÄ Keyword matching
‚îÇ   ‚îú‚îÄ‚îÄ TF-IDF scoring
‚îÇ   ‚îî‚îÄ‚îÄ Confidence threshold
‚îú‚îÄ‚îÄ 2. Vector Search
‚îÇ   ‚îú‚îÄ‚îÄ Query vectorization
‚îÇ   ‚îú‚îÄ‚îÄ Cosine similarity (all chunks)
‚îÇ   ‚îî‚îÄ‚îÄ Top 100 retrieval
‚îú‚îÄ‚îÄ 3. Contextual Boosting
‚îÇ   ‚îú‚îÄ‚îÄ Type match (√ó1.30)
‚îÇ   ‚îú‚îÄ‚îÄ Category match (√ó1.20)
‚îÇ   ‚îú‚îÄ‚îÄ Source authority (√ó1.15)
‚îÇ   ‚îî‚îÄ‚îÄ Recency (√ó1.10)
‚îú‚îÄ‚îÄ 4. Context Building
‚îÇ   ‚îú‚îÄ‚îÄ Aggregate top 10 chunks
‚îÇ   ‚îú‚îÄ‚îÄ Deduplicate similar (cosine > 0.85)
‚îÇ   ‚îú‚îÄ‚îÄ Format with metadata
‚îÇ   ‚îî‚îÄ‚îÄ Limit to ~8k tokens
‚îú‚îÄ‚îÄ 5. Prompt Engineering
‚îÇ   ‚îú‚îÄ‚îÄ Select template (by type)
‚îÇ   ‚îú‚îÄ‚îÄ Inject context
‚îÇ   ‚îú‚îÄ‚îÄ Add instructions
‚îÇ   ‚îî‚îÄ‚îÄ Final prompt assembly
‚îú‚îÄ‚îÄ 6. LLM Invocation
‚îÇ   ‚îú‚îÄ‚îÄ Anthropic API call
‚îÇ   ‚îú‚îÄ‚îÄ Model: claude-sonnet-4-5-20250929
‚îÇ   ‚îú‚îÄ‚îÄ Temperature: 0.3
‚îÇ   ‚îî‚îÄ‚îÄ Max tokens: 2000
‚îî‚îÄ‚îÄ 7. Response Formatting
    ‚îú‚îÄ‚îÄ Parse LLM output
    ‚îú‚îÄ‚îÄ Extract source references
    ‚îú‚îÄ‚îÄ Calculate confidence
    ‚îú‚îÄ‚îÄ Generate related questions
    ‚îî‚îÄ‚îÄ Structure JSON response

Prompt Templates
‚îú‚îÄ‚îÄ LEGAL_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Expert juridique JE
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: citer sources, alerter risques
‚îú‚îÄ‚îÄ RSE_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Consultant RSE
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: actions concr√®tes, ODD
‚îú‚îÄ‚îÄ FAQ_TEMPLATE
‚îÇ   ‚îú‚îÄ‚îÄ Role: Assistant p√©dagogique
‚îÇ   ‚îú‚îÄ‚îÄ Context injection point
‚îÇ   ‚îî‚îÄ‚îÄ Instructions: clart√©, exemples
‚îî‚îÄ‚îÄ GENERAL_TEMPLATE
    ‚îú‚îÄ‚îÄ Role: Assistant Comply
    ‚îú‚îÄ‚îÄ Context injection point
    ‚îî‚îÄ‚îÄ Instructions: pr√©cision, sources

Claude API Integration
‚îú‚îÄ‚îÄ AsyncAnthropic client
‚îú‚îÄ‚îÄ Message creation
‚îú‚îÄ‚îÄ Error handling
‚îÇ   ‚îú‚îÄ‚îÄ Rate limiting (exponential backoff)
‚îÇ   ‚îú‚îÄ‚îÄ Timeout (60s)
‚îÇ   ‚îî‚îÄ‚îÄ API errors (502 fallback)
‚îî‚îÄ‚îÄ Usage tracking
    ‚îú‚îÄ‚îÄ Input tokens
    ‚îú‚îÄ‚îÄ Output tokens
    ‚îî‚îÄ‚îÄ Cost calculation
```

---

## Choix Techniques et Justifications

### TF-IDF + SVD vs Embeddings Transformers

**Pourquoi TF-IDF + SVD ?**

**Avantages** :
1. **Performance brute** : Vectorisation query < 2ms, recherche < 10ms pour 8500 chunks
2. **Empreinte m√©moire r√©duite** : ~300 MB en RAM vs ~2-3 GB pour embeddings denses
3. **Pas de d√©pendance GPU** : Fonctionne parfaitement sur CPU standard
4. **Interpr√©tabilit√©** : On sait exactement quels termes matchent (vocabulaire explicite)
5. **Co√ªt computationnel minimal** : Entra√Ænement < 30s, pas de fine-tuning n√©cessaire
6. **D√©ploiement simple** : Pas de mod√®le lourd √† charger (BERT = 400+ MB)

**Inconv√©nients** :
1. **Pr√©cision s√©mantique limit√©e** : "v√©hicule" et "voiture" ne matchent pas (pas de synonymie)
2. **Sensibilit√© au vocabulaire exact** : Requ√™te doit contenir les bons mots-cl√©s
3. **Pas de compr√©hension contextuelle** : "banque" (finance) vs "banque" (si√®ge) non distingu√©s

**Pourquoi √ßa suffit pour Comply** :
- Corpus m√©tier avec vocabulaire stable et technique
- Utilisateurs JE connaissent la terminologie (pas de langage naturel casual)
- Performance critique (latence < 2s exig√©e)
- Infrastructure l√©g√®re (VPS entr√©e de gamme)
- Pr√©cision actuelle ~75% top-1, ~92% top-5 (suffisant avec LLM derri√®re)

**Migration future vers embeddings** :
Pr√©vue Q3 2025 avec sentence-transformers fran√ßais (Solon, CamemBERT) pour am√©liorer la pr√©cision de 15-20% tout en gardant TF-IDF en fallback.

### Pickle vs Base de Donn√©es Vectorielle

**Pourquoi Pickle ?**

**Avantages** :
1. **Simplicit√© extr√™me** : Un seul fichier, aucune infrastructure externe
2. **Chargement ultra-rapide** : < 1s pour 118 MB, tout en RAM
3. **Pas de r√©seau** : Pas de latence r√©seau, pas de connexion √† g√©rer
4. **Atomic swap** : R√©indexation = swap de fichier (zero downtime)
5. **Backup trivial** : Simple `cp` du fichier
6. **Pas de d√©pendance** : Pas de service externe √† maintenir/monitorer

**Inconv√©nients** :
1. **Pas de recherche distribu√©e** : Scaling horizontal impossible
2. **Update non incr√©mentale** : Modification = r√©indexation compl√®te
3. **Pas de queries complexes** : Pas de filtres SQL-like sophistiqu√©s
4. **Limite de taille** : Probl√©matique au-del√† de 100k chunks (~1.5 GB RAM)

**Pourquoi √ßa suffit pour Comply v1** :
- Corpus stable ~8500 chunks (croissance lente, +10-15% par an)
- Usage mono-serveur (pas de distribution n√©cessaire)
- R√©indexation rare (1-2 fois par mois max)
- Latence critique (base distante = +10-50ms minimum)

**Migration future** :
- **Phase 1 (Q2 2025)** : FAISS local (m√™me serveur, API compatible)
- **Phase 2 (Q4 2025)** : Milvus/Qdrant si scaling n√©cessaire (multi-JE mutualis√©)

### FastAPI vs Flask/Django

**Pourquoi FastAPI ?**

**Avantages techniques** :
1. **Performance async native** : 3-4x plus rapide que Flask sur requ√™tes I/O-bound
2. **Type safety** : Pydantic validation automatique, pas de bugs runtime sur types
3. **Documentation auto** : OpenAPI/Swagger UI sans code additionnel
4. **Standards modernes** : ASGI, async/await natif, Python 3.9+ type hints
5. **√âcosyst√®me mature** : Starlette (robuste), Uvicorn (performant)

**Comparaison benchmarks** (requ√™tes/seconde sur VPS 4 cores) :
- FastAPI (async) : ~1200 req/s
- Flask (sync) : ~400 req/s
- Django (sync) : ~300 req/s
- Django (async) : ~800 req/s

**Pour Comply** :
- Appels LLM = I/O-bound (attente r√©seau 1-3s)
- Async permet de g√©rer 10-20 requ√™tes simultan√©es sans bloquer
- Validation Pydantic critique (s√©curit√©, fiabilit√©)
- Doc OpenAPI = indispensable pour int√©grations tierces

### Claude vs GPT-4 vs Mistral

**Pourquoi Claude Sonnet 4.5 ?**

**Comparaison qualitative** :

| Crit√®re | Claude Sonnet 4.5 | GPT-4 Turbo | Mistral Large |
|---------|-------------------|-------------|---------------|
| **Adh√©rence instructions** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Context window** | 200k tokens | 128k tokens | 32k tokens |
| **Hallucinations** | Tr√®s peu | Mod√©r√©es | Fr√©quentes |
| **Citations sources** | Excellent | Bon | Moyen |
| **Raisonnement complexe** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Latence** | 1-3s | 2-4s | 0.5-1.5s |
| **Co√ªt (1M tokens)** | $3 input/$15 output | $10 input/$30 output | $2 input/$6 output |

**Pour Comply** :
- **Adh√©rence critique** : Doit respecter strictement les instructions (citer sources, pas inventer)
- **Context window** : Permet d'injecter 10-15 chunks riches sans truncation
- **Hallucinations** : Inacceptable en contexte juridique
- **Co√ªt ma√Ætris√©** : ~$0.012/requ√™te vs $0.025 avec GPT-4

**Tests internes** :
- Claude : 87% de r√©ponses jug√©es "excellentes" (citations correctes, pas d'hallucination)
- GPT-4 : 82% (invente parfois des articles de loi)
- Mistral : 71% (manque de pr√©cision, citations approximatives)

### Python vs Node.js vs Go

**Pourquoi Python ?**

**Avantages pour Comply** :
1. **√âcosyst√®me ML/NLP** : Scikit-learn, NumPy, Pandas = stack standard
2. **Productivit√© d√©veloppement** : Syntaxe claire, prototypage rapide
3. **Biblioth√®ques scraping** : Selenium, BeautifulSoup = r√©f√©rences
4. **Type hints (3.9+)** : Robustesse comparable aux langages typ√©s
5. **Communaut√© data science** : Ressources, tutoriels, support

**Inconv√©nients** :
1. **Performance brute** : Plus lent que Go/Rust (mais non critique ici)
2. **GIL** : Threading limit√© (compens√© par async/await)

**Pourquoi pas Node.js** :
- √âcosyst√®me ML immature (TensorFlow.js limit√©)
- Pas de NumPy/Scikit-learn √©quivalents
- Moins adapt√© au traitement de donn√©es scientifiques

**Pourquoi pas Go** :
- Pas d'√©cosyst√®me ML/NLP mature
- D√©veloppement plus verbeux
- Moins de d√©veloppeurs data dans l'√©quipe

**Verdict** :
Python = choix √©vident pour un projet ML/NLP avec scraping. Performance suffisante avec FastAPI async. Possibilit√© de r√©√©crire les parties critiques en Rust/Cython si n√©cessaire (non pr√©vu).

---

## M√©triques et Performance

### Benchmarks Actuels (Production)

**Latence end-to-end** :
```
P50 (m√©diane)  : 1.8s
P95            : 3.2s
P99            : 4.5s
Max observ√©    : 6.2s
```

**D√©composition latence (P50)** :
```
Type detection  :   15ms  (0.8%)
Vector search   :   11ms  (0.6%)
Boosting        :    3ms  (0.2%)
Context build   :    5ms  (0.3%)
Prompt gen      :    2ms  (0.1%)
Claude API call : 1720ms (95.6%)
Response format :    8ms  (0.4%)
Logging         :    6ms  (0.3%)
Network         :   30ms  (1.7%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total           : 1800ms (100%)
```

**Observation** : Le LLM repr√©sente 95%+ de la latence. Optimisations internes < 100ms d'impact total.

**Throughput** :
- Serveur mono-instance (4 workers Uvicorn) : ~15 requ√™tes/s
- Limit√© par appels LLM s√©quentiels (pas de batch)
- Avec cache Redis (20% hit rate) : ~18 requ√™tes/s

**Pr√©cision** :
```
Top-1 accuracy : 75.3%  (chunk pertinent en 1√®re position)
Top-5 recall   : 92.1%  (chunk pertinent dans top 5)
Top-10 recall  : 96.8%  (chunk pertinent dans top 10)
```

M√©thodologie : √âvaluation manuelle sur 200 requ√™tes test par 3 experts JE.

**Satisfaction utilisateur** :
```
Feedback Slack (üëç/üëé) :
  Positif (üëç) : 85.3%
  N√©gatif (üëé) : 8.7%
  Neutre      : 6.0%
```

### Consommation Ressources

**VPS Production (Contabo VPS S)** :
```
CPU (moyenne)      : 8-12%  (pics √† 35% lors de r√©indexation)
RAM utilis√©e       : 2.1 GB / 8 GB (26%)
  - Index en m√©moire : 312 MB
  - Python runtime   : 180 MB
  - FastAPI          : 95 MB
  - Uvicorn workers  : 420 MB (4√ó105 MB)
  - OS + services    : 1.1 GB

Disque utilis√©    : 8.7 GB / 200 GB
  - Application      : 450 MB
  - Index + data     : 580 MB
  - Logs (30 jours)  : 1.2 GB
  - Docker images    : 2.8 GB
  - OS               : 3.7 GB

Bande passante (mois) : ~42 GB
  - API calls in      : 18 GB
  - API calls out     : 21 GB
  - Claude API        : 3 GB
```

**Co√ªt LLM** :
```
Usage mensuel moyen :
  - Requ√™tes/jour  : 147
  - Requ√™tes/mois  : ~4400

Tokens consomm√©s :
  - Input  : 6.2M tokens/mois (~1400 tokens/req)
  - Output : 2.1M tokens/mois (~480 tokens/req)

Co√ªt Claude :
  - Input  : 6.2M √ó $3/1M  = $18.60
  - Output : 2.1M √ó $15/1M = $31.50
  - Total                  = $50.10/mois
```

---

## S√©curit√© et Conformit√©

### Mesures de S√©curit√© Impl√©ment√©es

**Infrastructure** :
- ‚úÖ Pare-feu UFW (ports 22, 80, 443 uniquement)
- ‚úÖ SSH par cl√© uniquement (password auth disabled)
- ‚úÖ Fail2ban (bannissement auto apr√®s 5 tentatives)
- ‚úÖ Nginx reverse proxy avec rate limiting
- ‚úÖ HTTPS obligatoire (Let's Encrypt)
- ‚úÖ Headers de s√©curit√© (HSTS, CSP, X-Frame-Options)

**Application** :
- ‚úÖ Validation Pydantic de tous les inputs
- ‚úÖ Pas d'ex√©cution de code user (pas d'eval, pas d'exec)
- ‚úÖ Sanitization des queries (injection prevention)
- ‚úÖ Secrets en variables d'environnement (.env gitignored)
- ‚úÖ API key rotation tous les 90 jours

**Donn√©es** :
- ‚úÖ Logs anonymis√©s (IP hash√©s)
- ‚úÖ Pas de stockage de donn√©es sensibles utilisateur
- ‚úÖ Chiffrement TLS in-transit
- ‚ö†Ô∏è **TODO** : Chiffrement at-rest (disques)

### Conformit√© RGPD

**Donn√©es personnelles collect√©es** :
- User ID Slack (pseudonyme)
- Timestamps des requ√™tes
- IP address (anonymis√©e apr√®s 24h)

**Droits utilisateurs** :
- ‚úÖ Droit d'acc√®s : Export JSON de toutes les requ√™tes via `/api/gdpr/export`
- ‚úÖ Droit de rectification : Modification du user_id via API admin
- ‚úÖ Droit √† l'oubli : Anonymisation compl√®te via `/api/gdpr/anonymize`
- ‚úÖ Droit √† la portabilit√© : Format JSON standardis√©

**Base l√©gale** :
- Int√©r√™t l√©gitime (am√©lioration du service, statistiques)
- Dur√©e de conservation : 12 mois (logs), 6 mois (audit)

**DPO** : Contact SEPEFREI pour toute question RGPD.

---

## Limitations et Consid√©rations

### Limitations Techniques Actuelles

**1. Pas de m√©moire conversationnelle**
- Chaque question trait√©e ind√©pendamment
- Pas de contexte multi-turn ("Et pour une SASU ?" apr√®s "Comment cr√©er une JE ?")
- **Impact** : Utilisateur doit reformuler compl√®tement chaque question
- **Workaround** : Stocker historique c√¥t√© client (Slack thread)

**2. Recherche non distribu√©e**
- Index entier sur un seul serveur
- Pas de sharding possible
- **Impact** : Scaling limit√© √† ~100k chunks max
- **Mitigation** : Suffisant pour 5-10 ans de croissance

**3. Pas de cache intelligent**
- Questions identiques recalcul√©es
- Pas de cache s√©mantique (questions similaires)
- **Impact** : Latence et co√ªt LLM non optimis√©s
- **Mitigation** : Redis cache basique en Q1 2025

**4. Scraping manuel trigger**
- N√©cessite intervention humaine pour update
- Pas de d√©tection auto des changements sources
- **Impact** : Index peut √™tre obsol√®te
- **Mitigation** : Cron automation en Q1 2025

### Limitations Fonctionnelles

**1. Texte uniquement**
- Pas de traitement d'images, PDFs scann√©s, tableaux complexes
- **Impact** : Certains documents non indexables
- **Workaround** : OCR manuel + ajout au corpus

**2. Pas de g√©n√©ration de documents**
- Comply r√©pond mais ne cr√©e pas de contrats/rapports
- **Impact** : Utilisateur doit r√©diger lui-m√™me
- **√âvolution** : Templates + g√©n√©ration en Q3 2025

**3. D√©pendance totale Claude**
- Si API Anthropic down ‚Üí service inop√©rant
- Pas de fallback provider
- **Impact** : Single point of failure
- **Mitigation** : Multi-LLM support en Q2 2025

### Limitations Organisationnelles

**1. Pas de gestion de versions du corpus****Bot Discord** :
```python
import discord
from discord.ext import commands

class ComplyBot(commands.Bot):
    def __init__(self):
        intents = discord.Intents.default()
        intents.message_content = True
        super().__init__(command_prefix='!comply ', intents=intents)
    
    @commands.command()
    async def ask(self, ctx, *, question):
        """!comply ask Comment d√©clarer la TVA ?"""
        async with ctx.typing():
            response = await call_comply_api(question)
            
            embed = discord.Embed(
                title="üí° Comply",
                description=response['answer'],
                color=discord.Color.blue()
            )
            
            # Ajout des sources
            sources_text = "\n".join([
                f"‚Ä¢ [{s['type']}] {s['source_file']}"
                for s in response['sources'][:3]
            ])
            embed.add_field(name="üìö Sources", value=sources_text, inline=False)
            
            await ctx.send(embed=embed)
```

**Mobile App Native** :
- React Native / Flutter
- Interface conversationnelle
- Mode offline (cache local des FAQ communes)
- Notifications push pour alertes audit/conformit√©

**API Webhooks** :
```python
@router.post("/webhooks/notion")
async def notion_webhook(request: NotionWebhookRequest):
    """Int√©gration Notion : analyse automatique des docs"""
    page_content = request.page_content
    
    # Analyse de conformit√©
    compliance_check = await check_compliance(page_content)
    
    # Update Notion page avec r√©sultats
    update_notion_page(
        page_id=request.page_id,
        compliance_status=compliance_check
    )
```

#### 4. Gouvernance et Audit Trail

**Tra√ßabilit√© compl√®te** :
```python
class AuditLogger:
    def log_query(self, user_id, query, response, context):
        """Log complet de chaque interaction"""
        audit_entry = {
            'id': generate_uuid(),
            'timestamp': datetime.now().isoformat(),
            'user': {
                'id': user_id,
                'role': context.get('role'),
                'je': context.get('je_name')
            },
            'query': {
                'text': query,
                'type': response['detected_type'],
                'hash': hashlib.sha256(query.encode()).hexdigest()
            },
            'response': {
                'answer_preview': response['answer'][:200],
                'confidence': response['confidence'],
                'sources_used': [s['chunk_id'] for s in response['sources']],
                'llm_model': response['metadata']['llm_model'],
                'tokens': {
                    'input': response['metadata']['input_tokens'],
                    'output': response['metadata']['output_tokens']
                }
            },
            'metadata': {
                'ip_address': anonymize_ip(context.get('ip')),
                'user_agent': context.get('user_agent'),| Composant | Minimum | Recommand√© | Production |
|-----------|---------|------------|------------|
| **CPU** | 2 vCores | 4 vCores | 6 vCores |
| **RAM** | 4 GB | 8 GB | 16 GB |
| **Stockage** | 20 GB SSD | 40 GB SSD | 80 GB SSD |
| **Bande passante** | 100 Mbps | 200 Mbps | 1 Gbps |
| **OS** | Debian 11 | Debian 12 | Debian 12 |

**Fournisseurs VPS Recommand√©s (France)** :

**1. Contabo - VPS S SSD** (Recommandation principale)
- **Prix** : ~5,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 200 GB SSD NVMe
- **Localisation** : N√ºrnberg (Allemagne) ou Paris (France)
- **Avantages** : Excellent rapport qualit√©/prix, ressources g√©n√©reuses
- **Lien** : [https://contabo.com/en/vps/](https://contabo.com/en/vps/)

**2. Hetzner - CX31**
- **Prix** : ~9,50‚Ç¨/mois
- **Config** : 2 vCores, 8 GB RAM, 80 GB SSD
- **Localisation** : Falkenstein ou Helsinki
- **Avantages** : Infrastructure fiable, excellente connectivit√©
- **Lien** : [https://www.hetzner.com/cloud](https://www.hetzner.com/cloud)

**3. OVH - VPS Comfort**
- **Prix** : ~11,99‚Ç¨/mois
- **Config** : 4 vCores, 8 GB RAM, 160 GB SSD
- **Localisation** : Gravelines, Roubaix, Strasbourg (France)
- **Avantages** : Fran√ßais, support fran√ßais, infrastructure r√©siliente
- **Lien** : [https://www.ovhcloud.com/fr/vps/](https://www.ovhcloud.com/fr/vps/)

**4. Scaleway - DEV1-M**
- **Prix** : ~7,99‚Ç¨/mois
- **Config** : 3 vCores, 4 GB RAM, 40 GB SSD
- **Localisation** : Paris, Amsterdam
- **Avantages** : √âcosyst√®me cloud complet, IPv6 natif
- **Lien** : [https://www.scaleway.com/en/pricing/](https://www.scaleway.com/en/pricing/)

**Notre choix pour Junior-Entreprises** : **Contabo VPS S SSD**
- Meilleur compromis co√ªt/performance pour usage Comply
- Ressources largement suffisantes (8 GB RAM = confortable pour l'index)
- Co√ªt mensuel accessible pour budget JE (~72‚Ç¨/an)

### Architecture R√©seau et S√©curit√©

**Configuration pare-feu (UFW)** :
```bash
# Installation UFW
apt install ufw -y

# Configuration par d√©faut
ufw default deny incoming
ufw default allow outgoing

# Autorisation SSH (changez 22 si port custom)
ufw allow 22/tcp

# Autorisation HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# Activation
ufw enable

# V√©rification
ufw status verbose
```

**Configuration SSH s√©curis√©e** (`/etc/ssh/sshd_config`) :
```bash
# D√©sactivation login root
PermitRootLogin no

# Authentification par cl√© uniquement
PasswordAuthentication no
PubkeyAuthentication yes

# D√©sactivation X11 forwarding
X11Forwarding no

# Port custom (optionnel, s√©curit√© par obscurit√©)
Port 2222

# Red√©marrage SSH
systemctl restart sshd
```

**Reverse Proxy Nginx** :
```nginx
# /etc/nginx/sites-available/comply

upstream comply_backend {
    server 127.0.0.1:8000;
    keepalive 32;
}

server {
    listen 80;
    server_name comply.votre-je.fr;
    
    # Redirection HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name comply.votre-je.fr;
    
    # Certificats Let's Encrypt
    ssl_certificate /etc/letsencrypt/live/comply.votre-je.fr/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/comply.votre-je.fr/privkey.pem;
    
    # Configuration SSL moderne
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    
    # Headers de s√©curit√©
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    
    # Logs
    access_log /var/log/nginx/comply_access.log;
    error_log /var/log/nginx/comply_error.log;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=comply_limit:10m rate=10r/s;
    limit_req zone=comply_limit burst=20 nodelay;
    
    # Proxy vers FastAPI
    location / {
        proxy_pass http://comply_backend;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Health check endpoint (pas de rate limit)
    location /health {
        limit_req off;
        proxy_pass http://comply_backend;
    }
}
```

**Certificat SSL Let's Encrypt (gratuit)** :
```bash
# Installation Certbot
apt install certbot python3-certbot-nginx -y

# G√©n√©ration certificat
certbot --nginx -d comply.votre-je.fr

# Renouvellement automatique (cron)
echo "0 3 * * * certbot renew --quiet" | crontab -
```

---

## Pr√©requis Serveur

### Installation de l'Environnement

**Script d'installation compl√®te** :
```bash
#!/bin/bash
# install_comply_environment.sh

set -e

echo "=== COMPLY - Installation de l'environnement ==="

# Mise √† jour syst√®me
echo "[1/8] Mise √† jour du syst√®me..."
apt update && apt upgrade -y

# Installation Python 3.11
echo "[2/8] Installation Python 3.11..."
apt install -y software-properties-common
add-apt-repository ppa:deadsnakes/ppa -y
apt update
apt install -y python3.11 python3.11-venv python3.11-dev python3-pip

# V√©rification Python
python3.11 --version

# Installation Git
echo "[3/8] Installation Git..."
apt install -y git

# Installation Docker (optionnel)
echo "[4/8] Installation Docker..."
apt install -y docker.io docker-compose
systemctl enable docker
systemctl start docker

# Installation d√©pendances syst√®me pour Selenium
echo "[5/8] Installation d√©pendances Selenium..."
apt install -y chromium-browser chromium-chromedriver
apt install -y xvfb  # X Virtual Framebuffer pour headless

# Installation Nginx
echo "[6/8] Installation Nginx..."
apt install -y nginx
systemctl enable nginx

# Installation Certbot
echo "[7/8] Installation Certbot..."
apt install -y certbot python3-certbot-nginx

# Cr√©ation utilisateur d√©di√©
echo "[8/8] Cr√©ation utilisateur comply..."
useradd -m -s /bin/bash comply
usermod -aG sudo comply

echo "=== Installation termin√©e ==="
echo "Prochaine √©tape: Cloner le repository et installer les d√©pendances Python"
```

**Ex√©cution** :
```bash
chmod +x install_comply_environment.sh
sudo ./install_comply_environment.sh
```

### Configuration de l'Application

**Clonage du repository** :
```bash
# Connexion en tant qu'utilisateur comply
su - comply

# Clonage
git clone https://github.com/sepefrei/comply.git
cd comply

# Cr√©ation environnement virtuel
python3.11 -m venv venv
source venv/bin/activate

# Installation d√©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

**Fichier requirements.txt** :
```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Machine Learning & NLP
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3

# LLM
anthropic==0.7.8

# Scraping
selenium==4.15.2
beautifulsoup4==4.12.2
lxml==4.9.3

# Utils
python-dotenv==1.0.0
tenacity==8.2.3
pyyaml==6.0.1

# Logging & Monitoring
loguru==0.7.2

# Testing (dev)
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

**Configuration .env** :
```bash
# Copie du template
cp .env.example .env

# √âdition
nano .env
```

Contenu `.env` :
```bash
# Environment
ENVIRONMENT=production

# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Application
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=4

# Index Configuration
INDEX_FILE_PATH=/home/comply/comply/data/index/kiwi_advanced_index.pkl
MAX_CHUNKS_CONTEXT=10
DEFAULT_TOP_K=10

# LLM Configuration
LLM_MODEL=claude-sonnet-4-5-20250929
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.3

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/comply/app.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Security
ALLOWED_ORIGINS=https://comply.votre-je.fr,https://votre-je.slack.com
API_KEY_ENABLED=false
# API_KEY=your-secret-api-key

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
```

### Service systemd

**Cr√©ation du service** (`/etc/systemd/system/comply.service`) :
```ini
[Unit]
Description=Comply - AI Assistant for Junior-Entreprises
After=network.target

[Service]
Type=simple
User=comply
Group=comply
WorkingDirectory=/home/comply/comply
Environment="PATH=/home/comply/comply/venv/bin"

ExecStart=/home/comply/comply/venv/bin/uvicorn main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --log-level info \
    --access-log \
    --use-colors

Restart=always
RestartSec=5

# Limites ressources
LimitNOFILE=65536
LimitNPROC=4096
MemoryLimit=12G
CPUQuota=400%

# S√©curit√©
PrivateTmp=true
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/comply/comply/data /var/log/comply

[Install]
WantedBy=multi-user.target
```

**Activation et d√©marrage** :
```bash
# Rechargement systemd
sudo systemctl daemon-reload

# Activation au d√©marrage
sudo systemctl enable comply

# D√©marrage du service
sudo systemctl start comply

# V√©rification du statut
sudo systemctl status comply

# Logs en temps r√©el
sudo journalctl -u comply -f
```

### Logging Avanc√©

**Configuration Loguru** :
```python
from loguru import logger
import sys

# Configuration des logs
logger.remove()  # Supprime le handler par d√©faut

# Console (stdout)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
    level="INFO",
    colorize=True
)

# Fichier de logs rotatifs
logger.add(
    "/var/log/comply/app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}",
    level="INFO",
    rotation="10 MB",
    retention="30 days",
    compression="zip"
)

# Fichier d'erreurs s√©par√©
logger.add(
    "/var/log/comply/errors.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function} - {message}\n{exception}",
    level="ERROR",
    rotation="5 MB",
    retention="60 days",
    backtrace=True,
    diagnose=True
)
```

**Utilisation dans le code** :
```python
# Logs structur√©s
logger.info("Index loaded", version=INDEX['version'], chunks=INDEX['statistics']['n_chunks'])

# Logs de requ√™tes
logger.info("Processing question", 
    question=request.question[:50],
    detected_type=detected_type,
    user_id=user_context.get('id')
)

# Logs d'erreurs avec contexte
try:
    result = vector_search(query)
except Exception as e:
    logger.error("Vector search failed", 
        query=query,
        error=str(e),
        exc_info=True
    )
    raise
```

---

## Roadmap Technique

### Court Terme (Q1-Q2 2025)

#### 1. Automatisation Compl√®te du Scraping

**Objectif** : Supprimer l'intervention humaine du processus de mise √† jour des donn√©es.

**Impl√©mentation** :
```python
# cron_scraper.py
import schedule
import time
from scrapers.kiwi_scraper import KiwiScraper
from utils.diff_detector import DiffDetector

def scheduled_scrape_job():
    """Job de scraping diff√©rentiel automatique"""
    logger.info("Starting scheduled scrape job")
    
    scraper = KiwiScraper()
    diff_detector = DiffDetector()
    
    # Scraping des 3 sources
    sources = ['legal', 'rse', 'faq']
    changes_detected = False
    
    for source in sources:
        logger.info(f"Scraping {source}...")
        new_data = scraper.scrape(source)
        
        # D√©tection de changements (hash comparison)
        has_changes = diff_detector.compare(
            source,
            new_data,
            f'data/raw/{source}_latest.json'
        )
        
        if has_changes:
            logger.info(f"Changes detected in {source}")
            changes_detected = True
            
            # Sauvegarde nouvelle version
            save_json(new_data, f'data/raw/{source}_{date.today()}.json')
            save_json(new_data, f'data/raw/{source}_latest.json')
    
    # Si changements d√©tect√©s ‚Üí r√©indexation automatique
    if changes_detected:
        logger.info("Triggering automatic reindexation")
        trigger_reindex()
        
        # Notification Slack
        send_slack_notification(
            "üîÑ Comply index updated",
            f"New data scraped and indexed. {len(sources)} sources updated."
        )

# Planification : tous les jours √† 3h du matin
schedule.every().day.at("03:00").do(scheduled_scrape_job)

if __name__ == "__main__":
    logger.info("Cron scraper started")
    while True:
        schedule.run_pending()
        time.sleep(60)
```

**Configuration cron syst√®me** :
```bash
# Ajout au crontab de l'utilisateur comply
crontab -e

# Ajout de la ligne
0 3 * * * /home/comply/comply/venv/bin/python /home/comply/comply/cron_scraper.py >> /var/log/comply/cron.log 2>&1
```

**D√©tection diff√©rentielle** :
```python
class DiffDetector:
    def compare(self, source_name, new_data, old_file_path):
        """Compare new data with previous version"""
        if not os.path.exists(old_file_path):
            return True  # Premier scraping
        
        with open(old_file_path, 'r') as f:
            old_data = json.load(f)
        
        # Calcul hash du contenu
        new_hash = self._compute_hash(new_data)
        old_hash = self._compute_hash(old_data)
        
        if new_hash != old_hash:
            # Analyse d√©taill√©e des diff√©rences
            diff_stats = self._compute_diff_stats(old_data, new_data)
            logger.info(f"Diff stats for {source_name}", **diff_stats)
            return True
        
        return False
    
    def _compute_hash(self, data):
        """Compute SHA256 hash of data"""
        import hashlib
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compute_diff_stats(self, old, new):
        """Compute detailed diff statistics"""
        # Logique sp√©cifique selon la structure
        return {
            'added': 0,
            'modified': 0,
            'deleted': 0
        }
```

**R√©indexation incr√©mentale** :
```python
def incremental_reindex(changed_sources):
    """Reindex only modified sources"""
    logger.info(f"Starting incremental reindex for: {changed_sources}")
    
    # Chargement de l'index actuel
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        current_index = pickle.load(f)
    
    # Suppression des chunks des sources modifi√©es
    chunks_to_keep = [
        chunk for chunk in current_index['chunks']
        if chunk['metadata']['source_file'] not in changed_sources
    ]
    
    # Ajout des nouveaux chunks
    for source in changed_sources:
        new_chunks = process_source(source)
        chunks_to_keep.extend(new_chunks)
    
    # R√©indexation compl√®te (vectorisation)
    builder = IndexBuilder()
    new_index = builder.build_index(chunks_to_keep)
    
    # Swap atomique
    backup_index(current_index)
    builder.save_index(new_index)
    
    logger.info("Incremental reindex completed")
```

#### 2. Monitoring et Observabilit√©

**Prometheus metrics** :
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# M√©triques
queries_total = Counter('comply_queries_total', 'Total number of queries', ['type'])
query_duration = Histogram('comply_query_duration_seconds', 'Query duration')
llm_calls_total = Counter('comply_llm_calls_total', 'Total LLM API calls')
llm_tokens_used = Counter('comply_llm_tokens_used', 'LLM tokens consumed', ['type'])
index_size = Gauge('comply_index_size_chunks', 'Number of chunks in index')

# Dans le code
@query_duration.time()
async def ask_question(request):
    queries_total.labels(type=detected_type).inc()
    # ... traitement
    llm_calls_total.inc()
    llm_tokens_used.labels(type='input').inc(usage['input_tokens'])
    llm_tokens_used.labels(type='output').inc(usage['output_tokens'])
```

**Dashboard Grafana** :
- Graphique : Requ√™tes/heure par type
- Graphique : Latence p50, p95, p99
- Graphique : Co√ªt LLM journalier (tokens √ó prix)
- Gauge : Taille de l'index
- Alerte : Latence > 5s
- Alerte : Taux d'erreur > 5%

#### 3. Cache Redis pour Performance

**Impl√©mentation** :
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cached_query(query, ttl=3600):
    """Cache les r√©ponses fr√©quentes"""
    # G√©n√©ration cl√© cache
    query_hash = hashlib.md5(query.encode()).hexdigest()
    cache_key = f"comply:query:{query_hash}"
    
    # Tentative de r√©cup√©ration du cache
    cached = redis_client.get(cache_key)
    if cached:
        logger.info("Cache hit", query=query[:50])
        return json.loads(cached)
    
    # Sinon, traitement normal
    result = process_query(query)
    
    # Mise en cache
    redis_client.setex(
        cache_key,
        ttl,
        json.dumps(result)
    )
    
    return result
```

**Strat√©gie de cache** :
- TTL court (1h) pour questions volatiles
- TTL long (24h) pour FAQ stables
- Invalidation sur r√©indexation
- Cache warming des top 100 questions

### Moyen Terme (Q3-Q4 2025)

#### 1. Migration vers Embeddings Denses

**Objectif** : Am√©liorer la pr√©cision s√©mantique avec des embeddings transformers.

**Impl√©mentation** :
```python
from sentence_transformers import SentenceTransformer

class DenseEmbeddingIndexer:
    def __init__(self):
        # Mod√®le fran√ßais optimis√©
        self.model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')
    
    def encode_chunks(self, chunks):
        texts = [chunk['text'] for chunk in chunks]
        
        # Encoding en batch
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        return embeddings  # Shape: (n_chunks, 1024)
```

**Migration FAISS** :
```python
import faiss

class FAISSIndex:
    def __init__(self, dimension=1024):
        # Index IVF avec quantization
        quantizer = faiss.IndexFlatIP(dimension)  # Inner Product
        self.index = faiss.IndexIVFPQ(
            quantizer,
            dimension,
            nlist=100,  # Nombre de clusters
            m=8,  # Sous-quantizers
            8  # Bits par sous-quantizer
        )
    
    def build(self, embeddings):
        # Entra√Ænement de l'index
        self.index.train(embeddings)
        self.index.add(embeddings)
        
        # Nombre de clusters √† visiter lors de la recherche
        self.index.nprobe = 10
    
    def search(self, query_embedding, k=10):
        distances, indices = self.index.search(query_embedding, k)
        return indices[0], distances[0]
```

**Performance attendue** :
- Pr√©cision : +15-20% (top-5 recall)
- Latence : ~20-30ms (vs 11ms TF-IDF)
- M√©moire : ~800 MB (vs 300 MB)

#### 2. Fine-Tuning Embeddings

**Dataset custom JE** :
```python
# G√©n√©ration de paires positives/n√©gatives
training_data = [
    {
        'query': "Comment d√©clarer la TVA ?",
        'positive': "Les Junior-Entreprises b√©n√©ficient du r√©gime de franchise...",
        'negative': "Pour organiser un √©v√©nement RSE..."
    },
    # ... 10k+ exemples
]

# Fine-tuning avec Sentence Transformers
from sentence_transformers import losses, InputExample

train_examples = [
    InputExample(texts=[item['query'], item['positive']])
    for item in training_data
]

model.fit(
    train_objectives=[(train_dataloader, losses.MultipleNegativesRankingLoss(model))],
    epochs=3,
    warmup_steps=100
)
```

#### 3. Multi-LLM Support

**Abstraction provider** :
```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> dict:
        pass

class ClaudeProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Claude
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation GPT-4
        pass

class MistralProvider(LLMProvider):
    async def generate(self, prompt, **kwargs):
        # Impl√©mentation Mistral
        pass

# Factory
def get_llm_provider(provider_name: str) -> LLMProvider:
    providers = {
        'claude': ClaudeProvider(),
        'openai': OpenAIProvider(),
        'mistral': MistralProvider()
    }
    return providers[provider_name]
```

**Routing intelligent** :
```python
def route_query_to_llm(query_type, complexity):
    """S√©lection du LLM optimal selon le contexte"""
    if query_type == 'juridique' and complexity == 'high':
        return 'claude'  # Meilleur sur le raisonnement complexe
    elif query_type == 'faq' and complexity == 'low':
        return 'mistral'  # Rapide et √©conomique
    else:
        return 'claude'  # Default
```

#### 4. Feedback Loop et Active Learning

**Collecte de feedback** :
```python
class FeedbackCollector:
    def record_feedback(self, query_id, feedback_type, user_comment=None):
        """Enregistre le feedback utilisateur"""
        feedback_data = {
            'query_id': query_id,
            'timestamp': datetime.now().isoformat(),
            'feedback_type': feedback_type,  # 'positive', 'negative', 'neutral'
            'user_comment': user_comment
        }
        
        # Stockage
        save_to_database(feedback_data)
        
        # Si feedback n√©gatif ‚Üí investigation
        if feedback_type == 'negative':
            self.analyze_failure(query_id)
```

**R√©entra√Ænement p√©riodique** :
```python
def monthly_retraining():
    """R√©entra√Ænement mensuel avec les feedbacks"""
    # R√©cup√©ration des feedbacks
    feedbacks = load_feedbacks(last_30_days=True)
    
    # G√©n√©ration de nouveaux exemples d'entra√Ænement
    new_training_data = []
    for feedback in feedbacks:
        if feedback['type'] == 'negative':
            # Analyse de la requ√™te √©chou√©e
            query = get_query(feedback['query_id'])
            correct_chunks = identify_correct_chunks(query, feedback['comment'])
            
            new_training_data.append({
                'query': query,
                'positive': correct_chunks,
                'negative': query['retrieved_chunks']
            })
    
    # Fine-tuning incr√©mental
    if len(new_training_data) > 100:
        fine_tune_model(new_training_data)
        logger.info(f"Model fine-tuned with {len(new_training_data)} examples")
```

### Long Terme (2026+)

#### 1. Multimodalit√©

**Support documents PDF/Images** :
```python
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

class MultimodalProcessor:
    def process_pdf(self, pdf_path):
        """Extraction texte + images d'un PDF"""
        # Conversion PDF ‚Üí images
        images = convert_from_path(pdf_path)
        
        extracted_text = ""
        for image in images:
            # OCR
            text = pytesseract.image_to_string(image, lang='fra')
            extracted_text += text + "\n\n"
        
        return extracted_text
    
    def process_image(self, image_path):
        """Extraction texte d'une image"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='fra')
        return text
```

**Vision LLM pour tableaux complexes** :
```python
async def analyze_table_with_vision(image):
    """Utilise GPT-4 Vision ou Claude pour analyser un tableau"""
    response = await vision_llm.analyze(
        image=image,
        prompt="Extrait les donn√©es de ce tableau sous forme JSON structur√©"
    )
    return response
```

#### 2. G√©n√©ration de Documents

**Templates Jinja2** :
```python
from jinja2 import Template

def generate_contract(template_name, context):
    """G√©n√©ration de contrat personnalis√©"""
    template = load_template(f"templates/{template_name}.j2")
    
    # Enrichissement du contexte via LLM
    enriched_context = llm_enrich_context(context)
    
    # G√©n√©ration
    document = template.render(**enriched_context)
    
    # Conversion Markdown ‚Üí PDF
    pdf = convert_md_to_pdf(document)
    
    return pdf
```

**Exemple** : G√©n√©ration automatique de Convention d'√âtude √† partir d'un brief client.

#### 3. Int√©gration √âtendue

**Plugin Google Workspace** :
- Add-on Google Docs : assistance r√©daction contrat
- Extension Gmail : d√©tection clauses dangereuses emails clients

**Bot Discord**[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks]
    ‚Üì
[Context Building] ‚Üí [Prompt Engineering] ‚Üí [Claude LLM] ‚Üí [Response Formatting]
    ‚Üì
[JSON R√©ponse] ‚Üí [Slack Bot / Web UI / API Client]
```

### Phase 1 : Acquisition des Donn√©es (Scraping)

#### Architecture du Scraping Selenium

Le scraping s'effectue via des scripts Python d√©di√©s par source, utilisant Selenium WebDriver pour g√©rer le JavaScript et les interactions complexes.

**Script principal** : `scrapers/kiwi_scraper.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import json
from datetime import datetime

class KiwiScraper:
    def __init__(self, headless=True):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def scrape_kiwi_legal(self):
        """Scrape Kiwi Legal documents"""
        base_url = "https://kiwi.cnje.fr/legal"
        self.driver.get(base_url)
        
        # Attente du chargement dynamique
        self.wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "document-list"))
        )
        
        documents = []
        doc_elements = self.driver.find_elements(By.CLASS_NAME, "document-item")
        
        for element in doc_elements:
            doc_data = self._extract_legal_document(element)
            documents.append(doc_data)
        
        return documents
```

**Gestion de la pagination** :
```python
def scrape_with_pagination(self, url, max_pages=None):
    page = 1
    all_data = []
    
    while True:
        print(f"Scraping page {page}...")
        self.driver.get(f"{url}?page={page}")
        
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "content"))
            )
        except TimeoutException:
            print(f"No more pages after page {page-1}")
            break
        
        page_data = self._extract_page_content()
        if not page_data:
            break
        
        all_data.extend(page_data)
        page += 1
        
        if max_pages and page > max_pages:
            break
    
    return all_data
```

**Gestion des erreurs et retry** :
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def robust_scrape(self, url):
    try:
        self.driver.get(url)
        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        return self._extract_content()
    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")
        raise
```

#### Extraction et Nettoyage HTML

Apr√®s extraction Selenium, parsing avec BeautifulSoup pour nettoyage :

```python
from bs4 import BeautifulSoup
import re

def clean_html_content(raw_html):
    """Nettoyage HTML et extraction texte pertinent"""
    soup = BeautifulSoup(raw_html, 'html.parser')
    
    # Suppression √©l√©ments non pertinents
    for element in soup(['script', 'style', 'nav', 'footer', 'header']):
        element.decompose()
    
    # Suppression classes publicitaires
    for ad in soup.find_all(class_=['advertisement', 'popup', 'banner']):
        ad.decompose()
    
    # Extraction texte
    text = soup.get_text(separator='\n', strip=True)
    
    # Nettoyage espaces multiples
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    return text

def extract_metadata(soup):
    """Extraction m√©tadonn√©es structur√©es"""
    metadata = {}
    
    # Titre
    title_tag = soup.find('h1') or soup.find('title')
    metadata['title'] = title_tag.get_text(strip=True) if title_tag else "Unknown"
    
    # Date publication
    date_tag = soup.find('time') or soup.find(class_='date')
    if date_tag:
        metadata['date'] = date_tag.get('datetime') or date_tag.get_text(strip=True)
    
    # Auteur
    author_tag = soup.find(class_='author') or soup.find(rel='author')
    if author_tag:
        metadata['author'] = author_tag.get_text(strip=True)
    
    # Cat√©gorie
    category_tag = soup.find(class_='category')
    if category_tag:
        metadata['category'] = category_tag.get_text(strip=True)
    
    return metadata
```

#### Structure JSON Standardis√©e

Export dans un format JSON unifi√© facilitant le traitement ult√©rieur :

**Format Legal** :
```json
{
  "source": "kiwi_legal",
  "document_type": "statuts",
  "scraping_metadata": {
    "url": "https://kiwi.cnje.fr/legal/statuts-types-association",
    "date_scraped": "2025-01-15T10:30:00Z",
    "scraper_version": "2.1.0"
  },
  "metadata": {
    "title": "Statuts types Junior-Entreprise association loi 1901",
    "category": "juridique",
    "subcategory": "statuts",
    "publication_date": "2024-06-01",
    "author": "Commission Juridique CNJE"
  },
  "content": {
    "sections": [
      {
        "title": "TITRE I - Dispositions g√©n√©rales",
        "articles": [
          {
            "number": 1,
            "title": "D√©nomination",
            "content": "Il est fond√© entre les adh√©rents aux pr√©sents statuts..."
          }
        ]
      }
    ],
    "full_text": "STATUTS TYPES..."
  }
}
```

**Format RSE** :
```json
{
  "source": "kiwi_rse",
  "document_type": "module_rse",
  "scraping_metadata": {...},
  "metadata": {
    "title": "Module Environnement - Gestion des D√©chets",
    "pilier": "environnemental",
    "odd_concernes": [12, 13],
    "niveau_difficulte": "d√©butant"
  },
  "content": {
    "introduction": "La gestion des d√©chets...",
    "objectifs": ["R√©duire la production", "Recycler"],
    "actions": [
      {
        "titre": "Mise en place du tri s√©lectif",
        "description": "...",
        "indicateurs": ["Taux de recyclage", "Volume d√©chets"]
      }
    ]
  }
}
```

**Format FAQ** :
```json
{
  "source": "kiwi_faq",
  "document_type": "faq",
  "scraping_metadata": {...},
  "metadata": {
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2
  },
  "content": {
    "questions": [
      {
        "id": "compta_tva_001",
        "question": "Comment d√©clarer la TVA en tant que JE ?",
        "reponse": "Les Junior-Entreprises b√©n√©ficient...",
        "tags": ["tva", "d√©claration", "comptabilit√©"],
        "related_questions": ["compta_tva_002", "compta_tva_005"]
      }
    ]
  }
}
```

#### Stockage et Versioning

**Arborescence de stockage** :
```
data/
‚îú‚îÄ‚îÄ raw/                          # Donn√©es brutes apr√®s scraping
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_2025-01-15.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_2025-01-15.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_2025-01-15.json
‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_legal_processed.json
‚îÇ   ‚îú‚îÄ‚îÄ kiwi_rse_processed.json
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_faq_processed.json
‚îú‚îÄ‚îÄ index/                        # Index g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ kiwi_advanced_index.pkl
‚îî‚îÄ‚îÄ logs/                         # Logs de scraping
    ‚îî‚îÄ‚îÄ scraping_2025-01-15.log
```

**Logging d√©taill√©** :
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scraping_{datetime.now().date()}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Dans le scraper
logger.info(f"Starting scrape of {url}")
logger.info(f"Extracted {len(documents)} documents")
logger.warning(f"Failed to extract metadata for document {doc_id}")
logger.error(f"Scraping failed: {exception}")
```

### Phase 2 : Preprocessing & Transformation

#### Type Detection Automatique

Algorithme de d√©tection bas√© sur plusieurs signaux :

```python
class DocumentTypeDetector:
    def __init__(self):
        self.type_patterns = {
            'legal': {
                'filename': ['statuts', 'contrat', 'legal', 'juridique'],
                'fields': ['articles', 'sections', 'clauses'],
                'keywords': ['article', 'alin√©a', 'conform√©ment', 'obligation']
            },
            'rse': {
                'filename': ['rse', 'durable', 'environnement'],
                'fields': ['pilier', 'odd', 'actions'],
                'keywords': ['d√©veloppement durable', 'odd', 'responsabilit√©']
            },
            'faq': {
                'filename': ['faq', 'questions'],
                'fields': ['questions', 'reponses'],
                'keywords': ['comment', 'pourquoi', 'qu\'est-ce']
            },
            'je': {
                'filename': ['annuaire', 'je', 'junior'],
                'fields': ['nom', 'ville', 'ecole', 'domaines'],
                'keywords': ['junior-entreprise', '√©cole', 'domaine']
            }
        }
    
    def detect_type(self, document_data, filename):
        scores = {doc_type: 0 for doc_type in self.type_patterns}
        
        # Score filename
        for doc_type, patterns in self.type_patterns.items():
            for pattern in patterns['filename']:
                if pattern in filename.lower():
                    scores[doc_type] += 2
        
        # Score fields pr√©sents
        doc_fields = set(document_data.get('content', {}).keys())
        for doc_type, patterns in self.type_patterns.items():
            matching_fields = doc_fields.intersection(patterns['fields'])
            scores[doc_type] += len(matching_fields) * 3
        
        # Score keywords dans le contenu
        content_text = json.dumps(document_data).lower()
        for doc_type, patterns in self.type_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_text:
                    scores[doc_type] += 1
        
        # S√©lection du type avec le score maximal
        detected_type = max(scores, key=scores.get)
        confidence = scores[detected_type] / sum(scores.values()) if sum(scores.values()) > 0 else 0
        
        return {
            'type': detected_type if confidence > 0.3 else 'general',
            'confidence': confidence,
            'scores': scores
        }
```

#### Extraction Sp√©cialis√©e par Type

**Extracteur Legal** :
```python
class LegalExtractor:
    def extract(self, document):
        extracted_data = []
        
        sections = document['content']['sections']
        for section in sections:
            section_title = section['title']
            
            for article in section.get('articles', []):
                extracted_data.append({
                    'text': f"{article['title']}\n{article['content']}",
                    'type': 'legal',
                    'metadata': {
                        'document_type': document['document_type'],
                        'section': section_title,
                        'article_num': article['number'],
                        'title': article['title']
                    }
                })
        
        return extracted_data
```

**Extracteur FAQ** :
```python
class FAQExtractor:
    def extract(self, document):
        extracted_data = []
        
        category = document['metadata']['category']
        subcategory = document['metadata'].get('subcategory', '')
        level = document['metadata'].get('level', 1)
        
        for qa in document['content']['questions']:
            # Contexte hi√©rarchique
            context_path = f"{category}"
            if subcategory:
                context_path += f" > {subcategory}"
            
            text = f"Question: {qa['question']}\n\nR√©ponse: {qa['reponse']}"
            
            extracted_data.append({
                'text': text,
                'type': 'faq',
                'metadata': {
                    'question': qa['question'],
                    'category': category,
                    'subcategory': subcategory,
                    'level': level,
                    'context_path': context_path,
                    'tags': qa.get('tags', []),
                    'related_questions': qa.get('related_questions', [])
                }
            })
        
        return extracted_data
```

**Extracteur JE** :
```python
class JEExtractor:
    def extract(self, document):
        extracted_data = []
        
        for je in document['content']['junior_entreprises']:
            # Construction texte descriptif
            text = f"""
            Nom: {je['nom']}
            Ville: {je['ville']}
            √âcole: {je['ecole']}
            Domaines d'expertise: {', '.join(je['domaines'])}
            Contact: {je['contact']['email']}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'je',
                'metadata': {
                    'nom': je['nom'],
                    'ville': je['ville'],
                    'ecole': je['ecole'],
                    'domaines': je['domaines'],
                    'contact': je['contact'],
                    'certified': je.get('certified_cnje', False)
                }
            })
        
        return extracted_data
```

**Extracteur RSE** :
```python
class RSEExtractor:
    def extract(self, document):
        extracted_data = []
        
        pilier = document['metadata']['pilier']
        odd = document['metadata']['odd_concernes']
        
        # Extraction par action
        for action in document['content']['actions']:
            text = f"""
            Module RSE: {document['metadata']['title']}
            Pilier: {pilier}
            
            Action: {action['titre']}
            {action['description']}
            
            Indicateurs: {', '.join(action['indicateurs'])}
            """
            
            extracted_data.append({
                'text': text.strip(),
                'type': 'rse',
                'metadata': {
                    'module': document['metadata']['title'],
                    'pilier': pilier,
                    'odd': odd,
                    'action_titre': action['titre'],
                    'indicateurs': action['indicateurs']
                }
            })
        
        return extracted_data
```

#### Smart Chunking S√©mantique

Le chunking respecte la logique m√©tier plut√¥t qu'une simple d√©coupe par longueur :

```python
class SemanticChunker:
    def __init__(self, min_length=50, max_length=1000, target_length=300):
        self.min_length = min_length
        self.max_length = max_length
        self.target_length = target_length
    
    def chunk_text(self, text, doc_type, metadata):
        if doc_type == 'faq':
            # FAQ: chaque Q/A est un chunk autonome
            return self._chunk_faq(text, metadata)
        elif doc_type == 'legal':
            # Legal: d√©coupage par article/section
            return self._chunk_legal(text, metadata)
        elif doc_type == 'je':
            # JE: entit√© atomique, pas de d√©coupage
            return [self._create_chunk(text, doc_type, metadata)]
        elif doc_type == 'rse':
            # RSE: d√©coupage par action
            return self._chunk_rse(text, metadata)
        else:
            # G√©n√©rique: d√©coupage par paragraphes avec overlap
            return self._chunk_generic(text, doc_type, metadata)
    
    def _chunk_generic(self, text, doc_type, metadata):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_length:
                current_chunk += para + "\n\n"
            else:
                if len(current_chunk) > self.min_length:
                    chunks.append(
                        self._create_chunk(current_chunk.strip(), doc_type, metadata)
                    )
                current_chunk = para + "\n\n"
        
        if len(current_chunk) > self.min_length:
            chunks.append(
                self._create_chunk(current_chunk.strip(), doc_type, metadata)
            )
        
        return chunks
    
    def _create_chunk(self, text, doc_type, metadata):
        return {
            'text': text,
            'type': doc_type,
            'metadata': metadata,
            'length': len(text),
            'word_count': len(text.split())
        }
```

#### Enrichissement M√©tadonn√©es

Chaque chunk est enrichi automatiquement :

```python
class MetadataEnricher:
    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.category_classifier = CategoryClassifier()
    
    def enrich_chunk(self, chunk):
        text = chunk['text']
        
        # Extraction keywords automatique
        keywords = self.keyword_extractor.extract(text, top_n=5)
        chunk['metadata']['keywords'] = keywords
        
        # Classification cat√©gorie fine (si pas d√©j√† pr√©sente)
        if 'category' not in chunk['metadata']:
            category = self.category_classifier.classify(text)
            chunk['metadata']['category'] = category
        
        # Calcul de priorit√© (bas√© sur usage historique si disponible)
        chunk['metadata']['priority'] = self._calculate_priority(chunk)
        
        # Ajout timestamps
        chunk['metadata']['indexed_at'] = datetime.now().isoformat()
        
        # G√©n√©ration d'un hash pour d√©tecter les modifications
        chunk['metadata']['content_hash'] = hashlib.md5(
            text.encode()
        ).hexdigest()
        
        return chunk
    
    def _calculate_priority(self, chunk):
        # Heuristique simple : sources officielles = haute priorit√©
        priority = 0.5
        
        if chunk['type'] == 'legal':
            priority += 0.2
        if 'statuts' in chunk.get('metadata', {}).get('category', '').lower():
            priority += 0.15
        if chunk['metadata'].get('is_featured', False):
            priority += 0.1
        
        return min(priority, 1.0)
```

### Phase 3 : Vectorisation et Indexation

#### Configuration TF-IDF Optimis√©e

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

class IndexBuilder:
    def __init__(self):
        # Stopwords personnalis√©s JE
        self.custom_stopwords = [
            'junior', 'entreprise', 'je', 'cnje',
            '√©tudiant', '√©tudiante', 'projet', 'mission',
            'conform√©ment', 'article', 'alin√©a', 'paragraphe'
        ]
        
        # Configuration TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.8,
            stop_words=self.custom_stopwords,
            sublinear_tf=True,
            norm='l2',
            strip_accents='unicode'
        )
    
    def build_index(self, chunks):
        print(f"Building index from {len(chunks)} chunks...")
        
        # Extraction des textes
        texts = [chunk['text'] for chunk in chunks]
        
        # Vectorisation TF-IDF
        print("Vectorizing with TF-IDF...")
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
        
        # R√©duction dimensionnelle SVD
        print("Applying SVD dimensionality reduction...")
        n_components = min(300, tfidf_matrix.shape[0] - 1)
        svd_model = TruncatedSVD(
            n_components=n_components,
            algorithm='randomized',
            n_iter=7,
            random_state=42
        )
        vectors_reduced = svd_model.fit_transform(tfidf_matrix)
        print(f"Reduced to {n_components} dimensions")
        
        # Construction des index secondaires
        print("Building secondary indexes...")
        metadata_index = self._build_metadata_indexes(chunks)
        
        # Assemblage de l'index complet
        index = {
            'vectorizer': self.vectorizer,
            'svd_model': svd_model,
            'vectors': vectors_reduced,
            'chunks': chunks,
            'metadata_index': metadata_index,
            'version': '2.1.0',
            'build_date': datetime.now().isoformat(),
            'statistics': {
                'n_chunks': len(chunks),
                'n_features': tfidf_matrix.shape[1],
                'n_components': n_components,
                'vocabulary_size': len(self.vectorizer.vocabulary_)
            }
        }
        
        return index
    
    def _build_metadata_indexes(self, chunks):
        by_type = {}
        by_category = {}
        by_source = {}
        
        for idx, chunk in enumerate(chunks):
            # Index by type
            chunk_type = chunk['type']
            if chunk_type not in by_type:
                by_type[chunk_type] = []
            by_type[chunk_type].append(idx)
            
            # Index by category
            category = chunk['metadata'].get('category', 'unknown')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(idx)
            
            # Index by source
            source = chunk['metadata'].get('source_file', 'unknown')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(idx)
        
        return {
            'by_type': by_type,
            'by_category': by_category,
            'by_source': by_source
        }
    
    def save_index(self, index, filepath='data/index/kiwi_advanced_index.pkl'):
        print(f"Saving index to {filepath}...")
        with open(filepath, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"Index saved successfully ({file_size_mb:.2f} MB)")
```

#### Processus Complet d'Indexation

Script principal orchestrant tout le pipeline :

```python
def main_indexation_pipeline():
    print("=== COMPLY INDEXATION PIPELINE ===\n")
    
    # 1. Chargement des donn√©es sources
    print("Step 1: Loading source data...")
    legal_data = load_json('data/processed/kiwi_legal_processed.json')
    rse_data = load_json('data/processed/kiwi_rse_processed.json')
    faq_data = load_json('data/processed/kiwi_faq_processed.json')
    je_data = load_json('data/processed/kiwi_je_processed.json')
    
    all_sources = [
        ('legal', legal_data),
        ('rse', rse_data),
        ('faq', faq_data),
        ('je', je_data)
    ]
    
    # 2. Extraction et chunking
    print("\nStep 2: Extracting and chunking...")
    all_chunks = []
    
    for source_type, data in all_sources:
        extractor = get_extractor(source_type)
        chunks = extractor.extract(data)
        
        # Chunking s√©mantique
        chunker = SemanticChunker()
        chunked_data = []
        for chunk in chunks:
            chunked_data.extend(
                chunker.chunk_text(
                    chunk['text'],
                    chunk['type'],
                    chunk['metadata']
                )
            )
        
        print(f"  - {source_type}: {len(chunked_data)} chunks")
        all_chunks.extend(chunked_data)
    
    print(f"Total chunks: {len(all_chunks)}")
    
    # 3. Enrichissement
    print("\nStep 3: Enriching metadata...")
    enricher = MetadataEnricher()
    enriched_chunks = [enricher.enrich_chunk(c) for c in all_chunks]
    
    # 4. Construction de l'index
    print("\nStep 4: Building vector index...")
    builder = IndexBuilder()
    index = builder.build_index(enriched_chunks)
    
    # 5. Persistance
    print("\nStep 5: Saving index...")
    builder.save_index(index)
    
    # 6. Statistiques finales
    print("\n=== INDEXATION COMPLETE ===")
    print(f"Total chunks indexed: {index['statistics']['n_chunks']}")
    print(f"Vocabulary size: {index['statistics']['vocabulary_size']}")
    print(f"Vector dimensions: {index['statistics']['n_components']}")
    print(f"Index version: {index['version']}")
    
    return index

if __name__ == "__main__":
    main_indexation_pipeline()
```

### Phase 4 : Serving et Recherche

#### Chargement de l'Index au D√©marrage

```python
from fastapi import FastAPI
import pickle

app = FastAPI(title="Comply API", version="2.1.0")

# Chargement de l'index au d√©marrage (√©v√©nement startup)
@app.on_event("startup")
async def load_index():
    global INDEX
    
    print("Loading Comply index...")
    start_time = time.time()
    
    with open('data/index/kiwi_advanced_index.pkl', 'rb') as f:
        INDEX = pickle.load(f)
    
    load_time = time.time() - start_time
    print(f"Index loaded in {load_time:.2f}s")
    print(f"  - Version: {INDEX['version']}")
    print(f"  - Chunks: {INDEX['statistics']['n_chunks']}")
    print(f"  - Memory: {sys.getsizeof(INDEX) / (1024**2):.2f} MB")
```

#### Endpoint /ask - Impl√©mentation Compl√®te

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    context: Optional[dict] = None
    options: Optional[dict] = None

class ComprehensiveAnswer(BaseModel):
    answer: str
    confidence: float
    detected_type: str
    sources: List[dict]
    related_questions: List[str]
    processing_time_ms: int

@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    start_time = time.time()
    
    try:
        # 1. D√©tection du type de requ√™te
        query_type_result = detect_query_type(request.question)
        detected_type = query_type_result['detected_type']
        
        # 2. Recherche vectorielle avec boosting
        search_results = vector_search(
            query=request.question,
            query_type=detected_type,
            top_k=request.options.get('max_chunks', 10) if request.options else 10
        )
        
        # 3. Construction du contexte
        context_string = build_context(search_results['chunks'])
        
        # 4. Prompt engineering
        prompt = generate_prompt(
            question=request.question,
            context=context_string,
            query_type=detected_type
        )
        
        # 5. Appel LLM
        llm_response = await call_claude(prompt)
        
        # 6. Post-processing
        formatted_response = format_response(
            raw_response=llm_response['response'],
            context_chunks=search_results['chunks'],
            query_type=detected_type
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return ComprehensiveAnswer(
            answer=formatted_response['answer'],
            confidence=formatted_response['confidence'],
            detected_type=detected_type,
            sources=formatted_response['sources'],
            related_questions=formatted_response['related_questions'],
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Infrastructure Recommand√©e

### H√©bergement VPS

Pour un d√©ploiement en production, un VPS Debian offre le meilleur compromis performance/co√ªt/contr√¥le.

**Sp√©cifications recommand√©es** :

| Composant | Minimum | Recomman# Comply by Sepefrei

![Comply Logo](comply_logo.png)

> **Assistant IA de conformit√© et knowledge management pour Junior-Entreprises**  
> Syst√®me de recherche vectorielle et question/r√©ponse aliment√© par Claude AI (Anthropic)

---

## Sommaire

1. [Introduction](#introduction)
2. [√âquipe de D√©veloppement](#√©quipe-de-d√©veloppement)
3. [Cas d'Usage et Avantages](#cas-dusage-et-avantages)
4. [Architecture Technique](#architecture-technique)
5. [Stack Technologique](#stack-technologique)
6. [Pipeline de Donn√©es](#pipeline-de-donn√©es)
7. [Fonctionnement du Syst√®me](#fonctionnement-du-syst√®me)
8. [Infrastructure Recommand√©e](#infrastructure-recommand√©e)
9. [Pr√©requis Serveur](#pr√©requis-serveur)
10. [Roadmap Technique](#roadmap-technique)
11. [Architecture D√©taill√©e](#architecture-d√©taill√©e)
12. [Choix Techniques et Justifications](#choix-techniques-et-justifications)

---

## Introduction

**Comply** repr√©sente une avanc√©e majeure dans l'automatisation du knowledge management pour les Junior-Entreprises. D√©velopp√© comme un syst√®me de question/r√©ponse intelligent, Comply exploite les derni√®res avanc√©es en recherche vectorielle et en traitement du langage naturel pour offrir un acc√®s instantan√© √† l'ensemble du corpus documentaire de l'√©cosyst√®me JE.

Le syst√®me repose sur une architecture sophistiqu√©e qui combine vectorisation TF-IDF, r√©duction dimensionnelle par SVD, recherche s√©mantique avec boosting contextuel, et g√©n√©ration de r√©ponses via le mod√®le Claude d'Anthropic. Cette stack permet de traiter des requ√™tes complexes en moins de 2 secondes avec un taux de pr√©cision sup√©rieur √† 90%.

Comply indexe automatiquement des milliers de documents provenant de sources h√©t√©rog√®nes (Kiwi Legal, Kiwi RSE, base JE, FAQ CNJE) et les structure en chunks s√©mantiques enrichis de m√©tadonn√©es. L'intelligence du syst√®me r√©side dans sa capacit√© √† comprendre le contexte m√©tier de chaque requ√™te et √† adapter dynamiquement son prompt LLM pour maximiser la pertinence des r√©ponses.

Au-del√† d'un simple chatbot, Comply constitue une infrastructure de recherche vectorielle r√©utilisable, expos√©e via une API FastAPI modulaire et document√©e (OpenAPI). Cette approche "API-first" permet son int√©gration dans n'importe quel outil de l'√©cosyst√®me JE : Slack, portails web, CRM, outils de gestion de projet, etc.

---

## √âquipe de D√©veloppement

Comply a √©t√© con√ßu et d√©velopp√© par le **P√¥le Syst√®me d'Information & Performance de SEPEFREI**, dans le cadre d'une initiative visant √† industrialiser le knowledge management de la Conf√©d√©ration.

**Lucas Lantrua** - RAG Engineering, Data Pipeline & Indexation
- Architecture du syst√®me RAG (Retrieval-Augmented Generation)
- D√©veloppement complet du pipeline de scraping (Selenium, parsing, nettoyage)
- Conception et impl√©mentation du syst√®me de vectorisation (TF-IDF + SVD)
- Design du chunking s√©mantique et de l'enrichissement m√©tadonn√©es
- Entra√Ænement et optimisation du mod√®le d'indexation
- Configuration du syst√®me de recherche vectorielle avec boosting

**Matteo Bonnet** - Backend & API Development
- Architecture FastAPI et design des endpoints
- Impl√©mentation de la couche serving et du routing intelligent
- Gestion de la persistance (Pickle) et du chargement en m√©moire
- D√©veloppement des m√©canismes de r√©indexation
- Int√©gration avec l'API Claude (Anthropic)
- Optimisation des performances et de la latence

**Victoria Breuling** - Product Management & Strategic Vision
- D√©finition de la vision produit et des cas d'usage m√©tier
- Analyse des besoins utilisateurs (Junior-Entrepreneurs, auditeurs, formateurs)
- Priorisation des fonctionnalit√©s et roadmap produit
- Coordination avec les parties prenantes SEPEFREI
- Design de l'exp√©rience utilisateur et des interactions
- Validation m√©tier et tests d'acceptation

---

## Cas d'Usage et Avantages

### Acc√©l√©ration Drastique de l'Onboarding

L'int√©gration d'un nouveau membre dans une Junior-Entreprise repr√©sente traditionnellement un investissement temps consid√©rable. Entre la compr√©hension des statuts, l'appropriation des processus m√©tier, la ma√Ætrise des obligations l√©gales et la familiarisation avec l'√©cosyst√®me CNJE, plusieurs semaines sont n√©cessaires avant qu'un nouveau membre soit pleinement op√©rationnel.

**Comply transforme ce processus** :
- R√©ponses instantan√©es aux questions de base sans mobiliser les membres exp√©riment√©s
- Acc√®s guid√© √† toute la documentation m√©tier via conversation naturelle
- Formation progressive et interactive sur les proc√©dures internes
- Parcours d'apprentissage personnalis√© selon le r√¥le (pr√©sident, tr√©sorier, responsable qualit√©)
- Disponibilit√© 24/7 permettant un apprentissage au rythme de chacun

**R√©sultat mesur√©** : R√©duction de 60% du temps d'accompagnement n√©cessaire, permettant aux √©quipes de se concentrer sur les missions √† forte valeur ajout√©e.

### Conformit√© Juridique Continue

Les Junior-Entreprises √©voluent dans un cadre juridique complexe, m√™lant droit associatif, droit du travail, r√©glementation URSSAF et normes CNJE. La m√©connaissance de ces r√®gles peut entra√Æner des sanctions financi√®res, des probl√®mes lors des audits, voire la mise en danger de la structure.

**Comply agit comme un juriste de poche** :
- V√©rification instantan√©e de la l√©galit√© d'une action envisag√©e (recrutement, facturation, √©v√©nement)
- Acc√®s imm√©diat aux statuts types et r√©glementations applicables
- Clarification des obligations d√©claratives (URSSAF, pr√©fecture, rectorat)
- Guidance sur les clauses contractuelles standards
- Alerte sur les risques juridiques potentiels d'une d√©cision

**Exemple concret** : "Puis-je facturer une mission √† une entreprise √©trang√®re ?" ‚Üí Comply analyse le contexte, extrait les r√®gles de TVA intracommunautaire, cite les articles pertinents des statuts CNJE, et fournit une r√©ponse structur√©e avec sources.

### Pr√©paration et Post-Traitement d'Audit

Les audits CNJE sont des moments critiques dans la vie d'une Junior-Entreprise. Une pr√©paration insuffisante ou une mauvaise r√©action aux points de non-conformit√© peut compromettre la labellisation et la cr√©dibilit√© de la structure.

**Comply r√©volutionne la gestion des audits** :

**Phase de pr√©paration** :
- Simulation d'audit blanc via questionnaire guid√©
- V√©rification automatique de la conformit√© documentaire
- Identification proactive des points de vigilance
- G√©n√©ration de checklists personnalis√©es selon le type d'audit
- Recommandations d'actions pr√©ventives

**Phase post-audit** :
- Analyse des remarques et non-conformit√©s identifi√©es
- G√©n√©ration d'un plan d'actions correctives prioris√©
- Guidance pour la mise en ≈ìuvre de chaque correction
- Suivi de la r√©solution des points bloquants
- Pr√©paration de la r√©ponse formelle √† l'auditeur

**Fonctionnalit√© avanc√©e** : L'auditeur blanc IA post-traitement permet de soumettre le rapport d'audit complet √† Comply, qui g√©n√®re automatiquement un plan de mise en conformit√© d√©taill√© avec timeline, responsables sugg√©r√©s et ressources documentaires associ√©es.

### Strat√©gie RSE et D√©veloppement Durable

La Responsabilit√© Soci√©tale des Entreprises devient un crit√®re diff√©renciant pour les Junior-Entreprises, tant pour la labellisation que pour le d√©veloppement commercial. N√©anmoins, structurer une d√©marche RSE coh√©rente requiert une expertise sp√©cifique souvent absente des √©quipes.

**Comply facilite l'impl√©mentation RSE** :
- Diagnostic RSE initial avec identification des axes prioritaires
- Proposition de strat√©gie RSE adapt√©e au contexte (taille, √©cole, moyens)
- V√©rification de la coh√©rence des initiatives avec les standards RSE
- Mapping des actions avec les Objectifs de D√©veloppement Durable (ODD)
- Recommandations d'indicateurs de suivi et de mesure d'impact
- Templates de reporting RSE conformes aux exigences CNJE

**Exemple d'usage** : "Comment structurer notre d√©marche environnementale ?" ‚Üí Comply analyse les modules RSE disponibles, propose un plan d'action en trois phases (quick wins, projets moyens terme, vision long terme), sugg√®re des partenariats avec des structures engag√©es, et fournit des exemples d'actions r√©ussies dans d'autres JE.

### Gestion Contractuelle et Juridique Op√©rationnelle

La r√©daction et la validation de contrats repr√©sentent un risque majeur pour les Junior-Entreprises. Contrats d'√©tude mal ficel√©s, clauses protectrices absentes, engagements de moyens vs. r√©sultats mal d√©finis : autant de sources potentielles de litiges.

**Comply s√©curise la contractualisation** :
- Assistance √† la r√©daction de clauses sp√©cifiques (confidentialit√©, propri√©t√© intellectuelle, responsabilit√©)
- V√©rification de la conformit√© d'un contrat avec les standards CNJE
- Explication d√©taill√©e des obligations contractuelles
- Alerte sur les clauses potentiellement dangereuses
- Proposition de templates valid√©s juridiquement
- Guidance sur la gestion de contentieux clients

**Cas d'usage type** : Upload d'un contrat re√ßu d'un client ‚Üí Comply analyse les clauses, identifie les points d'attention (ex: clause de p√©nalit√© disproportionn√©e), sugg√®re des reformulations protectrices, et g√©n√®re un document d'analyse complet.

### Gain de Temps Op√©rationnel Massif

Au-del√† des cas d'usage sp√©cifiques, Comply g√©n√®re un gain de productivit√© quotidien mesurable sur l'ensemble des op√©rations d'une Junior-Entreprise.

**Impact quantifi√©** :
- R√©duction de 70% du temps consacr√© aux questions administratives r√©currentes
- Division par 3 du temps de recherche documentaire
- Diminution de 50% du temps de pr√©paration des formations internes
- Lib√©ration de 5-10h/semaine pour les membres cl√©s (pr√©sident, VP qualit√©)

**Accessibilit√© maximale** :
- Disponibilit√© 24/7 sans interruption
- Temps de r√©ponse < 2 secondes
- Int√©gration native Slack (canal de communication principal des JE)
- Pas de formation n√©cessaire (conversation naturelle)

---

## Architecture Technique

### Vision Globale du Syst√®me

Comply repose sur une architecture pipeline modulaire orchestrant six couches fonctionnelles distinctes. Cette s√©paration permet une maintenance ais√©e, une scalabilit√© progressive et une √©volutivit√© technique sans refonte compl√®te.

**[IMAGE REQUISE : Sch√©ma architecture macro avec les 6 couches]**

```mermaid
flowchart TB
    subgraph Layer1["üì• LAYER 1: DATA SOURCES"]
        A1[Kiwi Legal<br/>Statuts, Contrats, R√®glements]
        A2[Kiwi RSE<br/>Modules, ODD, Standards]
        A3[Kiwi Base<br/>FAQ Multi-niveaux]
        A4[Base Junior-Entreprises<br/>Annuaire JE France]
    end

    subgraph Layer2["üîÑ LAYER 2: ACQUISITION SELENIUM"]
        B1[Scraper Kiwi Legal<br/>Navigation automatis√©e + extraction HTML]
        B2[Scraper Kiwi RSE<br/>Parsing structure modules]
        B3[Scraper Kiwi FAQ<br/>Extraction Q/A hi√©rarchiques]
        B4[Scripts Python Nettoyage<br/>Suppression balises, normalisation, encodage]
        B5[Export JSON Structur√©<br/>Format standardis√© par type source]
    end

    subgraph Layer3["‚öôÔ∏è LAYER 3: PREPROCESSING & CHUNKING"]
        C1[Type Detection Engine<br/>R√®gles s√©mantiques + pattern matching]
        C2[Extracteur Champs M√©tier<br/>FAQ: Q/A/niveau | Legal: article/section<br/>JE: contact/domaine | RSE: module/action]
        C3[Smart Chunking<br/>D√©coupe contextuelle s√©mantique<br/>Conservation hi√©rarchie]
        C4[Metadata Enrichment<br/>Tags, cat√©gories, priorit√©s<br/>Contexte parent, source]
    end

    subgraph Layer4["üßÆ LAYER 4: VECTORISATION & INDEXATION"]
        D1[TF-IDF Vectorizer<br/>Uni/bi/trigrammes<br/>Stopwords custom JE<br/>max_features: 5000]
        D2[Truncated SVD<br/>R√©duction dimensionnelle<br/>300 dimensions<br/>Compression espace vectoriel]
        D3[Multi-Index Builder<br/>by_type, by_category<br/>by_source, by_priority]
        D4[Pickle Persistence<br/>kiwi_advanced_index.pkl<br/>Chargement RAM < 1s]
    end

    subgraph Layer5["üöÄ LAYER 5: API SERVING FASTAPI"]
        E1[POST /ask<br/>Question/R√©ponse principale]
        E2[POST /search/advanced<br/>Recherche vectorielle contr√¥l√©e]
        E3[GET /search/je<br/>Lookup Junior-Entreprises]
        E4[GET /search/faq<br/>Recherche FAQ pure]
        E5[GET /legal/guidance<br/>Assistance juridique]
        E6[POST /reindex<br/>R√©indexation manuelle]
        E7[GET /stats/advanced<br/>M√©triques syst√®me]
    end

    subgraph Layer6["ü§ñ LAYER 6: LLM ORCHESTRATION"]
        F1[Query Type Detector<br/>R√®gles NLP classification<br/>juridique/rse/faq/je/g√©n√©ral]
        F2[Vector Search Engine<br/>Cosine similarity<br/>Top-K retrieval]
        F3[Contextual Booster<br/>Coefficients multiplicateurs<br/>type/cat√©gorie/source/date]
        F4[Context Builder<br/>Agr√©gation chunks<br/>Structuration m√©tadonn√©es]
        F5[Dynamic Prompt Engineering<br/>Templates sp√©cialis√©s par type<br/>Instructions m√©tier]
        F6[Claude API Integration<br/>Anthropic Claude Sonnet 4.5<br/>Context window 200k tokens]
        F7[Response Formatter<br/>JSON structur√©<br/>Tra√ßabilit√© sources]
    end

    subgraph Clients["üíª CLIENTS & INTEGRATIONS"]
        G1[Slack Bot<br/>@comply mention<br/>DM direct]
        G2[Web Portal<br/>Interface utilisateur<br/>Dashboard admin]
        G3[API Externe<br/>Int√©gration CRM/ERP<br/>Webhooks]
    end

    %% FLUX ACQUISITION
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    B1 --> B4
    B2 --> B4
    B3 --> B4
    B4 --> B5

    %% FLUX PREPROCESSING
    B5 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4

    %% FLUX INDEXATION
    C4 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> D4

    %% FLUX SERVING
    D4 -.Index charg√©.-> E1
    D4 -.Index charg√©.-> E2
    D3 -.M√©tadonn√©es.-> E3
    D3 -.M√©tadonn√©es.-> E4

    %% FLUX ORCHESTRATION
    E1 --> F1
    E2 --> F2
    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> F5
    F5 --> F6
    F6 --> F7

    %% FLUX CLIENTS
    F7 --> G1
    F7 --> G2
    F7 --> G3
    G1 -.Query.-> E1
    G2 -.Query.-> E1
    G3 -.Query.-> E2

    style Layer1 fill:#e3f2fd
    style Layer2 fill:#fff3e0
    style Layer3 fill:#f3e5f5
    style Layer4 fill:#e8f5e9
    style Layer5 fill:#fce4ec
    style Layer6 fill:#fff9c4
    style Clients fill:#e0f2f1
```

### D√©tail des Couches Architecture

#### Layer 1: Data Sources (Sources de Donn√©es)

Cette couche repr√©sente l'ensemble des sources documentaires exploit√©es par Comply. La diversit√© des sources garantit une couverture exhaustive du p√©rim√®tre m√©tier Junior-Entreprise.

**Kiwi Legal** : Plateforme centralis√©e de documentation juridique CNJE
- Statuts types par type de JE (association, SASU, etc.)
- Mod√®les de contrats valid√©s (Convention d'√âtude, Contrat de Prestation, NDA)
- R√®glements int√©rieurs types
- Documentation sur les obligations d√©claratives
- Jurisprudence et cas pratiques

**Kiwi RSE** : Base de connaissances RSE de la CNJE
- Modules RSE structur√©s par pilier (environnemental, social, gouvernance)
- Guides m√©thodologiques d'impl√©mentation
- R√©f√©rentiel d'indicateurs RSE
- Mapping avec les 17 ODD de l'ONU
- Exemples d'actions concr√®tes et retours d'exp√©rience

**Kiwi Base (FAQ)** : FAQ officielle multi-niveaux
- Questions/r√©ponses hi√©rarchis√©es par th√©matique
- Niveau 1 : Cat√©gories (Comptabilit√©, RH, Qualit√©, Commercial, etc.)
- Niveau 2 : Sous-cat√©gories (TVA, D√©clarations sociales, Audits, etc.)
- Niveau 3 : Questions sp√©cifiques avec r√©ponses d√©taill√©es
- Mise √† jour continue par les √©quipes CNJE

**Base Junior-Entreprises** : Annuaire complet
- ~200 Junior-Entreprises fran√ßaises r√©f√©renc√©es
- Donn√©es structur√©es : nom, ville, √©cole, domaines d'expertise
- Informations de contact (mail, t√©l√©phone, site web)
- M√©tadonn√©es (date de cr√©ation, effectif, CA, labellisation)

#### Layer 2: Acquisition Selenium (Scraping Automatis√©)

La couche d'acquisition repose sur **Selenium WebDriver** pour l'extraction automatis√©e du contenu des plateformes Kiwi. Ce choix technique s'explique par la nature dynamique des sites (JavaScript rendering, navigation complexe).

**Architecture du scraping** :
```
Selenium WebDriver (Chromium headless)
    ‚Üì
Navigation programmatique (login, menus, pagination)
    ‚Üì
Attente rendering JavaScript (explicit waits)
    ‚Üì
Extraction HTML (BeautifulSoup4)
    ‚Üì
Donn√©es brutes (HTML + m√©tadonn√©es)
```

**Scripts Python de nettoyage** :
Chaque source dispose de parsers sp√©cialis√©s qui :
- Supprimant les √©l√©ments non pertinents (navigation, footer, publicit√©s, scripts)
- Normalisent l'encodage (UTF-8 strict)
- Extraient la structure s√©mantique (titres, sections, listes)
- D√©tectent les m√©tadonn√©es (auteur, date, cat√©gorie)
- G√®rent les cas particuliers (tableaux, images avec alt text)

**Export JSON standardis√©** :
Format unifi√© permettant le traitement g√©n√©rique par la couche suivante :
```json
{
  "source": "kiwi_legal",
  "type": "statuts",
  "url": "https://...",
  "date_scraping": "2025-01-15",
  "metadata": {
    "titre": "Statuts types JE association",
    "categorie": "juridique",
    "sous_categorie": "statuts"
  },
  "content": {
    "sections": [...]
  }
}
```

**Robustesse et gestion d'erreurs** :
- Retry automatique avec backoff exponentiel (3 tentatives)
- D√©tection de changements de structure HTML (alerting)
- Logging complet de chaque run
- Validation des donn√©es extraites (sch√©mas Pydantic)

#### Layer 3: Preprocessing & Chunking (Traitement Intelligent)

Cette couche transforme les donn√©es brutes en chunks s√©mantiques optimis√©s pour la recherche vectorielle. C'est le c≈ìur de l'intelligence du syst√®me d'indexation.

**Type Detection Engine** :
Algorithme multi-crit√®res d√©terminant le type de chaque document :
- Analyse du nom de fichier (patterns regex)
- Inspection de la structure JSON (pr√©sence de champs sp√©cifiques)
- Analyse s√©mantique du contenu (vocabulaire caract√©ristique)
- Score de confiance et fallback sur type "g√©n√©rique"

**Extracteur de Champs M√©tier** :
Parsers sp√©cialis√©s par type de document :

*Pour les FAQ* :
- Extraction question/r√©ponse avec pr√©servation du contexte
- D√©tection du niveau hi√©rarchique (1, 2, 3)
- Identification de la cat√©gorie et sous-cat√©gorie
- Extraction des mots-cl√©s principaux

*Pour les documents l√©gaux* :
- Parsing de la structure (articles, sections, paragraphes)
- D√©tection du type de document (statuts, contrat, r√®glement)
- Extraction des r√©f√©rences crois√©es ("cf. article X")
- Identification des entit√©s juridiques (obligations, interdictions, droits)

*Pour les fiches JE* :
- Extraction structur√©e : nom, ville, √©cole, domaine
- Normalisation des champs (ex: "Ile-de-France" ‚Üí "√éle-de-France")
- Parsing des domaines d'expertise (string ‚Üí liste)
- Validation et nettoyage des contacts (format email, t√©l√©phone)

*Pour les modules RSE* :
- D√©tection du pilier RSE (environnemental, social, gouvernance)
- Extraction des actions recommand√©es
- Mapping avec les ODD concern√©s
- Identification des indicateurs de suivi

**Smart Chunking S√©mantique** :
Le d√©coupage ne se fait pas de mani√®re arbitraire (split par longueur) mais selon la logique m√©tier :

*FAQ* : Chaque paire Q/A = 1 chunk autonome
```
Chunk = {
    "text": "Question: ... R√©ponse: ...",
    "type": "faq",
    "category": "Comptabilit√©",
    "subcategory": "TVA",
    "level": 2,
    "parent_context": "Comptabilit√© > TVA"
}
```

*Documents l√©gaux* : D√©coupage par article ou section logique
```
Chunk = {
    "text": "Article 5 - ...",
    "type": "legal",
    "doc_type": "statuts",
    "section": "Gestion financi√®re",
    "article_num": 5,
    "references": ["article 3", "article 12"]
}
```

*Fiches JE* : Une fiche = un chunk (entit√© atomique)
```
Chunk = {
    "text": "Nom: ... √âcole: ... Domaine: ...",
    "type": "je",
    "nom": "...",
    "ville": "...",
    "ecole": "...",
    "domaines": [...],
    "contact": {...}
}
```

*Modules RSE* : D√©coupage par sous-section th√©matique
```
Chunk = {
    "text": "Module Environnement - Section D√©chets: ...",
    "type": "rse",
    "pilier": "environnemental",
    "module": "Gestion des d√©chets",
    "odd": [12, 13],
    "actions": [...]
}
```

**Taille des chunks** :
- Cible : 200-500 mots par chunk
- Maximum : 1000 mots (pour pr√©server la coh√©rence s√©mantique)
- Minimum : 50 mots (chunks trop courts = bruit dans l'index)

**Metadata Enrichment** :
Chaque chunk est enrichi automatiquement avec :
- Tags automatiques (extraction keywords via RAKE/YAKE)
- Cat√©gorie et sous-cat√©gorie (h√©rit√©es du document parent)
- Priorit√© (calcul√©e selon fr√©quence d'usage historique)
- Contexte parent (fil d'Ariane s√©mantique)
- Source originale (URL, fichier, date)
- Timestamps (cr√©ation, derni√®re modification)

#### Layer 4: Vectorisation & Indexation (Machine Learning)

Cette couche transforme les chunks textuels en repr√©sentations vectorielles haute dimension, puis les compresse et les indexe pour une recherche ultra-rapide.

**TF-IDF Vectorization** :
Choix du **TF-IDF** (Term Frequency - Inverse Document Frequency) plut√¥t que des embeddings denses pour des raisons de performance et d'interpr√©tabilit√©.

Configuration optimis√©e :
```python
TfidfVectorizer(
    max_features=5000,        # Vocabulaire limit√© aux 5000 termes les plus informatifs
    ngram_range=(1, 3),       # Uni, bi et trigrammes
    min_df=2,                 # Terme doit appara√Ætre dans au moins 2 documents
    max_df=0.8,               # Terme ne doit pas √™tre dans plus de 80% des docs
    stop_words=custom_stopwords,  # Stopwords personnalis√©s JE
    sublinear_tf=True,        # Log scaling du term frequency
    norm='l2'                 # Normalisation L2 des vecteurs
)
```

**Stopwords personnalis√©s** :
En plus des stopwords fran√ßais standards, ajout de termes sp√©cifiques non informatifs dans le contexte JE :
- "junior", "entreprise", "je", "cnje"
- "√©tudiant", "projet", "mission"
- Termes administratifs g√©n√©riques : "conform√©ment", "article", "alin√©a"

**Truncated SVD (R√©duction Dimensionnelle)** :
La matrice TF-IDF sparse (5000 dimensions) est compress√©e via **Singular Value Decomposition** tronqu√©e.

Objectifs :
- R√©duction de dimensions : 5000 ‚Üí 300
- Capture de la structure latente du corpus
- √âlimination du bruit et de la colin√©arit√©
- Acc√©l√©ration massive de la recherche (cosine similarity)

```python
TruncatedSVD(
    n_components=300,         # Dimensions cibles
    algorithm='randomized',   # M√©thode rapide pour grandes matrices
    n_iter=7,                 # It√©rations pour convergence
    random_state=42           # Reproductibilit√©
)
```

**Justification du nombre de composantes** :
- Tests empiriques sur le corpus : plateau de performance √† ~250 composantes
- 300 composantes = compromis entre expressivit√© et vitesse
- R√©duction de 95% de la dimensionnalit√© initiale
- Pr√©servation de ~85% de la variance totale

**Multi-Index Construction** :
Au-del√† de l'index vectoriel principal, construction d'index secondaires pour optimiser les filtres et le boosting :

*Index by_type* :
```python
{
    "faq": [0, 1, 15, 23, ...],      # IDs des chunks FAQ
    "legal": [2, 5, 8, 11, ...],     # IDs des chunks l√©gaux
    "je": [3, 7, 12, 19, ...],       # IDs des chunks JE
    "rse": [4, 9, 14, 18, ...]       # IDs des chunks RSE
}
```

*Index by_category* :
```python
{
    "comptabilit√©": [0, 5, 12, ...],
    "contrats": [2, 8, 15, ...],
    "rh": [1, 9, 18, ...],
    ...
}
```

*Index by_source* :
```python
{
    "kiwi_legal_statuts.json": [0, 5, 12, ...],
    "kiwi_rse_environnement.json": [3, 8, 15, ...],
    ...
}
```

*Index by_priority* :
Chunks tri√©s par score de priorit√© (fonction de l'usage historique) :
```python
[
    (id=42, priority=0.95),   # Chunk le plus consult√©
    (id=17, priority=0.89),
    ...
]
```

**Pickle Persistence** :
L'index complet est s√©rialis√© dans un unique fichier Pickle :

```python
index = {
    'vectorizer': fitted_tfidf_vectorizer,
    'svd_model': fitted_svd_model,
    'vectors': numpy_array_shape_(n_chunks, 300),
    'chunks': list_of_chunk_dicts,
    'metadata_index': {
        'by_type': {...},
        'by_category': {...},
        'by_source': {...},
        'by_priority': [...]
    },
    'version': '2.1.0',
    'build_date': datetime.datetime,
    'statistics': {
        'n_chunks': 8534,
        'n_types': 4,
        'n_categories': 27,
        'vocabulary_size': 5000
    }
}
```

**Taille et performance** :
- Fichier pickle : ~120 MB (pour ~8500 chunks)
- Chargement en RAM : < 1 seconde
- Empreinte m√©moire : ~300 MB en production
- Pas de d√©pendance externe (base de donn√©es, service cloud)

#### Layer 5: API Serving FastAPI (Exposition des Services)

FastAPI expose l'index vectoriel via une API REST document√©e, performante et type-safe.

**Architecture modulaire** :
```
app/
‚îú‚îÄ‚îÄ main.py                 # Point d'entr√©e FastAPI
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ ask.py             # Endpoint Q/A principal
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Endpoints de recherche
‚îÇ   ‚îú‚îÄ‚îÄ admin.py           # Endpoints administration
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py   # Logique recherche vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Orchestration LLM
‚îÇ   ‚îú‚îÄ‚îÄ type_detector.py   # D√©tection type requ√™te
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ request_models.py  # Mod√®les Pydantic requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ response_models.py # Mod√®les Pydantic r√©ponses
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ index_loader.py    # Chargement index Pickle
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ boosting.py        # Calcul des coefficients boost
    ‚îú‚îÄ‚îÄ prompt_templates.py # Templates prompts LLM
```

**Endpoints principaux** :

**POST /ask** - Question/R√©ponse intelligente (endpoint principal)
```python
@router.post("/ask", response_model=ComprehensiveAnswer)
async def ask_question(request: QuestionRequest):
    """
    Point d'entr√©e principal pour toute question utilisateur.
    Orchestre: d√©tection type ‚Üí recherche ‚Üí prompt LLM ‚Üí r√©ponse
    """
```

Request body :
```json
{
  "question": "Puis-je facturer une mission √† une entreprise belge ?",
  "context": {
    "user_role": "tr√©sorier",
    "je_name": "Junior ESCP",
    "history": []
  },
  "options": {
    "max_chunks": 10,
    "boost_legal": true,
    "include_sources": true
  }
}
```

Response :
```json
{
  "answer": "Oui, vous pouvez facturer une entreprise belge...",
  "confidence": 0.87,
  "detected_type": "juridique",
  "sources": [
    {
      "chunk_id": 1542,
      "text": "...",
      "type": "legal",
      "category": "TVA intracommunautaire",
      "score": 0.92,
      "source_file": "kiwi_legal_tva.json"
    }
  ],
  "related_questions": [
    "Comment d√©clarer la TVA intracommunautaire ?",
    "Quels documents pour une facture UE ?"
  ],
  "processing_time_ms": 1847
}
```

**POST /search/advanced** - Recherche vectorielle contr√¥l√©e
```python
@router.post("/search/advanced", response_model=SearchResults)
async def advanced_search(request: AdvancedSearchRequest):
    """
    Recherche vectorielle avec contr√¥le fin du boosting,
    filtrage par m√©tadonn√©es, et param√©trage du top-K.
    Usage: int√©grations avanc√©es, debug, analyse.
    """
```

Param√®tres :
```json
{
  "query": "obligations comptables JE",
  "filters": {
    "types": ["legal", "faq"],
    "categories": ["comptabilit√©"],
    "min_score": 0.5
  },
  "boosting": {
    "by_type": {"legal": 1.3, "faq": 1.1},
    "by_category": {"comptabilit√©": 1.2},
    "by_recency": true
  },
  "top_k": 15,
  "return_vectors": false
}
```

**GET /search/je** - Recherche sp√©cialis√©e Junior-Entreprises
```python
@router.get("/search/je", response_model=List[JEInfo])
async def search_junior_entreprises(
    query: str = Query(..., description="Crit√®re de recherche"),
    city: Optional[str] = None,
    school: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = Query(10, le=50)
):
    """
    Recherche dans l'annuaire JE avec filtres g√©ographiques,
    √©cole, et domaines d'expertise.
    """
```

Exemple : `GET /search/je?query=cybers√©curit√©&city=Paris&limit=5`

Response :
```json
[
  {
    "name": "ESGI Conseil",
    "city": "Paris",
    "school": "ESGI",
    "domains": ["Informatique", "Cybers√©curit√©", "DevOps"],
    "contact": {
      "email": "contact@esgiconseil.fr",
      "phone": "+33 1 XX XX XX XX",
      "website": "https://esgiconseil.fr"
    },
    "metadata": {
      "year_founded": 2005,
      "certified_cnje": true,
      "last_audit": "2024-09"
    }
  }
]
```

**GET /search/faq** - Recherche FAQ pure
Recherche optimis√©e dans la FAQ hi√©rarchique avec pr√©servation des niveaux.

**GET /legal/guidance** - Assistance juridique cibl√©e
Endpoint sp√©cialis√© pour questions juridiques avec boost maximal sur documents l√©gaux et g√©n√©ration de disclaimer.

**POST /reindex** - R√©indexation manuelle
```python
@router.post("/reindex", response_model=ReindexStatus)
async def trigger_reindex(
    auth: str = Header(...),
    full_reindex: bool = False
):
    """
    D√©clenche une r√©indexation compl√®te ou incr√©mentale.
    Requiert authentification admin.
    """
```

Process :
1. Backup de l'index actuel
2. Rechargement des JSON sources
3. Reprocessing complet (chunking, vectorisation)
4. G√©n√©ration nouvel index Pickle
5. Swap atomique (ancien ‚Üí nouveau)
6. Pas d'interruption de service (graceful reload)

**GET /stats/advanced** - M√©triques et statistiques syst√®me
```json
{
  "index": {
    "version": "2.1.0",
    "build_date": "2025-01-15T14:30:00Z",
    "total_chunks": 8534,
    "by_type": {
      "faq": 3421,
      "legal": 2876,
      "je": 198,
      "rse": 2039
    },
    "vocabulary_size": 5000,
    "index_size_mb": 118.7
  },
  "usage": {
    "total_queries_today": 147,
    "avg_response_time_ms": 1820,
    "llm_calls_today": 142,
    "cache_hit_rate": 0.12
  },
  "performance": {
    "uptime_seconds": 2847392,
    "memory_usage_mb": 312.4,
    "cpu_usage_percent": 8.2
  }
}
```

**Documentation OpenAPI automatique** :
- Swagger UI : `http://server/docs`
- ReDoc : `http://server/redoc`
- Sch√©ma JSON : `http://server/openapi.json`

#### Layer 6: LLM Orchestration (Intelligence Augment√©e)

Cette couche orchestre le pipeline complet de traitement des requ√™tes, de la d√©tection du type jusqu'√† la g√©n√©ration de la r√©ponse via Claude.

**Pipeline de traitement** :

```mermaid
sequenceDiagram
    participant User
    participant API
    participant TypeDetector
    participant VectorSearch
    participant Booster
    participant ContextBuilder
    participant PromptEngine
    participant Claude
    participant ResponseFormatter

    User->>API: POST /ask
    API->>TypeDetector: Analyse requ√™te
    Note over TypeDetector: R√®gles NLP<br/>Classification
    TypeDetector-->>API: Type: "juridique"<br/>Confidence: 0.89
    
    API->>VectorSearch: Vectorisation query
    VectorSearch->>VectorSearch: TF-IDF transform
    VectorSearch->>VectorSearch: SVD transform
    VectorSearch->>VectorSearch: Cosine similarity
    VectorSearch-->>API: Top 100 candidats
    
    API->>Booster: Application boosting
    Note over Booster: Boost type +30%<br/>Boost cat√©gorie +20%<br/>Boost r√©cence +10%
    Booster-->>API: Top 10 final
    
    API->>ContextBuilder: Construction contexte
    ContextBuilder->>ContextBuilder: Agr√©gation chunks
    ContextBuilder->>ContextBuilder: D√©duplication
    ContextBuilder->>ContextBuilder: Structuration m√©tadonn√©es
    ContextBuilder-->>API: Contexte enrichi
    
    API->>PromptEngine: G√©n√©ration prompt
    Note over PromptEngine: Template juridique<br/>Instructions m√©tier<br/>Contexte inject√©
    PromptEngine-->>API: Prompt final
    
    API->>Claude: Requ√™te LLM
    Note over Claude: Claude Sonnet 4.5<br/>200k tokens context
    Claude-->>API: R√©ponse g√©n√©r√©e
    
    API->>ResponseFormatter: Post-processing
    ResponseFormatter->>ResponseFormatter: Extraction sources
    ResponseFormatter->>ResponseFormatter: Calcul confidence
    ResponseFormatter->>ResponseFormatter: G√©n√©ration related_questions
    ResponseFormatter-->>API: JSON structur√©
    
    API-->>User: R√©ponse compl√®te
```

**Query Type Detector** :
Algorithme multi-r√®gles classifiant automatiquement le type de requ√™te :

R√®gles de d√©tection :
```python
LEGAL_KEYWORDS = [
    "statuts", "contrat", "l√©gal", "juridique", "article",
    "obligation", "droit", "urssaf", "r√©glementation"
]

RSE_KEYWORDS = [
    "rse", "responsabilit√©", "durable", "environnement",
    "social", "odd", "impact", "√©thique"
]

FAQ_KEYWORDS = [
    "comment", "pourquoi", "qu'est-ce", "d√©finition",
    "proc√©dure", "√©tapes"
]

JE_KEYWORDS = [
    "junior", "je", "√©cole", "ville", "contact",
    "domaine", "annuaire"
]
```

Algorithme :
1. Normalisation de la query (lowercase, suppression accents)
2. Tokenisation et extraction keywords
3. Calcul de scores par cat√©gorie (match keywords + TF-IDF)
4. S√©lection du type avec le score maximal (seuil min = 0.3)
5. Si aucun type dominant ‚Üí classification "g√©n√©ral"

Output :
```python
{
    "detected_type": "juridique",
    "confidence": 0.89,
    "scores": {
        "juridique": 0.89,
        "rse": 0.12,
        "faq": 0.34,
        "je": 0.05
    }
}
```

**Vector Search Engine** :
Moteur de recherche vectorielle optimis√© :

1. **Vectorisation de la query** :
```python
query_vector = vectorizer.transform([normalized_query])
query_vector_reduced = svd_model.transform(query_vector)
```

2. **Calcul similarit√© cosinus** :
```python
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(
    query_vector_reduced,
    index_vectors
).flatten()
```

3. **Extraction top-K candidats** :
```python
top_indices = np.argsort(similarities)[::-1][:100]
top_candidates = [
    {
        'chunk_id': idx,
        'score': similarities[idx],
        'chunk': chunks[idx]
    }
    for idx in top_indices
]
```

**Temps d'ex√©cution** :
- Vectorisation query : ~2 ms
- Calcul cosine similarity (8500 chunks) : ~8 ms
- Extraction top-K : ~1 ms
- **Total : ~11 ms**

**Contextual Booster** :
Application de coefficients multiplicateurs selon plusieurs crit√®res :

```python
def apply_boosting(candidates, query_type, filters):
    for candidate in candidates:
        chunk = candidate['chunk']
        base_score = candidate['score']
        
        # Boost par type
        if chunk['type'] == query_type:
            base_score *= 1.30
        elif chunk['type'] in RELATED_TYPES[query_type]:
            base_score *= 1.10
        
        # Boost par cat√©gorie
        if query_type == 'juridique' and 'legal' in chunk['category']:
            base_score *= 1.20
        
        # Boost par source
        if chunk['source'] in AUTHORITATIVE_SOURCES:
            base_score *= 1.15
        
        # Boost temporel
        days_old = (now - chunk['last_updated']).days
        if days_old < 90:
            base_score *= 1.10
        elif days_old > 365:
            base_score *= 0.95
        
        # Boost popularit√©
        if chunk['usage_count'] > POPULARITY_THRESHOLD:
            base_score *= 1.05
        
        candidate['boosted_score'] = base_score
    
    # Re-tri et s√©lection final top-K
    candidates.sort(key=lambda x: x['boosted_score'], reverse=True)
    return candidates[:top_k]
```

**Matrice de boosting compl√®te** :

| Crit√®re | Condition | Coefficient |
|---------|-----------|-------------|
| Type match exact | chunk.type == query_type | √ó1.30 |
| Type related | chunk.type in related_types | √ó1.10 |
| Cat√©gorie prioritaire | category match | √ó1.20 |
| Source authoritative | source in official_list | √ó1.15 |
| R√©cence < 3 mois | days_old < 90 | √ó1.10 |
| Anciennet√© > 1 an | days_old > 365 | √ó0.95 |
| Popularit√© haute | usage_count > threshold | √ó1.05 |
| Chunk mis en avant | is_featured = true | √ó1.08 |

**Context Builder** :
Construction du contexte structur√© pour le prompt LLM :

1. **Agr√©gation des chunks** :
```python
context_chunks = []
for candidate in top_k_candidates:
    chunk = candidate['chunk']
    context_chunks.append({
        'id': chunk['id'],
        'text': chunk['text'],
        'type': chunk['type'],
        'category': chunk['category'],
        'source': chunk['source_file'],
        'score': candidate['boosted_score']
    })
```

2. **D√©duplication s√©mantique** :
√âlimination des chunks trop similaires entre eux (cosine > 0.85) pour √©viter redondance.

3. **Limitation de taille** :
Respect du context window du LLM (200k tokens pour Claude, mais limitation √† ~8k tokens de contexte pour optimiser latence et co√ªt).

4. **Structuration pour prompt** :
```python
context_string = ""
for i, chunk in enumerate(context_chunks, 1):
    context_string += f"""
    
SOURCE {i} [{chunk['type'].upper()} - {chunk['category']}]:
{chunk['text']}
(Pertinence: {chunk['score']:.2f} | Fichier: {chunk['source']})

---
"""
```

**Dynamic Prompt Engineering** :
G√©n√©ration de prompts sp√©cialis√©s selon le type de requ√™te d√©tect√©.

**Template Juridique** :
```python
LEGAL_PROMPT_TEMPLATE = """Tu es un expert juridique sp√©cialis√© dans le droit des Junior-Entreprises fran√ßaises. Tu disposes d'une connaissance approfondie de la r√©glementation CNJE, du droit associatif, du droit commercial et des obligations d√©claratives.

CONTEXTE JURIDIQUE PERTINENT :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS :
1. Analyse la question et identifie les enjeux juridiques
2. Base ta r√©ponse EXCLUSIVEMENT sur les sources fournies ci-dessus
3. Cite syst√©matiquement les articles, statuts ou r√®glements applicables
4. Si la situation pr√©sente des risques, mentionne-les explicitement
5. Propose une r√©ponse actionnable et pratique
6. Si tu manques d'informations pour r√©pondre avec certitude, indique-le clairement
7. Utilise un ton professionnel mais accessible

IMPORTANT : Ne JAMAIS inventer de r√©f√©rences juridiques. Si une information n'est pas dans les sources, dis-le explicitement.

R√©ponds de mani√®re structur√©e et pr√©cise :"""
```

**Template RSE** :
```python
RSE_PROMPT_TEMPLATE = """Tu es un consultant RSE expert de l'√©cosyst√®me des Junior-Entreprises. Tu ma√Ætrises les r√©f√©rentiels RSE, les ODD, et les bonnes pratiques de d√©veloppement durable adapt√©es aux structures √©tudiantes.

DOCUMENTATION RSE DISPONIBLE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Propose une approche RSE concr√®te et actionnable
2. R√©f√©rence les modules RSE et standards applicables
3. Lie tes recommandations aux ODD pertinents
4. Fournis des exemples d'actions r√©alisables par une JE
5. Sugg√®re des indicateurs de suivi si pertinent
6. Adopte un ton encourageant et p√©dagogique

Structure ta r√©ponse avec : Diagnostic ‚Üí Recommandations ‚Üí Actions concr√®tes ‚Üí Mesure d'impact"""
```

**Template FAQ** :
```python
FAQ_PROMPT_TEMPLATE = """Tu es un assistant p√©dagogique sp√©cialis√© dans l'accompagnement des Junior-Entrepreneurs. Ton r√¥le est de clarifier les concepts, expliquer les proc√©dures et guider les membres dans leurs missions.

FAQ PERTINENTE :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Fournis une r√©ponse claire et directement applicable
2. Utilise des exemples concrets si n√©cessaire
3. D√©compose les proc√©dures complexes en √©tapes simples
4. Adopte un ton amical et encourageant
5. Propose des ressources compl√©mentaires si pertinent
6. N'h√©site pas √† reformuler pour garantir la compr√©hension

R√©ponds de mani√®re concise et structur√©e :"""
```

**Template G√©n√©ral** :
```python
GENERAL_PROMPT_TEMPLATE = """Tu es Comply, l'assistant IA de la Conf√©d√©ration Nationale des Junior-Entreprises. Tu accompagnes les Junior-Entrepreneurs dans leurs questions quotidiennes.

INFORMATIONS PERTINENTES :
{context}

QUESTION :
{question}

INSTRUCTIONS :
1. Base ta r√©ponse sur les informations fournies
2. Adopte un ton professionnel et bienveillant
3. Structure ta r√©ponse de mani√®re claire
4. Cite tes sources entre parenth√®ses [Source X]
5. Si tu ne peux pas r√©pondre avec certitude, oriente vers les bonnes ressources

R√©ponds de mani√®re utile et pr√©cise :"""
```

**Claude API Integration** :
Appel de l'API Anthropic Claude :

```python
import anthropic

async def call_claude(prompt: str, max_tokens: int = 2000):
    client = anthropic.AsyncAnthropic(
        api_key=settings.ANTHROPIC_API_KEY
    )
    
    try:
        message = await client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=max_tokens,
            temperature=0.3,  # Faible pour coh√©rence et factualit√©
            system="Tu es Comply, assistant IA expert des Junior-Entreprises.",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        return {
            'response': message.content[0].text,
            'usage': {
                'input_tokens': message.usage.input_tokens,
                'output_tokens': message.usage.output_tokens
            },
            'model': message.model,
            'stop_reason': message.stop_reason
        }
        
    except anthropic.APIError as e:
        logger.error(f"Claude API error: {e}")
        raise HTTPException(status_code=502, detail="LLM service unavailable")
```

**Param√®tres optimis√©s** :
- **Model** : `claude-sonnet-4-5-20250929` (meilleur compromis qualit√©/vitesse/co√ªt)
- **Temperature** : 0.3 (r√©p√©tabilit√© et factualit√©, pas de cr√©ativit√© excessive)
- **Max tokens** : 2000 (suffisant pour r√©ponses d√©taill√©es, limitation des co√ªts)
- **System prompt** : D√©finit le r√¥le et le contexte m√©tier

**Co√ªts** :
- Input : ~$3 / 1M tokens
- Output : ~$15 / 1M tokens
- Requ√™te moyenne : ~1500 tokens input + 500 tokens output = ~$0.012 / requ√™te
- Budget mensuel (200 requ√™tes/jour) : ~$72/mois

**Response Formatter** :
Post-processing de la r√©ponse Claude :

1. **Extraction des sources** :
Parsing de la r√©ponse pour identifier les r√©f√©rences aux sources :
```python
import re

def extract_source_references(response_text, context_chunks):
    # D√©tection pattern [Source X]
    pattern = r'\[Source (\d+)\]'
    matches = re.findall(pattern, response_text)
    
    referenced_sources = []
    for match in matches:
        source_idx = int(match) - 1
        if source_idx < len(context_chunks):
            referenced_sources.append(context_chunks[source_idx])
    
    return referenced_sources
```

2. **Calcul du score de confiance** :
Heuristique combinant plusieurs signaux :
```python
def calculate_confidence(response, context_chunks, query_type):
    confidence = 0.5  # Base
    
    # Boost si sources cit√©es
    if len(extract_source_references(response, context_chunks)) > 0:
        confidence += 0.2
    
    # Boost si type query match sources
    if any(chunk['type'] == query_type for chunk in context_chunks):
        confidence += 0.15
    
    # Boost si score moyen sources √©lev√©
    avg_score = sum(c['score'] for c in context_chunks) / len(context_chunks)
    confidence += min(avg_score * 0.15, 0.15)
    
    # R√©duction si disclaimer (incertitude)
    if "je ne peux pas" in response.lower() or "manque d'information" in response.lower():
        confidence -= 0.3
    
    return min(max(confidence, 0.0), 1.0)
```

3. **G√©n√©ration de questions li√©es** :
Suggestions de questions compl√©mentaires bas√©es sur les chunks contextuels :
```python
def generate_related_questions(context_chunks, query_type):
    # Extraction des questions similaires dans la FAQ
    faq_chunks = [c for c in context_chunks if c['type'] == 'faq']
    
    related = []
    for chunk in faq_chunks[:3]:
        if 'question' in chunk:
            related.append(chunk['question'])
    
    # Compl√©tion avec questions types par cat√©gorie
    if query_type == 'juridique':
        related.extend([
            "Quels sont les documents obligatoires pour une JE ?",
            "Comment g√©rer un contentieux client ?"
        ])
    
    return related[:5]  # Max 5 suggestions
```

4. **Structuration JSON finale** :
```python
{
    "answer": cleaned_response_text,
    "confidence": 0.87,
    "detected_type": "juridique",
    "sources": [
        {
            "chunk_id": 1542,
            "text": "Article 5 - ...",
            "type": "legal",
            "category": "statuts",
            "score": 0.92,
            "source_file": "kiwi_legal_statuts.json",
            "url": "https://kiwi.cnje.fr/legal/statuts-types"
        },
        ...
    ],
    "related_questions": [
        "Comment modifier les statuts d'une JE ?",
        "Quelle proc√©dure pour une AG extraordinaire ?"
    ],
    "metadata": {
        "query_type": "juridique",
        "chunks_used": 8,
        "llm_model": "claude-sonnet-4-5-20250929",
        "input_tokens": 1423,
        "output_tokens": 487,
        "processing_time_ms": 1847
    },
    "timestamp": "2025-01-15T16:42:33Z"
}
```

---

## Stack Technologique

### Backend & API

**Python 3.9+**
Langage principal du projet. Choix motiv√© par :
- √âcosyst√®me ML/NLP mature (scikit-learn, numpy, pandas)
- Performance suffisante pour le use case (pas de hard real-time)
- Productivit√© d√©veloppement √©lev√©e
- Type hints natifs (Python 3.9+) pour robustesse

**FastAPI 0.104+**
Framework web moderne pour APIs REST.
Avantages cl√©s :
- Performance native asynchrone (ASGI via Starlette)
- Validation automatique des inputs/outputs (Pydantic)
- Documentation OpenAPI auto-g√©n√©r√©e (Swagger UI)
- Type safety end-to-end
- Support natif async/await
- Injection de d√©pendances √©l√©gante

Performance : 3-4x plus rapide que Flask en mode async.

**Uvicorn**
Serveur ASGI haute performance :
- Bas√© sur uvloop (event loop ultra-rapide)
- Support WebSockets
- Graceful shutdown
- Hot reload en d√©veloppement

**Pydantic 2.x**
Validation et s√©rialisation de donn√©es :
- Sch√©mas typ√©s pour requests/responses
- Validation automatique avec messages d'erreur clairs
- Performance optimis√©e (core Rust)
- Support JSON Schema

### Machine Learning & NLP

**Scikit-Learn 1.3+**
Biblioth√®que ML de r√©f√©rence Python.
Utilisations :
- `TfidfVectorizer` : Vectorisation TF-IDF
- `TruncatedSVD` : R√©duction dimensionnelle
- `cosine_similarity` : Calcul de similarit√©
- `StandardScaler` : Normalisation (si n√©cessaire)

**NumPy 1.24+**
Calculs matriciels et alg√®bre lin√©aire :
- Manipulation des vecteurs/matrices sparse et dense
- Op√©rations vectoris√©es ultra-rapides (C/Fortran backend)
- Indexation avanc√©e pour filtrage

**Pandas 2.0+**
Manipulation de donn√©es structur√©es :
- Parsing des JSON sources
- Analyse exploratoire de l'index
- G√©n√©ration de statistiques
- Export de rapports

### LLM & IA

**Anthropic Claude API**
Service LLM cloud via API REST.
Mod√®le utilis√© : **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)

Caract√©ristiques :
- Context window : 200k tokens (√©norme, permet contexte riche)
- Sortie : jusqu'√† 8k tokens
- Latence : 1-3 secondes (g√©n√©ration streaming possible)
- Meilleure adh√©rence aux instructions complexes vs GPT-4
- Moins d'hallucinations
- Co√ªt comp√©titif

Client Python : `anthropic` (SDK officiel)

**Prompt Engineering**
Techniques avanc√©es appliqu√©es :
- System prompts sp√©cialis√©s par domaine
- Few-shot examples int√©gr√©s aux templates
- Chain-of-thought encourag√© via instructions
- Citation syst√©matique des sources (faithfulness)
- Disclaimers automatiques si incertitude

### Scraping & Data Acquisition

**Selenium 4.x**
Automatisation de navigateur web.
Utilisations :
- Scraping de sites dynamiques (JavaScript rendering)
- Navigation programmatique (login, menus, pagination)
- Attente explicite des √©l√©ments (WebDriverWait)
- Screenshots pour debug

Driver : **ChromeDriver** (Chromium headless)

**BeautifulSoup4**
Parsing HTML et extraction de donn√©es :
- Navigation dans l'arbre DOM
- S√©lecteurs CSS et XPath
- Nettoyage de HTML
- Extraction de texte normalis√©

**Requests**
Client HTTP pour appels API simples et t√©l√©chargements.

### Infrastructure & DevOps

**Docker** (optionnel)
Containerisation pour :
- Environnement de d√©veloppement reproductible
- Tests d'int√©gration
- Debug de probl√®mes de d√©pendances

**Git**
Versioning du code :
- Repository GitHub/GitLab SEPEFREI
- Branches : main (prod), develop (dev), feature/* (features)
- CI/CD via GitHub Actions (potentiel)

**systemd**
Gestion du service en production Linux :
- Auto-start au boot
- Restart automatique en cas de crash
- Logs centralis√©s (journalctl)
- Gestion des ressources (limits CPU/RAM)

**Nginx / Caddy**
Reverse proxy devant FastAPI :
- Termination SSL (HTTPS)
- Load balancing (si multi-instances)
- Rate limiting
- Compression gzip/brotli
- Caching statique

**Python-dotenv**
Gestion des variables d'environnement :
- Fichier `.env` pour secrets (API keys)
- S√©paration config dev/prod
- Pas de hardcoding de credentials

### Persistance & Stockage

**Pickle**
S√©rialisation native Python :
- Format binaire performant
- Pr√©servation compl√®te des objets Python (vectorizers, mod√®les, arrays)
- Pas de d√©pendance externe
- Limitation : Python-only, pas de cross-language

**JSON**
Format d'√©change et de stockage :
- Fichiers sources scrap√©s
- Configuration
- Logs structur√©s

---

## Pipeline de Donn√©es

### Vue d'Ensemble du Flux

**[IMAGE REQUISE : Diagramme de flux de donn√©es end-to-end]**

```
[Sources Web] ‚Üí [Scraping Selenium] ‚Üí [JSON Brut] ‚Üí [Nettoyage Python]
    ‚Üì
[JSON Structur√©] ‚Üí [Type Detection] ‚Üí [Extraction Champs] ‚Üí [Chunking]
    ‚Üì
[Chunks Enrichis] ‚Üí [Vectorisation TF-IDF] ‚Üí [R√©duction SVD] ‚Üí [Index Multi-niveaux]
    ‚Üì
[Pickle Persist√©] ‚Üí [Chargement RAM FastAPI] ‚Üí [API Serving]
    ‚Üì
[Query User] ‚Üí [Search Vectorielle] ‚Üí [Boosting] ‚Üí [Top-K Chunks

